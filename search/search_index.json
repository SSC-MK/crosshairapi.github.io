{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Crosshair API","text":""},{"location":"#search-intelligence-platform","title":"Search &amp; Intelligence Platform","text":"<p>The Crosshair API is a self-hosted search and intelligence platform built on SearXNG. It unifies 23 search endpoints, 70+ search engines, NLP processing, OSINT reconnaissance, security testing, and dark web monitoring behind a single REST API.</p> <p>Self-Hosted &amp; Private</p> <p>Every component runs inside your own Docker Compose stack. No data leaves your infrastructure unless you explicitly query an external engine or threat feed.</p>"},{"location":"#capabilities","title":"Capabilities","text":""},{"location":"#search-aggregation","title":"Search &amp; Aggregation","text":"Capability Details 23 Search Endpoints Web, images, videos, news, suggest, clustered, IT/programming, packages, repos, Q&amp;A, scholar, science, music, files, social, movies, apps, torrents, books, shopping, map, onion 70+ Engines Google, Brave, DuckDuckGo, and many more Tor Integration Safe <code>.onion</code> site searching with DNS leak prevention SSE Streaming Real-time search results via Server-Sent Events"},{"location":"#nlp-content-processing","title":"NLP &amp; Content Processing","text":"Capability Details Entity Extraction Automatic extraction of organizations, people, locations, dates Relationship Graphs Co-occurrence analysis and graph visualization Result Clustering ML-powered topic clustering using sentence-transformers Content Extraction Extract text from URLs and PDFs, convert to markdown PII Detection Microsoft Presidio-powered detection of 20+ PII entity types with configurable anonymization (replace, mask, hash, redact)"},{"location":"#intelligence-osint","title":"Intelligence &amp; OSINT","text":"Capability Details Intelligence Gathering DNS lookups, WHOIS, breach notifications, subdomain enumeration Threat Intelligence Technology detection, security header analysis, TLS auditing, external threat APIs Extended Threat APIs abuse.ch (URLhaus, Malware Bazaar, ThreatFox), GreyNoise, AlienVault OTX TLS Fingerprinting JARM, JA3S, and JA4 fingerprints for TLS characterization Driftnet OSINT 25 endpoints for DNS, WHOIS, scanning, CT logs, CVE data Crypto Tracking Multi-chain wallet analysis (Bitcoin, Ethereum, Solana, BNB, XRP, Dogecoin)"},{"location":"#network-api-reverse-engineering","title":"Network &amp; API Reverse Engineering","text":"Capability Details Traffic Capture Browser-based network interception with Playwright API Discovery Smart endpoint detection with authentication inference Export Formats OpenAPI 3.0, Postman v2.1, Insomnia v4, HAR 1.2 Command Generation Copy-paste ready curl, HTTPie, Python, JavaScript, PowerShell GraphQL Introspection Schema discovery and type exploration API Monitoring Track API changes over time with breaking change detection and webhook alerts Proxy Mode Non-invasive HTTP proxy for capturing mobile/desktop app traffic Request Replay Replay and modify captured requests for testing"},{"location":"#security-testing","title":"Security Testing","text":"Capability Details Directory Fuzzing wfuzz-style directory/path discovery with built-in wordlists Parameter Fuzzing URL/form parameter testing with XSS, SQLi, LFI payloads Vulnerability Scanning Security header analysis, XSS, SQL injection, LFI detection Port Scanning TCP/UDP port scanning with CVE matching API Fuzzing SQL injection, XSS, command injection testing for APIs"},{"location":"#rules-automation","title":"Rules &amp; Automation","text":"Capability Details Sigma Rules Log analysis with Sigma detection rules (evaluate, validate, list) YARA Rules Pattern matching for malware/artifact detection Event-Driven Automation Declarative rulesets and trigger-based workflows Plugin System Extensible plugin architecture with Pluggy framework AI Agents ReAct-pattern agents for autonomous multi-step workflows"},{"location":"#breach-data-extraction","title":"Breach Data Extraction","text":"Capability Details Breach Chronology Spider Extract 90,000+ breach notification records Parallel Extraction 3 concurrent Camoufox browsers with automatic proxy rotation 100 Residential Proxies Proactive rotation every 4K records (zero rate limits) Cloudflare Bypass Playwright-native response capture with Camoufox anti-detect Firefox State-Based Partitioning 53 US states + UNKN broken by org type (61 partitions)"},{"location":"#web-crawling-spider-generation","title":"Web Crawling &amp; Spider Generation","text":"Capability Details Human Behavior Simulation Mouse movements, scrolling, typing delays with Camoufox anti-detect Firefox Recursive Crawling Follow links with configurable depth/scope Anti-Detection Bot detection retry, referrer spoofing, session warmup, proxy support Scrapy Code Generation Auto-generate spider code from target URLs using TDD methodology"},{"location":"#reconnaissance","title":"Reconnaissance","text":"Capability Details CMS Detection Identify Drupal, WordPress, Next.js, Nuxt.js, Shopify with confidence scoring API Discovery Probe common REST, GraphQL, JSON:API, and feed endpoints Embedded Data Extraction Extract JSON-LD, <code>__NEXT_DATA__</code>, Drupal settings from HTML Sitemap Search Pattern matching across sitemap URLs"},{"location":"#dark-web-monitoring","title":"Dark Web Monitoring","text":"Capability Details Ransomware Tracking Monitor 45+ ransomware groups, track victims, check domains Forum Monitoring Tor-based forum crawling with keyword alerts Paste Site Monitoring Search for leaked credentials and sensitive data Pattern Matching Detect emails, passwords, API keys, credit cards, SSNs"},{"location":"#operations","title":"Operations","text":"Capability Details URL Monitoring Track URLs for changes with webhook notifications Caching Redis/Valkey-based caching with automatic cache invalidation Security SSRF prevention, query validation, webhook secret hashing"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li> <p> Getting Started</p> <p>Clone the repo, start the stack, and run your first query in under five minutes.</p> <p> Quick Start</p> </li> <li> <p> Authentication</p> <p>Configure API keys, rate limits, and anonymous access.</p> <p> Authentication</p> </li> <li> <p> Architecture</p> <p>Docker Compose services, ports, and how they connect.</p> <p> Architecture</p> </li> <li> <p> Response Format</p> <p>Standard JSON envelope, error codes, pagination, and headers.</p> <p> Response Format</p> </li> </ul>"},{"location":"#tech-stack","title":"Tech Stack","text":"Layer Technology API Framework FastAPI (Python) Search Backend SearXNG meta-search engine Browser Automation Playwright + Camoufox anti-detect Firefox Cache Valkey (Redis-compatible) Anonymity Tor SOCKS5h proxy PDF Processing MinerU NLP spaCy, sentence-transformers, Presidio Local LLM Ollama (optional) Containerization Docker Compose"},{"location":"architecture/","title":"Architecture","text":"<p>The Crosshair API runs as a set of containerized services orchestrated by Docker Compose. Each service has a single responsibility and communicates with the others over an internal Docker network.</p>"},{"location":"architecture/#service-topology","title":"Service Topology","text":"<pre><code>graph TB\n    subgraph stack[\"Docker Compose Stack\"]\n        searchapi[\"&lt;b&gt;searchapi&lt;/b&gt;&lt;br/&gt;Port 8001&lt;br/&gt;&lt;i&gt;FastAPI&lt;/i&gt;\"]\n        searxng[\"&lt;b&gt;searxng&lt;/b&gt;&lt;br/&gt;Port 8888&lt;br/&gt;&lt;i&gt;Meta-Search&lt;/i&gt;\"]\n        valkey[\"&lt;b&gt;valkey&lt;/b&gt;&lt;br/&gt;Port 6379&lt;br/&gt;&lt;i&gt;Cache&lt;/i&gt;\"]\n        selectors[\"&lt;b&gt;selectors&lt;/b&gt;&lt;br/&gt;Port 8002&lt;br/&gt;&lt;i&gt;Playwright&lt;/i&gt;\"]\n        tor[\"&lt;b&gt;tor&lt;/b&gt;&lt;br/&gt;Port 9150&lt;br/&gt;&lt;i&gt;SOCKS5h&lt;/i&gt;\"]\n        mineru[\"&lt;b&gt;mineru&lt;/b&gt;&lt;br/&gt;Port 7986&lt;br/&gt;&lt;i&gt;PDF Processing&lt;/i&gt;\"]\n        tornet[\"&lt;b&gt;Tor Network&lt;/b&gt;&lt;br/&gt;&lt;i&gt;.onion&lt;/i&gt;\"]\n    end\n\n    searchapi --&gt;|search queries| searxng\n    searchapi --&gt;|cache read/write| valkey\n    searchapi --&gt;|browser automation| selectors\n    searchapi --&gt;|PDF extraction| mineru\n    searxng --&gt;|anonymous routing| tor\n    tor --&gt;|onion routing| tornet\n\n    style stack fill:transparent,stroke:#26a69a,stroke-width:2px\n    style searchapi fill:#26a69a,color:#fff,stroke:none\n    style searxng fill:#00897b,color:#fff,stroke:none\n    style valkey fill:#00796b,color:#fff,stroke:none\n    style selectors fill:#00695c,color:#fff,stroke:none\n    style tor fill:#004d40,color:#fff,stroke:none\n    style mineru fill:#00695c,color:#fff,stroke:none\n    style tornet fill:#37474f,color:#fff,stroke:none</code></pre>"},{"location":"architecture/#services","title":"Services","text":"Service Port Technology Description searchapi <code>8001</code> FastAPI (Python) Main API gateway -- all client requests enter here. Documented in this site. searxng <code>8888</code> SearXNG Meta-search engine backend that aggregates 70+ search engines. valkey <code>6379</code> Valkey (Redis-compatible) In-memory cache with automatic invalidation for search results and extracted content. tor <code>9150</code> Tor SOCKS5h proxy used by SearXNG for <code>.onion</code> queries and anonymous routing. Internal only. selectors <code>8002</code> Playwright Headless browser pool for CSS/XPath extraction, JavaScript rendering, and network interception. mineru <code>7986</code> MinerU PDF and document processing service for content extraction and markdown conversion. ollama <code>11434</code> Ollama Local LLM inference for summaries and AI agent workflows. External/optional."},{"location":"architecture/#service-interactions","title":"Service Interactions","text":""},{"location":"architecture/#request-flow","title":"Request Flow","text":"<p>A typical search request follows this path:</p> <pre><code>sequenceDiagram\n    participant Client\n    participant searchapi as searchapi :8001\n    participant valkey as valkey :6379\n    participant searxng as searxng :8888\n    participant tor as tor :9150\n\n    Client-&gt;&gt;searchapi: GET /v1/search?q=...\n    searchapi-&gt;&gt;valkey: Check cache\n    alt Cache hit\n        valkey--&gt;&gt;searchapi: Cached results\n    else Cache miss\n        searchapi-&gt;&gt;searxng: Forward query\n        searxng-&gt;&gt;tor: Route through Tor (if .onion)\n        tor--&gt;&gt;searxng: Results\n        searxng--&gt;&gt;searchapi: Aggregated results\n        searchapi-&gt;&gt;valkey: Store in cache\n    end\n    searchapi--&gt;&gt;Client: JSON response</code></pre>"},{"location":"architecture/#content-extraction-flow","title":"Content Extraction Flow","text":"<p>When extracting content from a URL or PDF:</p> <pre><code>sequenceDiagram\n    participant Client\n    participant searchapi as searchapi :8001\n    participant selectors as selectors :8002\n    participant mineru as mineru :7986\n\n    Client-&gt;&gt;searchapi: POST /v1/content/extract\n    alt HTML page\n        searchapi-&gt;&gt;selectors: Render with Playwright\n        selectors--&gt;&gt;searchapi: Extracted text + metadata\n    else PDF document\n        searchapi-&gt;&gt;mineru: Parse PDF\n        mineru--&gt;&gt;searchapi: Markdown + structure\n    end\n    searchapi--&gt;&gt;Client: JSON response</code></pre>"},{"location":"architecture/#network-layout","title":"Network Layout","text":"<p>Internal Network</p> <p>All services communicate over a private Docker bridge network. Only searchapi (port <code>8001</code>) needs to be exposed to external clients. The remaining ports are internal.</p> Exposed to Host Internal Only <code>searchapi</code> :8001 <code>searxng</code> :8888 <code>valkey</code> :6379 <code>tor</code> :9150 <code>selectors</code> :8002 <code>mineru</code> :7986 <p>Production Hardening</p> <p>In production, place a reverse proxy (Nginx, Caddy, Traefik) in front of searchapi to handle TLS termination, request logging, and additional access control.</p>"},{"location":"authentication/","title":"Authentication","text":"<p>Authentication in the Crosshair API is optional. When no API keys are configured, all endpoints are accessible anonymously with a default rate limit. When keys are configured, they provide named identification and per-key rate limits.</p>"},{"location":"authentication/#api-key-format","title":"API Key Format","text":"<p>API keys are defined through a single environment variable using a comma-separated list. Each entry follows the format:</p> <pre><code>key:name:rate_limit\n</code></pre> Field Description <code>key</code> The secret token the client sends with each request <code>name</code> A human-readable label for this key (used in logs and metrics) <code>rate_limit</code> Maximum number of requests allowed per hour for this key"},{"location":"authentication/#environment-variable","title":"Environment Variable","text":"<p>Set <code>API_KEYS</code> in your environment or <code>.env</code> file:</p> <pre><code># Format: key:name:rate_limit (comma-separated for multiple keys)\nAPI_KEYS=\"abc123:Production:10000,dev456:Development:1000\" # (1) (2) (3)\n</code></pre> <ol> <li><code>abc123</code> -- The secret token sent in the <code>X-API-Key</code> header. Use a long, random string in production (e.g., <code>openssl rand -hex 32</code>).</li> <li><code>Production</code> -- A human-readable label shown in server logs and metrics. Helps identify which application or team is making requests.</li> <li><code>10000</code> -- Maximum requests per hour for this key. Each key tracks its own quota independently. Exceeding this returns a <code>429 RATE_LIMITED</code> error.</li> </ol> <p>Multiple Keys</p> <p>You can define as many keys as needed. Each key has its own name and independent rate limit.</p> <pre><code>API_KEYS=\"prod-key-001:Prod-App:50000,staging-key:Staging:5000,dev456:Dev-Local:1000\"\n</code></pre>"},{"location":"authentication/#using-api-keys","title":"Using API Keys","text":"<p>Pass the key in the <code>X-API-Key</code> request header:</p> curlPythonJavaScript <pre><code>curl -H \"X-API-Key: abc123\" \\\n  \"http://localhost:8001/v1/search?q=test\"\n</code></pre> <pre><code>import requests\n\nresp = requests.get(\n    \"http://localhost:8001/v1/search\",\n    params={\"q\": \"test\"},\n    headers={\"X-API-Key\": \"abc123\"},\n)\nprint(resp.json())\n</code></pre> <pre><code>const resp = await fetch(\"http://localhost:8001/v1/search?q=test\", {\n  headers: { \"X-API-Key\": \"abc123\" },\n});\nconst data = await resp.json();\nconsole.log(data);\n</code></pre> <p>Keep Keys Secret</p> <p>API keys are sent as plain-text headers. Always use HTTPS in production, or restrict access to a trusted network.</p>"},{"location":"authentication/#anonymous-access","title":"Anonymous Access","text":"<p>If the <code>API_KEYS</code> environment variable is not set (or is empty), the API operates in anonymous mode:</p> <ul> <li>No <code>X-API-Key</code> header is required.</li> <li>All requests are allowed up to the default rate limit.</li> <li>This is the default out-of-the-box behavior.</li> </ul> <p>Default Rate Limit</p> <p>Anonymous requests share a single global rate limit. Set <code>API_KEYS</code> to give individual clients their own quotas.</p>"},{"location":"authentication/#authentication-errors","title":"Authentication Errors","text":"<p>When authentication is enabled but the request fails validation, the API returns one of the following error codes:</p> Error Code HTTP Status Meaning <code>UNAUTHORIZED</code> 401 No API key was provided and anonymous access is disabled <code>INVALID_API_KEY</code> 401 The provided key does not match any configured key <code>RATE_LIMITED</code> 429 The key's rate limit has been exceeded <p>Example error response:</p> <pre><code>{\n  \"success\": false,\n  \"data\": null,\n  \"meta\": {\n    \"request_id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\",\n    \"timestamp\": \"2026-01-31T03:00:00.000000Z\",\n    \"version\": \"1.0.0\"\n  },\n  \"error\": {\n    \"code\": \"INVALID_API_KEY\",\n    \"message\": \"The provided API key is not valid\",\n    \"category\": \"client\",\n    \"details\": null\n  }\n}\n</code></pre>"},{"location":"authentication/#quick-reference","title":"Quick Reference","text":"Item Value Header name <code>X-API-Key</code> Env variable <code>API_KEYS</code> Entry format <code>key:name:rate_limit</code> Separator <code>,</code> (comma) Anonymous mode Enabled when <code>API_KEYS</code> is unset"},{"location":"configuration/","title":"Configuration","text":"<p>All runtime behaviour is controlled through environment variables and Docker Compose configuration.</p>"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":""},{"location":"configuration/#core-services","title":"Core Services","text":"Variable Default Description <code>SEARXNG_URL</code> <code>http://searxng:8080</code> SearXNG backend URL <code>VALKEY_URL</code> <code>redis://valkey:6379/0</code> Valkey / Redis URL <code>OLLAMA_URL</code> <code>http://host.docker.internal:11434</code> Ollama LLM URL <code>OLLAMA_MODEL</code> <code>llama3.2:latest</code> Ollama model for summaries <code>READER_LM_MODEL</code> <code>reader-lm:latest</code> Model for markdown conversion <code>SELECTORS_URL</code> <code>http://localhost:8002</code> Selectors service URL <code>MINERU_URL</code> <code>http://172.17.0.1:7986</code> MinerU PDF service URL <code>CACHE_TTL</code> <code>3600</code> Cache TTL in seconds <code>LOG_LEVEL</code> <code>INFO</code> Logging level <code>CORS_ORIGINS</code> <code>http://localhost:3000,http://localhost:8080</code> Allowed CORS origins (comma-separated) <code>API_KEYS</code> (empty) API keys (format: <code>key:name:limit,...</code>) <code>DEFAULT_RATE_LIMIT</code> <code>100</code> Default rate limit per hour <code>RATE_LIMIT_WINDOW</code> <code>3600</code> Rate limit window in seconds <code>API_VERSION</code> <code>1.0.0</code> API version string"},{"location":"configuration/#threat-intelligence-api-keys-optional","title":"Threat Intelligence API Keys (Optional)","text":"<p>These keys are optional. Endpoints that use them will still work without them but return limited data.</p> Variable Description Free Tier <code>VIRUSTOTAL_API_KEY</code> VirusTotal URL / domain scanning 500 requests/day <code>SHODAN_API_KEY</code> Shodan IP intelligence 100 credits/month <code>ABUSEIPDB_API_KEY</code> AbuseIPDB IP reputation 1,000 checks/day <code>CENSYS_API_ID</code> Censys certificate search 250 requests/month <code>CENSYS_API_SECRET</code> Censys API secret -- <code>HYBRID_ANALYSIS_API_KEY</code> Hybrid Analysis malware sandbox 50 submissions/day <code>GOOGLE_SAFE_BROWSING_KEY</code> Google Safe Browsing 10,000 requests/day <code>SSLLABS_EMAIL</code> SSL Labs API (for rate limit increase) -- <p>Get Free API Keys</p> <p>All threat intelligence providers offer free tiers:</p> <ul> <li>VirusTotal -- https://www.virustotal.com/gui/join-us</li> <li>Shodan -- https://account.shodan.io/register</li> <li>AbuseIPDB -- https://www.abuseipdb.com/register</li> <li>Censys -- https://censys.io/register</li> <li>Hybrid Analysis -- https://www.hybrid-analysis.com/signup</li> <li>Google Safe Browsing -- https://console.cloud.google.com</li> </ul>"},{"location":"configuration/#example-env-file","title":"Example <code>.env</code> File","text":"<pre><code># Core Services\nSEARXNG_URL=http://searxng:8080\nVALKEY_URL=redis://valkey:6379/1\nOLLAMA_URL=http://172.17.0.1:11434\nOLLAMA_MODEL=llama3.2:latest\nCACHE_TTL=3600\nLOG_LEVEL=INFO\nCORS_ORIGINS=http://localhost:3000,http://localhost:8080\nAPI_KEYS=prod-key-123:Production:10000,dev-key-456:Development:1000\n\n# Threat Intelligence API Keys (optional)\nVIRUSTOTAL_API_KEY=your_virustotal_key\nSHODAN_API_KEY=your_shodan_key\nABUSEIPDB_API_KEY=your_abuseipdb_key\nHYBRID_ANALYSIS_API_KEY=your_hybrid_analysis_key\nGOOGLE_SAFE_BROWSING_KEY=your_google_key\n</code></pre>"},{"location":"configuration/#docker-deployment","title":"Docker Deployment","text":""},{"location":"configuration/#docker-composeyml","title":"<code>docker-compose.yml</code>","text":"<pre><code>services:\n  valkey:\n    image: valkey/valkey:7-alpine\n    restart: unless-stopped\n    volumes:\n      - valkey_data:/data\n\n  searxng:\n    image: searxng/searxng:latest\n    restart: unless-stopped\n    depends_on: [valkey]\n    ports:\n      - \"8888:8080\"\n    volumes:\n      - ./config:/etc/searxng\n    environment:\n      - SEARXNG_LIMITER=false\n      - SEARXNG_VALKEY_URL=valkey://valkey:6379/0\n\n  searchapi:\n    build:\n      context: ./api\n      dockerfile: Dockerfile\n    restart: unless-stopped\n    depends_on:\n      - searxng\n      - valkey\n    ports:\n      - \"8001:8000\"\n    environment:\n      - SEARXNG_URL=http://searxng:8080\n      - VALKEY_URL=redis://valkey:6379/1\n      - API_KEYS=your-api-key:Production:10000\n      - CACHE_TTL=3600\n      - LOG_LEVEL=INFO\n      - OLLAMA_URL=http://172.17.0.1:11434\n      - SELECTORS_URL=http://selectors:8002\n      # Threat Intelligence API Keys (optional)\n      - VIRUSTOTAL_API_KEY=${VIRUSTOTAL_API_KEY:-}\n      - SHODAN_API_KEY=${SHODAN_API_KEY:-}\n      - ABUSEIPDB_API_KEY=${ABUSEIPDB_API_KEY:-}\n      - HYBRID_ANALYSIS_API_KEY=${HYBRID_ANALYSIS_API_KEY:-}\n      - GOOGLE_SAFE_BROWSING_KEY=${GOOGLE_SAFE_BROWSING_KEY:-}\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/v1/health/live\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n\n  selectors:\n    build:\n      context: ./selectors\n      dockerfile: Dockerfile\n    restart: unless-stopped\n    ports:\n      - \"8002:8002\"\n    environment:\n      - EMBEDDING_MODEL=all-MiniLM-L6-v2\n\nvolumes:\n  valkey_data:\n</code></pre>"},{"location":"configuration/#service-architecture","title":"Service Architecture","text":"Service Port Description <code>valkey</code> 6379 (internal) Redis-compatible cache and rate limit store <code>searxng</code> 8888:8080 SearXNG meta-search engine backend <code>searchapi</code> 8001:8000 The main API service <code>selectors</code> 8002:8002 CSS/XPath selector intelligence service"},{"location":"configuration/#docker-commands","title":"Docker Commands","text":"<pre><code># Start all services\ndocker compose up -d\n\n# View logs\ndocker compose logs -f searchapi\n\n# Rebuild after changes\ndocker compose build searchapi &amp;&amp; docker compose up -d searchapi\n\n# Stop all services\ndocker compose down\n\n# Remove volumes (full data reset)\ndocker compose down -v\n</code></pre>"},{"location":"configuration/#dockerfile","title":"Dockerfile","text":"<pre><code>FROM python:3.12-slim\n\nWORKDIR /app\n\nRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\\n    build-essential curl &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\nRUN python -m spacy download en_core_web_sm\n\nCOPY . .\nRUN mkdir -p /app/data /app/logs\n\nEXPOSE 8000\n\nHEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n    CMD curl -f http://localhost:8000/v1/health/live || exit 1\n\nCMD [\"uvicorn\", \"main_new:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre> <p>Health Check</p> <p>The container exposes a health check endpoint at <code>/v1/health/live</code> that Docker uses to determine container health. The <code>searchapi</code> service includes a <code>healthcheck</code> block that polls this endpoint every 30 seconds.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Get the Crosshair API running locally and execute your first queries in under five minutes.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Required Software</p> <ul> <li>Docker (v20.10+)</li> <li>Docker Compose (v2.0+)</li> </ul> <p>Make sure both are installed and the Docker daemon is running:</p> <pre><code>docker --version\ndocker compose version\n</code></pre>"},{"location":"getting-started/#1-clone-and-start","title":"1. Clone and Start","text":"<pre><code>git clone &lt;your-repo-url&gt; searxng\ncd searxng\ndocker compose up -d\n</code></pre> <p>First Launch</p> <p>The initial pull downloads several container images (SearXNG, Valkey, Tor, Playwright, MinerU, etc.). This may take a few minutes on a fresh machine.</p>"},{"location":"getting-started/#2-verify-health","title":"2. Verify Health","text":"<p>Confirm that the API is alive by hitting the health endpoint:</p> curlPythonJavaScript <pre><code>curl http://localhost:8001/v1/health/live\n</code></pre> <pre><code>import requests\n\nresp = requests.get(\"http://localhost:8001/v1/health/live\")\nprint(resp.json())\n</code></pre> <pre><code>const resp = await fetch(\"http://localhost:8001/v1/health/live\");\nconst data = await resp.json();\nconsole.log(data);\n</code></pre> <p>Expected response:</p> <pre><code>{\"status\": \"alive\"} // (1)\n</code></pre> <ol> <li>The only possible value is <code>\"alive\"</code> -- this endpoint confirms the API process is running. It does not verify backend dependencies (use <code>/v1/health/ready</code> for that).</li> </ol> <p>Stack is Running</p> <p>If you see <code>{\"status\": \"alive\"}</code>, all core services are up and accepting requests.</p>"},{"location":"getting-started/#3-run-your-first-search","title":"3. Run Your First Search","text":"<p>Execute a web search across 70+ aggregated engines:</p> curlPythonJavaScript <pre><code>curl \"http://localhost:8001/v1/search?q=python+programming&amp;limit=5\" # (1) (2)\n</code></pre> <ol> <li><code>q</code> -- The search query. Supports Google dork syntax (<code>site:</code>, <code>filetype:</code>, <code>intitle:</code>, <code>\"exact phrase\"</code>, <code>-exclude</code>).</li> <li><code>limit=5</code> -- Return at most 5 results. Accepts 1-100; defaults to 10 if omitted.</li> </ol> <pre><code>import requests\n\nresp = requests.get(\n    \"http://localhost:8001/v1/search\",\n    params={\"q\": \"python programming\", \"limit\": 5},\n)\nresults = resp.json()\n\nfor item in results[\"data\"][\"results\"]:\n    print(item[\"title\"], item[\"url\"])\n</code></pre> <pre><code>const params = new URLSearchParams({\n  q: \"python programming\",\n  limit: \"5\",\n});\n\nconst resp = await fetch(`http://localhost:8001/v1/search?${params}`);\nconst results = await resp.json();\n\nresults.data.results.forEach((item) =&gt; {\n  console.log(item.title, item.url);\n});\n</code></pre>"},{"location":"getting-started/#4-extract-content-from-a-url","title":"4. Extract Content from a URL","text":"<p>Pull the main text content from any web page:</p> curlPythonJavaScript <pre><code>curl -X POST http://localhost:8001/v1/content/extract \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\": \"https://example.com\"}'\n</code></pre> <pre><code>import requests\n\nresp = requests.post(\n    \"http://localhost:8001/v1/content/extract\",\n    json={\"url\": \"https://example.com\"},\n)\nprint(resp.json()[\"data\"][\"text\"])\n</code></pre> <pre><code>const resp = await fetch(\"http://localhost:8001/v1/content/extract\", {\n  method: \"POST\",\n  headers: { \"Content-Type\": \"application/json\" },\n  body: JSON.stringify({ url: \"https://example.com\" }),\n});\n\nconst data = await resp.json();\nconsole.log(data.data.text);\n</code></pre>"},{"location":"getting-started/#whats-next","title":"What's Next?","text":"<p>Continue Learning</p> Topic Description Authentication Set up API keys and rate limits Architecture Understand the Docker Compose service topology Response Format Learn the standard JSON envelope, error codes, and pagination API Reference Explore individual endpoint groups (Search, Intelligence, Security, etc.)"},{"location":"response-format/","title":"Response Format","text":"<p>All <code>/v1</code> endpoints return a standardized JSON envelope. This page covers the response structure, headers, error codes, and pagination model.</p>"},{"location":"response-format/#success-response","title":"Success Response","text":"<p>Every successful response wraps endpoint-specific data inside a common envelope:</p> <pre><code>{\n  \"success\": true, // (1)\n  \"data\": { // (2)\n    // Endpoint-specific payload\n  },\n  \"meta\": { // (3)\n    \"request_id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\", // (4)\n    \"timestamp\": \"2026-01-31T03:00:00.000000Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 123 // (5)\n  },\n  \"pagination\": { // (6)\n    \"next_cursor\": \"eyJpZCI6ICIxMCJ9\", // (7)\n    \"has_more\": true,\n    \"returned\": 10\n  }\n}\n</code></pre> <ol> <li>Always <code>true</code> for 2xx responses. Check this field first to determine success vs. failure.</li> <li>The endpoint-specific payload. Each endpoint defines its own <code>data</code> schema -- see the individual endpoint docs.</li> <li>Request metadata included in every response. Useful for debugging, logging, and performance monitoring.</li> <li>A UUID that uniquely identifies this request. Also returned in the <code>X-Request-ID</code> response header. Include this in bug reports.</li> <li>Server-side processing time in milliseconds. Does not include network latency. Use this to identify slow queries.</li> <li>Present only on paginated endpoints. If this key is absent, the endpoint returns all results in a single response.</li> <li>Base64-encoded opaque cursor. Pass this as the <code>cursor</code> query parameter in your next request to fetch the next page.</li> </ol> Field Type Description <code>success</code> <code>boolean</code> Always <code>true</code> for 2xx responses <code>data</code> <code>object</code> The endpoint's result payload <code>meta.request_id</code> <code>string</code> UUID for tracing and support requests <code>meta.timestamp</code> <code>string</code> ISO 8601 UTC timestamp of the response <code>meta.version</code> <code>string</code> API version that handled the request <code>meta.processing_time_ms</code> <code>integer</code> Server-side processing time in milliseconds <code>pagination</code> <code>object \\| null</code> Present only on paginated endpoints (see Pagination below) <p>The <code>pagination</code> key</p> <p>Non-paginated endpoints omit the <code>pagination</code> field entirely. Its presence tells you the endpoint supports cursor-based paging.</p>"},{"location":"response-format/#error-response","title":"Error Response","text":"<p>When a request fails, the envelope switches to an error shape:</p> <pre><code>{\n  \"success\": false,\n  \"data\": null,\n  \"meta\": {\n    \"request_id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\",\n    \"timestamp\": \"2026-01-31T03:00:00.000000Z\",\n    \"version\": \"1.0.0\"\n  },\n  \"error\": {\n    \"code\": \"ERROR_CODE\", // (1)\n    \"message\": \"Human-readable message\",\n    \"category\": \"client|server|upstream\", // (2)\n    \"details\": null\n  }\n}\n</code></pre> <ol> <li>Machine-readable error code for programmatic handling. Use this (not HTTP status) to branch your retry logic. See the full list in Error Codes below.</li> <li>Indicates where the fault lies: <code>client</code> means fix your request, <code>server</code> means retry with back-off, <code>upstream</code> means an external dependency failed (also retry).</li> </ol> Field Type Description <code>success</code> <code>boolean</code> Always <code>false</code> for error responses <code>data</code> <code>null</code> Always <code>null</code> on errors <code>error.code</code> <code>string</code> Machine-readable error code (see table below) <code>error.message</code> <code>string</code> Human-readable explanation <code>error.category</code> <code>string</code> One of <code>client</code>, <code>server</code>, or <code>upstream</code> <code>error.details</code> <code>object \\| null</code> Optional structured details (validation errors, upstream messages, etc.)"},{"location":"response-format/#response-headers","title":"Response Headers","text":"<p>Every response includes the following custom headers:</p> Header Type Description <code>X-Request-ID</code> <code>string</code> Unique request identifier -- matches <code>meta.request_id</code> in the body. Use this when reporting issues. <code>X-Processing-Time-Ms</code> <code>integer</code> Server-side processing time in milliseconds -- matches <code>meta.processing_time_ms</code>. <p>Correlating Requests</p> <p>Include <code>X-Request-ID</code> in bug reports or support requests so the server operator can trace the exact request through logs.</p>"},{"location":"response-format/#error-codes","title":"Error Codes","text":"<p>The API defines 16 error codes organized into three categories.</p>"},{"location":"response-format/#client-errors-4xx","title":"Client Errors (4xx)","text":"<p>These indicate a problem with the request itself. Fix the request and retry.</p> Code HTTP Status Description <code>MISSING_REQUIRED_PARAM</code> 400 A required parameter was not provided <code>INVALID_PARAM_VALUE</code> 400 A parameter value is out of range or malformed <code>INVALID_URL</code> 400 The supplied URL is malformed or unreachable <code>UNAUTHORIZED</code> 401 Authentication is required but no credentials were provided <code>INVALID_API_KEY</code> 401 The provided API key does not match any configured key <code>FORBIDDEN</code> 403 The authenticated identity does not have permission <code>NOT_FOUND</code> 404 The requested resource does not exist <code>MONITOR_NOT_FOUND</code> 404 The specified monitor ID was not found <code>JOB_NOT_FOUND</code> 404 The specified job ID was not found <code>RATE_LIMITED</code> 429 The rate limit for this key (or anonymous tier) has been exceeded"},{"location":"response-format/#server-errors-5xx","title":"Server Errors (5xx)","text":"<p>These indicate an internal problem. Retry with exponential back-off.</p> Code HTTP Status Description <code>INTERNAL_ERROR</code> 500 An unexpected error occurred on the server <code>BROWSER_UNAVAILABLE</code> 503 The Playwright browser pool is exhausted or unreachable <code>CACHE_UNAVAILABLE</code> 503 The Valkey/Redis cache service is unreachable"},{"location":"response-format/#upstream-errors-5xx","title":"Upstream Errors (5xx)","text":"<p>These indicate a problem with an external dependency. The API tried but the upstream service failed.</p> Code HTTP Status Description <code>SEARCH_FAILED</code> 502 The SearXNG search backend returned an error <code>EXTRACTION_FAILED</code> 502 Content extraction from the target URL or document failed <code>UPSTREAM_TIMEOUT</code> 504 An upstream service did not respond within the timeout window <p>Retrying Upstream Errors</p> <p>Upstream errors (502, 504) are often transient. Implement retry logic with exponential back-off and a maximum of 3 attempts.</p>"},{"location":"response-format/#pagination","title":"Pagination","text":"<p>Paginated endpoints use cursor-based pagination. This approach is more reliable than offset-based pagination because it is immune to insertions and deletions between pages.</p>"},{"location":"response-format/#how-it-works","title":"How It Works","text":"<ol> <li>Make an initial request with a <code>limit</code> parameter.</li> <li>If more results exist, the response includes a <code>pagination</code> object with <code>next_cursor</code>.</li> <li>Pass <code>next_cursor</code> as the <code>cursor</code> query parameter in the next request.</li> <li>Repeat until <code>has_more</code> is <code>false</code>.</li> </ol>"},{"location":"response-format/#pagination-object","title":"Pagination Object","text":"Field Type Description <code>next_cursor</code> <code>string \\| null</code> Base64-encoded cursor for the next page. <code>null</code> when there are no more results. <code>has_more</code> <code>boolean</code> <code>true</code> if additional pages exist beyond the current response. <code>returned</code> <code>integer</code> Number of items returned in this page."},{"location":"response-format/#example","title":"Example","text":"First PageNext PageLast Page <pre><code>curl \"http://localhost:8001/v1/search?q=test&amp;limit=10\"\n</code></pre> <p>Response (truncated):</p> <pre><code>{\n  \"success\": true,\n  \"data\": { \"results\": [\"...\"] },\n  \"pagination\": {\n    \"next_cursor\": \"eyJpZCI6ICIxMCJ9\",\n    \"has_more\": true,\n    \"returned\": 10\n  }\n}\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/search?q=test&amp;limit=10&amp;cursor=eyJpZCI6ICIxMCJ9\"\n</code></pre> <p>Response (truncated):</p> <pre><code>{\n  \"success\": true,\n  \"data\": { \"results\": [\"...\"] },\n  \"pagination\": {\n    \"next_cursor\": \"eyJpZCI6ICIyMCJ9\",\n    \"has_more\": true,\n    \"returned\": 10\n  }\n}\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/search?q=test&amp;limit=10&amp;cursor=eyJpZCI6ICIyMCJ9\"\n</code></pre> <p>Response (truncated):</p> <pre><code>{\n  \"success\": true,\n  \"data\": { \"results\": [\"...\"] },\n  \"pagination\": {\n    \"next_cursor\": null,\n    \"has_more\": false,\n    \"returned\": 4\n  }\n}\n</code></pre> <p>Cursor Lifetime</p> <p>Cursors are stateless and do not expire. They encode just enough information for the server to resume from the correct position.</p>"},{"location":"response-format/#full-python-example","title":"Full Python Example","text":"<pre><code>import requests\n\nBASE = \"http://localhost:8001\"\nall_results = []\ncursor = None\n\nwhile True:\n    params = {\"q\": \"test\", \"limit\": 10}\n    if cursor:\n        params[\"cursor\"] = cursor\n\n    resp = requests.get(f\"{BASE}/v1/search\", params=params)\n    body = resp.json()\n\n    all_results.extend(body[\"data\"][\"results\"])\n\n    pagination = body.get(\"pagination\")\n    if not pagination or not pagination[\"has_more\"]:\n        break\n    cursor = pagination[\"next_cursor\"]\n\nprint(f\"Fetched {len(all_results)} total results\")\n</code></pre>"},{"location":"security/","title":"Security &amp; Rate Limiting","text":"<p>Security controls and rate limiting configuration for the API.</p>"},{"location":"security/#ssrf-prevention","title":"SSRF Prevention","text":"<p>All URL-based endpoints validate URLs to prevent Server-Side Request Forgery (SSRF) attacks. Requests targeting internal infrastructure are blocked before any outbound connection is made.</p>"},{"location":"security/#blocked-targets","title":"Blocked Targets","text":"Category Blocked Ranges / Hostnames Private IPv4 <code>10.0.0.0/8</code>, <code>172.16.0.0/12</code>, <code>192.168.0.0/16</code>, <code>127.0.0.0/8</code> Link-local <code>169.254.0.0/16</code> Private IPv6 <code>::1/128</code>, <code>fc00::/7</code>, <code>fe80::/10</code> Hostnames <code>localhost</code>, <code>internal</code>, <code>metadata</code>, <code>169.254.169.254</code> Cloud metadata AWS / GCP / Azure metadata endpoints"},{"location":"security/#example-blocked-request","title":"Example Blocked Request","text":"<pre><code>curl -X POST http://localhost:8001/v1/content/extract \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\": \"http://169.254.169.254/latest/meta-data/\"}'\n</code></pre> <pre><code>{\n  \"success\": false,\n  \"error\": {\n    \"code\": \"INVALID_URL\",\n    \"message\": \"Private/internal IP addresses are not allowed\",\n    \"category\": \"client\"\n  }\n}\n</code></pre>"},{"location":"security/#query-validation","title":"Query Validation","text":"<p>Search queries are sanitised to prevent injection attacks. Any request containing a blocked pattern is rejected with a <code>400</code> status.</p>"},{"location":"security/#blocked-patterns","title":"Blocked Patterns","text":"Pattern Reason Control characters (<code>\\x00</code>-<code>\\x1f</code>) Binary injection <code>&lt;script</code> XSS attempts <code>javascript:</code> JavaScript injection <code>data:</code> Data URI injection <code>vbscript:</code> VBScript injection"},{"location":"security/#example-blocked-request_1","title":"Example Blocked Request","text":"<pre><code>curl \"http://localhost:8001/v1/search?q=&lt;script&gt;alert(1)&lt;/script&gt;\"\n</code></pre> <pre><code>{\n  \"success\": false,\n  \"error\": {\n    \"code\": \"INVALID_PARAM_VALUE\",\n    \"message\": \"Query contains potentially dangerous content\",\n    \"category\": \"client\"\n  }\n}\n</code></pre>"},{"location":"security/#webhook-secret-hashing","title":"Webhook Secret Hashing","text":"<p>Webhook secrets are hashed before storage using industry-standard key derivation.</p> Property Value Algorithm PBKDF2-HMAC-SHA256 Iterations 100,000 Salt 16-byte random per secret Storage Only the hash is stored; the original secret is never persisted"},{"location":"security/#webhook-signature-verification","title":"Webhook Signature Verification","text":"<p>When the API sends a webhook, the payload is signed with the original secret. Verify it on your end:</p> <pre><code>import hmac\nimport hashlib\n\ndef verify_signature(payload: str, signature: str, secret: str) -&gt; bool:\n    expected = hmac.new(\n        secret.encode(),\n        payload.encode(),\n        hashlib.sha256\n    ).hexdigest()\n    return hmac.compare_digest(f\"sha256={expected}\", signature)\n</code></pre> <p>Secret Management</p> <p>Never log or expose webhook secrets. The API stores only the PBKDF2 hash -- if you lose the secret, you must regenerate it.</p>"},{"location":"security/#rate-limiting","title":"Rate Limiting","text":"<p>Rate limiting is enforced per API key, or per IP address for anonymous (unauthenticated) access.</p> Setting Default Description Requests per window 100 Configurable per key via <code>API_KEYS</code> env var Window size 3,600 s (1 hour) Configurable via <code>RATE_LIMIT_WINDOW</code> env var Exceeded response <code>429 RATE_LIMITED</code> Returned when the limit is exceeded"},{"location":"security/#response-headers","title":"Response Headers","text":"<p>Every API response includes rate-limit headers:</p> <pre><code>X-RateLimit-Limit: 100\nX-RateLimit-Remaining: 95\nX-RateLimit-Reset: 1706666400\n</code></pre> Header Description <code>X-RateLimit-Limit</code> Maximum requests allowed in the current window <code>X-RateLimit-Remaining</code> Requests remaining in the current window <code>X-RateLimit-Reset</code> Unix timestamp when the window resets"},{"location":"security/#configuring-per-key-limits","title":"Configuring Per-Key Limits","text":"<p>Set the <code>API_KEYS</code> environment variable with a comma-separated list of <code>key:name:limit</code> tuples:</p> <pre><code>API_KEYS=prod-key-123:Production:10000,dev-key-456:Development:1000\n</code></pre> <p>This gives the <code>Production</code> key 10,000 requests per window and the <code>Development</code> key 1,000 requests per window.</p> <p>Monitoring Usage</p> <p>Check the <code>X-RateLimit-Remaining</code> header in every response to track your consumption. Build alerting on this header to avoid hitting the limit unexpectedly.</p>"},{"location":"tags/","title":"Tags","text":"<p>Browse API documentation by capability area.</p>"},{"location":"tags/#tag:ai","title":"AI","text":"<ul> <li>            AI Agents          </li> </ul>"},{"location":"tags/#tag:automation","title":"Automation","text":"<ul> <li>            AI Agents          </li> <li>            Automation          </li> <li>            Rules Engine          </li> <li>            Spider Generator          </li> </ul>"},{"location":"tags/#tag:crypto","title":"Crypto","text":"<ul> <li>            Crypto Wallets          </li> </ul>"},{"location":"tags/#tag:dark-web","title":"Dark Web","text":"<ul> <li>            Dark Web          </li> <li>            Telegram Intel          </li> </ul>"},{"location":"tags/#tag:data","title":"Data","text":"<ul> <li>            Breach Spider          </li> <li>            Content Extraction          </li> <li>            Web Crawler          </li> </ul>"},{"location":"tags/#tag:network","title":"Network","text":"<ul> <li>            Network Analysis          </li> </ul>"},{"location":"tags/#tag:osint","title":"OSINT","text":"<ul> <li>            Breach Spider          </li> <li>            Crypto Wallets          </li> <li>            Dark Web          </li> <li>            Driftnet OSINT          </li> <li>            Intelligence          </li> <li>            Reconnaissance          </li> <li>            Telegram Intel          </li> </ul>"},{"location":"tags/#tag:search","title":"Search","text":"<ul> <li>            Search          </li> <li>            Streaming          </li> </ul>"},{"location":"tags/#tag:security","title":"Security","text":"<ul> <li>            Intelligence          </li> <li>            Network Analysis          </li> <li>            Reconnaissance          </li> <li>            Rules Engine          </li> <li>            Security Testing          </li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>Complete reference for all 304 API endpoints organized by domain.</p>"},{"location":"api/#endpoint-overview","title":"Endpoint Overview","text":"Section Endpoints Description Search 26 Web, images, videos, news, scholar, shopping, clustered, and 15+ specialized search types Content 8 Text extraction, screenshots, markdown conversion, PII anonymization Intelligence 47 DNS, WHOIS, breach checks, TLS fingerprinting, Censys, Vulners, SSL Labs, abuse.ch, GreyNoise, OTX Driftnet OSINT 25 DNS lookups, IP intel, port scanning, CT logs, CVE data via Driftnet API Network Analysis 29 Traffic capture, API discovery, export (OpenAPI/Postman/HAR), proxy mode, replay Security Testing 14 Directory fuzzing, parameter fuzzing, vulnerability scanning, port scanning, CVE matching Dark Web 49 Ransomware tracking, forum monitoring, paste sites, site scraping, change detection Breach Spider 7 90K+ breach record extraction with Camoufox and proxy rotation Spider Generator 7 Scrapy code generation with TDD, site analysis, templates Web Crawler 3 Human behavior simulation crawling with Camoufox Reconnaissance 6 CMS detection, API discovery, embedded data extraction, sitemap search Telegram Intel 17 Threat channel tracking, ransomware/hacktivist/stealer-log feeds Crypto Wallets 7 Multi-chain wallet tracking (BTC, ETH, SOL, BNB, XRP, DOGE) AI Agents 6 ReAct-pattern autonomous agents with multi-step tool use Rules Engine 16 Sigma detection rules, YARA pattern matching, declarative rulesets Automation 27 Monitors, jobs, events, triggers, plugins Streaming 2 Real-time SSE search streams"},{"location":"api/#quick-reference","title":"Quick Reference","text":"<p>All endpoints use the <code>/v1/</code> prefix. Responses follow a standard JSON envelope.</p> <pre><code># Base URL\nhttp://localhost:8001/v1/\n\n# Example: search the web\ncurl \"http://localhost:8001/v1/search?q=python+programming&amp;limit=5\"\n</code></pre> <p>For interactive testing, use the OpenAPI Explorer.</p>"},{"location":"api/agents/","title":"AI Agents","text":"<p>The Agents API provides autonomous AI agents that use the ReAct (Reasoning + Acting) pattern to complete multi-step tasks. Agents can plan, execute tools, observe results, and iterate until the task is complete.</p> <p>ReAct Pattern</p> <p>ReAct agents follow a Thought -&gt; Action -&gt; Observation loop:</p> <ol> <li>Thought -- The agent reasons about what to do next</li> <li>Action -- The agent selects and invokes a tool</li> <li>Observation -- The agent observes the result and decides the next step</li> </ol>","tags":["AI","Automation"]},{"location":"api/agents/#endpoints","title":"Endpoints","text":"","tags":["AI","Automation"]},{"location":"api/agents/#list-agent-types","title":"List Agent Types","text":"<p><code>GET /v1/agents</code></p> <p>List all available agent types and their capabilities.</p> <pre><code>curl http://localhost:8001/v1/agents\n</code></pre>","tags":["AI","Automation"]},{"location":"api/agents/#run-agent","title":"Run Agent","text":"<p><code>POST /v1/agents/{agent_type}/run</code></p> <p>Launch an agent to perform a task asynchronously.</p> <p>Path Parameters:</p> Name Type Description <code>agent_type</code> string Agent type (e.g., <code>research</code>, <code>osint</code>) <p>Request Body:</p> Field Type Default Description <code>task</code> string -- Natural language task description (required) <code>max_steps</code> integer <code>10</code> Maximum reasoning/action steps <code>timeout</code> integer <code>300</code> Task timeout in seconds Research AgentOSINT AgentPython <pre><code>curl -X POST http://localhost:8001/v1/agents/research/run \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"task\": \"Research the company Acme Corp and summarize their main products\",\n    \"max_steps\": 10,\n    \"timeout\": 300\n  }'\n</code></pre> <pre><code>curl -X POST http://localhost:8001/v1/agents/osint/run \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"task\": \"Gather publicly available information about domain example.com\",\n    \"max_steps\": 15,\n    \"timeout\": 600\n  }'\n</code></pre> <pre><code>import requests\nimport time\n\n# Start agent task\nresponse = requests.post(\n    'http://localhost:8001/v1/agents/research/run',\n    json={\n        'task': 'Research the company Acme Corp',\n        'max_steps': 10,\n        'timeout': 300\n    }\n)\ntask_id = response.json()['data']['task_id']\n\n# Poll for results\nwhile True:\n    status = requests.get(\n        f'http://localhost:8001/v1/agents/tasks/{task_id}'\n    ).json()\n\n    if status['data']['status'] in ('completed', 'failed'):\n        break\n    time.sleep(5)\n\nprint(status['data']['result'])\n</code></pre>","tags":["AI","Automation"]},{"location":"api/agents/#list-agent-tasks","title":"List Agent Tasks","text":"<p><code>GET /v1/agents/tasks</code></p> <p>List all agent tasks and their current status.</p> <pre><code>curl http://localhost:8001/v1/agents/tasks\n</code></pre>","tags":["AI","Automation"]},{"location":"api/agents/#get-task-status","title":"Get Task Status","text":"<p><code>GET /v1/agents/tasks/{task_id}</code></p> <p>Retrieve the status and results of a specific agent task.</p> <pre><code>curl http://localhost:8001/v1/agents/tasks/task_abc123\n</code></pre>","tags":["AI","Automation"]},{"location":"api/agents/#cancel-task","title":"Cancel Task","text":"<p><code>POST /v1/agents/tasks/{task_id}/cancel</code></p> <p>Cancel a running agent task.</p> <pre><code>curl -X POST http://localhost:8001/v1/agents/tasks/task_abc123/cancel\n</code></pre>","tags":["AI","Automation"]},{"location":"api/agents/#list-agent-tools","title":"List Agent Tools","text":"<p><code>GET /v1/agents/tools</code></p> <p>List all tools available to agents for use during task execution.</p> <pre><code>curl http://localhost:8001/v1/agents/tools\n</code></pre>","tags":["AI","Automation"]},{"location":"api/automation/","title":"Automation &amp; Orchestration","text":"<p>The Automation API provides a comprehensive suite of tools for building automated workflows: URL change monitors with webhook notifications, background job management, event-driven automation, conditional triggers, and a plugin system for extensibility.</p>","tags":["Automation"]},{"location":"api/automation/#monitors","title":"Monitors","text":"<p>Monitors track URLs or search queries for changes over time, with optional webhook notifications for real-time alerts.</p> <p>Use Cases</p> <ul> <li>Brand managers -- Monitor brand mentions and competitor activity</li> <li>Security teams -- Track threat intelligence sources for changes</li> <li>Researchers -- Watch topics for new developments</li> <li>SEO teams -- Track ranking changes and competitor content</li> </ul>","tags":["Automation"]},{"location":"api/automation/#list-monitors","title":"List Monitors","text":"<p><code>GET /v1/monitors</code></p> <p>List all configured monitors with their status and run history.</p> <p>Parameters:</p> Name Type Default Description <code>limit</code> integer <code>20</code> Maximum monitors to return (1-100) <code>cursor</code> string -- Pagination cursor for next page cURLcURL + jq <pre><code># List all monitors\ncurl \"http://localhost:8001/v1/monitors\"\n\n# List with pagination\ncurl \"http://localhost:8001/v1/monitors?limit=50\"\n</code></pre> <pre><code># Check active monitors\ncurl -s \"http://localhost:8001/v1/monitors\" \\\n  | jq '.data.monitors[] | {name, status, last_run}'\n</code></pre> Response <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"monitors\": [\n      {\n        \"id\": \"c61ca73ff821\",\n        \"name\": \"HN Monitor\",\n        \"url\": \"https://news.ycombinator.com\",\n        \"query\": \"\",\n        \"interval\": 3600,\n        \"notify_on_new\": true,\n        \"created_at\": \"2026-01-31T03:13:40.346459Z\",\n        \"last_run\": null,\n        \"run_count\": 0,\n        \"status\": \"active\"\n      }\n    ]\n  },\n  \"meta\": {\n    \"request_id\": \"d376122a-7f1e-4652-b6a9-21eff2bab7bc\",\n    \"timestamp\": \"2026-01-31T03:13:45.654452Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 0\n  },\n  \"pagination\": {\n    \"next_cursor\": null,\n    \"has_more\": false,\n    \"returned\": 1\n  }\n}\n</code></pre> <p>Monitor Object Fields:</p> Field Type Description <code>id</code> string Unique monitor ID (12-char hex) <code>name</code> string Monitor display name <code>url</code> string URL being monitored <code>query</code> string Search query (if monitoring search results) <code>interval</code> integer Check interval in seconds <code>notify_on_new</code> boolean Whether to notify on new results <code>created_at</code> string Creation timestamp (ISO 8601) <code>last_run</code> string Last check timestamp (<code>null</code> if never run) <code>run_count</code> integer Number of times monitor has run <code>status</code> string Monitor status: <code>active</code>, <code>paused</code>, <code>deleted</code>","tags":["Automation"]},{"location":"api/automation/#create-monitor","title":"Create Monitor","text":"<p><code>POST /v1/monitors</code></p> <p>Create a new monitor to automatically track a URL or search query at a configurable interval.</p> <p>Request Body:</p> Field Type Required Default Description <code>url</code> string Yes -- URL to monitor <code>query</code> string No -- Search query to monitor <code>interval</code> integer No <code>3600</code> Check interval in seconds (300-604800) <code>name</code> string No Auto-generated Monitor name <code>notify_on_new</code> boolean No <code>true</code> Notify on new results <p>Headers:</p> Name Description <code>Idempotency-Key</code> Prevent duplicate monitor creation cURLWith Idempotency KeyPython <pre><code># Monitor a competitor's blog for new posts\ncurl -X POST http://localhost:8001/v1/monitors \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\": \"https://competitor.com/blog\", \"name\": \"Competitor Blog\", \"interval\": 3600}'\n</code></pre> <pre><code># Monitor a GitHub releases page (idempotent)\ncurl -X POST http://localhost:8001/v1/monitors \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Idempotency-Key: github-react-releases\" \\\n  -d '{\"url\": \"https://github.com/facebook/react/releases\", \"name\": \"React Releases\", \"interval\": 21600}'\n</code></pre> <pre><code>import requests\n\ndef create_brand_monitors(brand_name, check_interval=3600):\n    sources = [\n        ('Google News', f'https://news.google.com/search?q=\"{brand_name}\"'),\n        ('Twitter', f'https://twitter.com/search?q=\"{brand_name}\"'),\n        ('Reddit', f'https://reddit.com/search?q=\"{brand_name}\"'),\n    ]\n\n    monitors = []\n    for name, url in sources:\n        response = requests.post(\n            'http://localhost:8001/v1/monitors',\n            json={\n                'url': url,\n                'name': f'{brand_name} - {name}',\n                'interval': check_interval,\n                'notify_on_new': True\n            },\n            headers={\n                'Idempotency-Key': f'brand-{brand_name.lower()}-{name.lower()}'\n            }\n        )\n        monitors.append(response.json()['data'])\n\n    return monitors\n</code></pre> Response <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"id\": \"c61ca73ff821\",\n    \"name\": \"HN Monitor\",\n    \"url\": \"https://news.ycombinator.com\",\n    \"interval\": 3600,\n    \"status\": \"active\",\n    \"created_at\": \"2026-01-31T03:13:40.346459Z\"\n  },\n  \"meta\": {\n    \"request_id\": \"461c2931-9333-4347-a65a-24a28aa56d4d\",\n    \"timestamp\": \"2026-01-31T03:13:40.346554Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 1\n  }\n}\n</code></pre>","tags":["Automation"]},{"location":"api/automation/#get-monitor-details","title":"Get Monitor Details","text":"<p><code>GET /v1/monitors/{monitor_id}</code></p> <p>Retrieve detailed information about a specific monitor, including its webhook configuration and recent results.</p> cURL <pre><code>curl \"http://localhost:8001/v1/monitors/0dd270a34f9b\"\n</code></pre> Response <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"monitor\": {\n      \"id\": \"0dd270a34f9b\",\n      \"name\": \"HN Monitor\",\n      \"url\": \"https://news.ycombinator.com\",\n      \"query\": \"\",\n      \"interval\": 3600,\n      \"notify_on_new\": true,\n      \"created_at\": \"2026-01-31T03:00:00Z\",\n      \"last_run\": null,\n      \"run_count\": 0,\n      \"status\": \"active\"\n    },\n    \"webhook\": null,\n    \"recent_results\": []\n  },\n  \"meta\": {\n    \"request_id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\",\n    \"timestamp\": \"2026-01-31T03:00:00.000000Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 2\n  }\n}\n</code></pre>","tags":["Automation"]},{"location":"api/automation/#delete-monitor","title":"Delete Monitor","text":"<p><code>DELETE /v1/monitors/{monitor_id}</code></p> <p>Remove a monitor that is no longer needed.</p> cURL <pre><code>curl -X DELETE \"http://localhost:8001/v1/monitors/0dd270a34f9b\"\n</code></pre> Response <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"id\": \"0dd270a34f9b\",\n    \"status\": \"deleted\"\n  },\n  \"meta\": {\n    \"request_id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\",\n    \"timestamp\": \"2026-01-31T03:00:00.000000Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 1\n  }\n}\n</code></pre>","tags":["Automation"]},{"location":"api/automation/#configure-webhook","title":"Configure Webhook","text":"<p><code>POST /v1/monitors/{monitor_id}/webhook</code></p> <p>Attach a webhook to a monitor for real-time push notifications when changes are detected.</p> <p>HMAC Verification</p> <p>Use the <code>secret</code> field to enable HMAC-SHA256 signatures on webhook payloads. The signature is sent in the <code>X-Signature</code> header as <code>sha256=&lt;hex-digest&gt;</code>.</p> <p>Request Body:</p> Field Type Default Description <code>url</code> string -- Webhook URL (required) <code>events</code> array <code>[\"change\", \"error\"]</code> Events to notify on <code>secret</code> string -- HMAC secret for payload signatures <code>verify</code> boolean <code>true</code> Verify webhook endpoint is reachable Slack IntegrationSecure WebhookPython Webhook Handler <pre><code>curl -X POST http://localhost:8001/v1/monitors/abc123/webhook \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"url\": \"https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK\",\n    \"events\": [\"change\"],\n    \"verify\": false\n  }'\n</code></pre> <pre><code>curl -X POST http://localhost:8001/v1/monitors/abc123/webhook \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"url\": \"https://your-server.com/webhooks/monitor\",\n    \"events\": [\"change\", \"error\"],\n    \"secret\": \"your-secret-key-32-chars-min\",\n    \"verify\": true\n  }'\n</code></pre> <pre><code>import hmac\nimport hashlib\nfrom flask import Flask, request, jsonify\n\napp = Flask(__name__)\nWEBHOOK_SECRET = 'your-secret-key-32-chars-min'\n\n@app.route('/webhooks/monitor', methods=['POST'])\ndef handle_webhook():\n    # Verify signature\n    signature = request.headers.get('X-Signature', '')\n    payload = request.get_data(as_text=True)\n\n    expected = 'sha256=' + hmac.new(\n        WEBHOOK_SECRET.encode(),\n        payload.encode(),\n        hashlib.sha256\n    ).hexdigest()\n\n    if not hmac.compare_digest(signature, expected):\n        return jsonify({'error': 'Invalid signature'}), 401\n\n    # Process event\n    event = request.json\n    if event['event'] == 'change':\n        notify_team(f\"Monitor {event['monitor_id']} detected changes!\")\n\n    return jsonify({'received': True})\n</code></pre> Response <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"monitor_id\": \"0dd270a34f9b\",\n    \"webhook_url\": \"https://webhook.site/test\",\n    \"events\": [\"change\"],\n    \"status\": \"configured\"\n  },\n  \"meta\": {\n    \"request_id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\",\n    \"timestamp\": \"2026-01-31T03:00:00.000000Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 50\n  }\n}\n</code></pre> <p>Webhook Payload Format:</p> <pre><code>{\n  \"event\": \"change\",\n  \"monitor_id\": \"0dd270a34f9b\",\n  \"timestamp\": \"2026-01-31T04:00:00Z\",\n  \"data\": {\n    \"new_results\": 5,\n    \"changes\": [\"...\"]\n  }\n}\n</code></pre> <p>Webhook Signature Header:</p> <pre><code>X-Signature: sha256=&lt;hmac-sha256-hex&gt;\n</code></pre>","tags":["Automation"]},{"location":"api/automation/#jobs","title":"Jobs","text":"<p>Jobs represent asynchronous background operations such as batch content extraction. Jobs auto-expire after 24 hours.</p> <p>Job Lifecycle</p> <p>Jobs transition through states: <code>pending</code> -&gt; <code>running</code> -&gt; <code>completed</code> | <code>failed</code> | <code>cancelled</code></p>","tags":["Automation"]},{"location":"api/automation/#get-job-status","title":"Get Job Status","text":"<p><code>GET /v1/jobs/{job_id}</code></p> <p>Poll for the status of an asynchronous batch operation.</p> cURLPolling Script (Bash)Python <pre><code>curl \"http://localhost:8001/v1/jobs/e31f008a-303f-4ec2-a994-e26cf4ce677a\"\n</code></pre> <pre><code>JOB_ID=\"e31f008a-303f-4ec2-a994-e26cf4ce677a\"\nwhile true; do\n  STATUS=$(curl -s \"http://localhost:8001/v1/jobs/$JOB_ID\" | jq -r '.data.status')\n  echo \"Status: $STATUS\"\n  [ \"$STATUS\" = \"completed\" ] &amp;&amp; break\n  [ \"$STATUS\" = \"failed\" ] &amp;&amp; exit 1\n  sleep 5\ndone\ncurl \"http://localhost:8001/v1/jobs/$JOB_ID\" | jq '.data.results'\n</code></pre> <pre><code>import requests\nimport time\n\ndef wait_for_job(job_id, timeout=300, poll_interval=5):\n    start = time.time()\n    while time.time() - start &lt; timeout:\n        response = requests.get(f'http://localhost:8001/v1/jobs/{job_id}')\n        data = response.json()['data']\n\n        if data['status'] == 'completed':\n            return data['results']\n        elif data['status'] == 'failed':\n            raise Exception(f\"Job failed: {data.get('error')}\")\n        elif data['status'] == 'cancelled':\n            raise Exception(\"Job was cancelled\")\n\n        print(f\"Progress: {data['completed']}/{data['total']}\")\n        time.sleep(poll_interval)\n\n    raise TimeoutError(f\"Job {job_id} did not complete in {timeout}s\")\n</code></pre> Response (Pending) <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"job_id\": \"e31f008a-303f-4ec2-a994-e26cf4ce677a\",\n    \"status\": \"pending\",\n    \"created\": \"2026-01-31T03:00:00\",\n    \"total\": 5,\n    \"completed\": 2,\n    \"failed\": 0\n  },\n  \"meta\": {\n    \"request_id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\",\n    \"timestamp\": \"2026-01-31T03:00:00.000000Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 1\n  }\n}\n</code></pre> Response (Completed) <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"job_id\": \"e31f008a-303f-4ec2-a994-e26cf4ce677a\",\n    \"status\": \"completed\",\n    \"created\": \"2026-01-31T03:00:00\",\n    \"total\": 2,\n    \"completed\": 2,\n    \"failed\": 0,\n    \"results\": [\n      {\"url\": \"https://example.com\", \"status\": \"success\", \"content_length\": 127},\n      {\"url\": \"https://example.org\", \"status\": \"success\", \"content_length\": 127}\n    ],\n    \"urls\": [\"https://example.com\", \"https://example.org\"]\n  },\n  \"meta\": {\n    \"request_id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\",\n    \"timestamp\": \"2026-01-31T03:01:00.000000Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 1\n  }\n}\n</code></pre>","tags":["Automation"]},{"location":"api/automation/#cancel-job","title":"Cancel Job","text":"<p><code>DELETE /v1/jobs/{job_id}</code></p> <p>Cancel a pending or running job. Already-completed jobs cannot be cancelled.</p> cURLCancel All Pending (Bash) <pre><code># Cancel a single job\ncurl -X DELETE \"http://localhost:8001/v1/jobs/e31f008a-303f-4ec2-a994-e26cf4ce677a\"\n</code></pre> <pre><code>for job in $(curl -s \"http://localhost:8001/v1/jobs\" \\\n  | jq -r '.data.jobs[] | select(.status==\"pending\") | .job_id'); do\n  curl -X DELETE \"http://localhost:8001/v1/jobs/$job\"\ndone\n</code></pre> Response <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"job_id\": \"e31f008a-303f-4ec2-a994-e26cf4ce677a\",\n    \"status\": \"cancelled\"\n  },\n  \"meta\": {\n    \"request_id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\",\n    \"timestamp\": \"2026-01-31T03:00:00.000000Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 1\n  }\n}\n</code></pre> <p>Already Completed</p> <p>Attempting to cancel a completed job returns an error:</p> <pre><code>{\n  \"success\": false,\n  \"error\": {\n    \"code\": \"INVALID_PARAM_VALUE\",\n    \"message\": \"Cannot cancel completed job\",\n    \"category\": \"client\"\n  }\n}\n</code></pre>","tags":["Automation"]},{"location":"api/automation/#events","title":"Events","text":"<p>Event-driven automation system for publishing and consuming custom events.</p>","tags":["Automation"]},{"location":"api/automation/#list-events","title":"List Events","text":"<p><code>GET /v1/events</code></p> <p>List recent events in the system.</p> <pre><code>curl \"http://localhost:8001/v1/events\"\n</code></pre>","tags":["Automation"]},{"location":"api/automation/#emit-event","title":"Emit Event","text":"<p><code>POST /v1/events/emit</code></p> <p>Publish a custom event into the event bus.</p> <p>Request Body:</p> <pre><code>{\n  \"event_type\": \"custom.alert\",\n  \"data\": {\"message\": \"Test alert\"}\n}\n</code></pre> cURL <pre><code>curl -X POST http://localhost:8001/v1/events/emit \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"event_type\": \"custom.alert\", \"data\": {\"message\": \"Test alert\"}}'\n</code></pre>","tags":["Automation"]},{"location":"api/automation/#event-statistics","title":"Event Statistics","text":"<p><code>GET /v1/events/stats</code></p> <p>Retrieve aggregated event statistics.</p> <pre><code>curl \"http://localhost:8001/v1/events/stats\"\n</code></pre>","tags":["Automation"]},{"location":"api/automation/#triggers","title":"Triggers","text":"<p>Configure automation rules that execute actions when specific event conditions are met.</p> <p>How Triggers Work</p> <p>A trigger watches for events of a given <code>event_type</code>, evaluates its <code>conditions</code> against the event data, and executes <code>actions</code> (such as webhooks) when conditions match.</p>","tags":["Automation"]},{"location":"api/automation/#list-triggers","title":"List Triggers","text":"<p><code>GET /v1/triggers</code></p> <p>List all configured triggers.</p> <pre><code>curl \"http://localhost:8001/v1/triggers\"\n</code></pre>","tags":["Automation"]},{"location":"api/automation/#create-trigger","title":"Create Trigger","text":"<p><code>POST /v1/triggers</code></p> <p>Create a new automation trigger.</p> <p>Request Body:</p> <pre><code>{\n  \"name\": \"Alert on malware\",\n  \"event_type\": \"scan.completed\",\n  \"conditions\": {\"verdict\": \"malicious\"},\n  \"actions\": [{\"type\": \"webhook\", \"url\": \"https://...\"}]\n}\n</code></pre> cURL <pre><code>curl -X POST http://localhost:8001/v1/triggers \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"Alert on malware\",\n    \"event_type\": \"scan.completed\",\n    \"conditions\": {\"verdict\": \"malicious\"},\n    \"actions\": [{\"type\": \"webhook\", \"url\": \"https://your-server.com/alerts\"}]\n  }'\n</code></pre>","tags":["Automation"]},{"location":"api/automation/#get-trigger-details","title":"Get Trigger Details","text":"<p><code>GET /v1/triggers/{trigger_id}</code></p> <p>Retrieve configuration details for a specific trigger.</p> <pre><code>curl \"http://localhost:8001/v1/triggers/trg_abc123\"\n</code></pre>","tags":["Automation"]},{"location":"api/automation/#update-trigger","title":"Update Trigger","text":"<p><code>PUT /v1/triggers/{trigger_id}</code></p> <p>Update an existing trigger's configuration.</p> <pre><code>curl -X PUT http://localhost:8001/v1/triggers/trg_abc123 \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"Updated trigger name\"}'\n</code></pre>","tags":["Automation"]},{"location":"api/automation/#delete-trigger","title":"Delete Trigger","text":"<p><code>DELETE /v1/triggers/{trigger_id}</code></p> <p>Remove a trigger.</p> <pre><code>curl -X DELETE \"http://localhost:8001/v1/triggers/trg_abc123\"\n</code></pre>","tags":["Automation"]},{"location":"api/automation/#test-trigger","title":"Test Trigger","text":"<p><code>POST /v1/triggers/test</code></p> <p>Test trigger conditions against sample event data without actually firing actions.</p> <pre><code>curl -X POST http://localhost:8001/v1/triggers/test \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"event_type\": \"scan.completed\", \"data\": {\"verdict\": \"malicious\"}}'\n</code></pre>","tags":["Automation"]},{"location":"api/automation/#plugins","title":"Plugins","text":"<p>Extensible plugin system built on the Pluggy framework. Plugins can extend API functionality at runtime.</p>","tags":["Automation"]},{"location":"api/automation/#list-plugins","title":"List Plugins","text":"<p><code>GET /v1/plugins</code></p> <p>List all loaded plugins and their status.</p> <pre><code>curl \"http://localhost:8001/v1/plugins\"\n</code></pre>","tags":["Automation"]},{"location":"api/automation/#load-plugin","title":"Load Plugin","text":"<p><code>POST /v1/plugins/load</code></p> <p>Load a new plugin into the system.</p> <pre><code>curl -X POST http://localhost:8001/v1/plugins/load \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"path\": \"/path/to/plugin.py\"}'\n</code></pre>","tags":["Automation"]},{"location":"api/automation/#reload-all-plugins","title":"Reload All Plugins","text":"<p><code>POST /v1/plugins/reload</code></p> <p>Reload all currently loaded plugins.</p> <pre><code>curl -X POST http://localhost:8001/v1/plugins/reload\n</code></pre>","tags":["Automation"]},{"location":"api/automation/#get-plugin-details","title":"Get Plugin Details","text":"<p><code>GET /v1/plugins/{name}</code></p> <p>Get detailed information about a specific plugin.</p> <pre><code>curl \"http://localhost:8001/v1/plugins/my-plugin\"\n</code></pre>","tags":["Automation"]},{"location":"api/automation/#enable-plugin","title":"Enable Plugin","text":"<p><code>POST /v1/plugins/{name}/enable</code></p> <p>Enable a disabled plugin.</p> <pre><code>curl -X POST http://localhost:8001/v1/plugins/my-plugin/enable\n</code></pre>","tags":["Automation"]},{"location":"api/automation/#disable-plugin","title":"Disable Plugin","text":"<p><code>POST /v1/plugins/{name}/disable</code></p> <p>Disable a plugin without unloading it.</p> <pre><code>curl -X POST http://localhost:8001/v1/plugins/my-plugin/disable\n</code></pre>","tags":["Automation"]},{"location":"api/automation/#reload-plugin","title":"Reload Plugin","text":"<p><code>POST /v1/plugins/{name}/reload</code></p> <p>Reload a specific plugin to pick up code changes.</p> <pre><code>curl -X POST http://localhost:8001/v1/plugins/my-plugin/reload\n</code></pre>","tags":["Automation"]},{"location":"api/automation/#unload-plugin","title":"Unload Plugin","text":"<p><code>DELETE /v1/plugins/{name}</code></p> <p>Completely unload a plugin from the system.</p> <pre><code>curl -X DELETE \"http://localhost:8001/v1/plugins/my-plugin\"\n</code></pre>","tags":["Automation"]},{"location":"api/breach-spider/","title":"Breach Spider","text":"<p>Extract all 90,000+ breach notification records from the Data Breach Chronology database (databreachchronology.org).</p>","tags":["Data","OSINT"]},{"location":"api/breach-spider/#overview","title":"Overview","text":"<p>The Breach Spider is a parallel extraction engine built on Camoufox anti-detect browsers. It partitions the database by state and organization type, rotates through 100 residential proxies, and deduplicates results by record UUID -- delivering the complete dataset in approximately 38 minutes with zero rate limits.</p> <p>Observed Performance</p> Metric Value Total records 90,278 Extraction time ~38 minutes API calls 974 Rate limits hit 0 Errors 0 Proxy rotations 47 Speed ~2.6 s per page (Turnstile refresh + API call)","tags":["Data","OSINT"]},{"location":"api/breach-spider/#key-features","title":"Key Features","text":"Feature Description Parallel Camoufox browsers Multiple anti-detect Firefox instances run concurrently 100 residential proxy rotation Proactive rotation at 4K records, before the 5K rate limit Cloudflare Turnstile bypass <code>page.expect_response()</code> at the browser protocol level State-based partitioning 53 states + UNKN broken by 8 org types = 61 zero-overlap partitions Deduplication By record UUID -- no duplicates even with overlapping strategies Export JSON or CSV download","tags":["Data","OSINT"]},{"location":"api/breach-spider/#endpoints","title":"Endpoints","text":"","tags":["Data","OSINT"]},{"location":"api/breach-spider/#post-v1breachesspiderparallel-start-parallel-extraction","title":"POST <code>/v1/breaches/spider/parallel</code> -- Start Parallel Extraction","text":"<p>Run parallel breach extraction using Camoufox browsers. This is the recommended endpoint.</p>","tags":["Data","OSINT"]},{"location":"api/breach-spider/#query-parameters","title":"Query Parameters","text":"Parameter Type Default Description <code>strategy</code> string <code>states</code> Partition strategy (see table below) <code>max_records</code> int <code>null</code> Maximum records to collect (<code>null</code> = unlimited) <code>max_workers</code> int 3 Parallel browser instances (1-20) <code>proxy_file</code> string auto Path to proxy file (auto-detects <code>/app/proxies.txt</code>) <code>proxy_key</code> string <code>null</code> Legacy Webshare API key <code>background</code> bool <code>true</code> Run in background (recommended)","tags":["Data","OSINT"]},{"location":"api/breach-spider/#partition-strategies","title":"Partition Strategies","text":"Strategy Partitions Description <code>states</code> 61 53 states + UNKN broken by 8 org types. Recommended. Zero overlap. <code>org_types</code> 8 By organization type (BSO, BSF, MED, UNKN, BSR, EDU, NGO, GOV) <code>states_and_types</code> 385+ Full grid: state x org_type. Maximum parallelism. <code>search</code> 159 Search terms (has overlap, deduped by UUID) <code>search_extended</code> 360+ Search terms + letter pair combinations <code>full</code> 220+ States + UNKN breakdown + search terms. Maximum coverage.","tags":["Data","OSINT"]},{"location":"api/breach-spider/#examples","title":"Examples","text":"Default (recommended)Quick test (500 records)Maximum parallelism <pre><code>curl -X POST \"http://localhost:8001/v1/breaches/spider/parallel\"\n</code></pre> <pre><code>curl -X POST \"http://localhost:8001/v1/breaches/spider/parallel?strategy=states&amp;max_workers=1&amp;max_records=500\"\n</code></pre> <pre><code>curl -X POST \"http://localhost:8001/v1/breaches/spider/parallel?strategy=states&amp;max_workers=5\"\n</code></pre>","tags":["Data","OSINT"]},{"location":"api/breach-spider/#response","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"message\": \"Parallel spider started in background\",\n  \"data\": {\n    \"strategy\": \"states\", // (1)\n    \"max_workers\": 3, // (2)\n    \"proxy\": \"residential_file\", // (3)\n    \"max_records\": null,\n    \"check_status\": \"/v1/breaches/spider/status\"\n  }\n}\n</code></pre> <ol> <li>The partition strategy used to divide the database for parallel extraction. <code>\"states\"</code> splits by US state + UNKN, broken into org-type sub-partitions for zero overlap.</li> <li>Number of concurrent Camoufox anti-detect browser instances. Higher values speed up extraction but consume more memory (~500 MB per worker). Max 20.</li> <li>Proxy source in use. <code>\"residential_file\"</code> means proxies were loaded from the local file (<code>/app/proxies.txt</code>). Proxies rotate proactively at 4K records to stay below the 5K rate-limit threshold.</li> </ol>","tags":["Data","OSINT"]},{"location":"api/breach-spider/#post-v1breachesspiderextract-all-full-extraction","title":"POST <code>/v1/breaches/spider/extract-all</code> -- Full Extraction","text":"<p>Convenience wrapper that runs <code>/spider/parallel</code> with the <code>full</code> strategy.</p>","tags":["Data","OSINT"]},{"location":"api/breach-spider/#query-parameters_1","title":"Query Parameters","text":"Parameter Type Default Description <code>max_workers</code> int 3 Parallel browser instances <code>proxy_file</code> string auto Path to proxy file <code>proxy_key</code> string <code>null</code> Legacy Webshare API key","tags":["Data","OSINT"]},{"location":"api/breach-spider/#example","title":"Example","text":"<pre><code>curl -X POST \"http://localhost:8001/v1/breaches/spider/extract-all\"\n</code></pre>","tags":["Data","OSINT"]},{"location":"api/breach-spider/#response_1","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"message\": \"Full extraction started in background\",\n  \"data\": {\n    \"strategy\": \"full\",\n    \"max_workers\": 3,\n    \"proxy\": \"residential_file\",\n    \"partitions\": \"53 states + UNKN x 7 org_types + 159 search terms\",\n    \"target_records\": \"90,000+\",\n    \"proxy_capacity\": \"100 proxies x 5K = 500K records\",\n    \"check_status\": \"/v1/breaches/spider/status\",\n    \"get_records\": \"/v1/breaches/spider/records\",\n    \"export\": \"/v1/breaches/spider/export\"\n  }\n}\n</code></pre>","tags":["Data","OSINT"]},{"location":"api/breach-spider/#get-v1breachesspiderstatus-check-status","title":"GET <code>/v1/breaches/spider/status</code> -- Check Status","text":"<p>Get the current spider run status. Poll this while extraction runs in the background.</p>","tags":["Data","OSINT"]},{"location":"api/breach-spider/#example_1","title":"Example","text":"<pre><code>curl \"http://localhost:8001/v1/breaches/spider/status\"\n</code></pre>","tags":["Data","OSINT"]},{"location":"api/breach-spider/#status-values","title":"Status Values","text":"Status Description <code>never_run</code> Spider has not been run yet <code>running</code> Extraction is in progress <code>completed</code> Extraction finished successfully <code>failed</code> Extraction encountered a fatal error","tags":["Data","OSINT"]},{"location":"api/breach-spider/#response-running","title":"Response (running)","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"status\": \"running\",\n    \"started_at\": \"2026-02-11T17:11:03.352824\",\n    \"strategy\": \"states\",\n    \"max_workers\": \"3\",\n    \"proxy\": \"residential_file\",\n    \"records_collected\": \"0\"\n  }\n}\n</code></pre>","tags":["Data","OSINT"]},{"location":"api/breach-spider/#response-completed","title":"Response (completed)","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"status\": \"completed\",\n    \"started_at\": \"2026-02-11T17:11:03.352824\",\n    \"completed_at\": \"2026-02-11T17:48:54.355182\",\n    \"records_collected\": \"90278\",\n    \"unique_records\": \"90278\",\n    \"rate_limits_hit\": \"0\"\n  }\n}\n</code></pre>","tags":["Data","OSINT"]},{"location":"api/breach-spider/#get-v1breachesspiderrecords-get-records","title":"GET <code>/v1/breaches/spider/records</code> -- Get Records","text":"<p>Get collected breach records with optional filtering.</p>","tags":["Data","OSINT"]},{"location":"api/breach-spider/#query-parameters_2","title":"Query Parameters","text":"Parameter Type Default Description <code>limit</code> int 100 Records per page <code>offset</code> int 0 Pagination offset <code>state</code> string <code>null</code> Filter by state code (e.g. <code>CA</code>, <code>NY</code>, <code>TX</code>) <code>org_type</code> string <code>null</code> Filter by organization type (e.g. <code>MED</code>, <code>BSO</code>, <code>EDU</code>) <code>search</code> string <code>null</code> Search organization name (case-insensitive substring)","tags":["Data","OSINT"]},{"location":"api/breach-spider/#examples_1","title":"Examples","text":"First 10 recordsHealthcare in CaliforniaSearch by name <pre><code>curl \"http://localhost:8001/v1/breaches/spider/records?limit=10\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/breaches/spider/records?state=CA&amp;org_type=MED\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/breaches/spider/records?search=hospital&amp;limit=50\"\n</code></pre>","tags":["Data","OSINT"]},{"location":"api/breach-spider/#response_2","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"records\": [\n      {\n        \"id\": \"81f4c5a9-fdb6-5395-9b6f-f21a609b6c4d\",\n        \"organization\": \"Phreesia, Inc.\",\n        \"reported_date\": \"2026-02-06\",\n        \"breach_date\": \"2025-08-17\",\n        \"breach_type\": \"HACK\",\n        \"total_affected\": null,\n        \"organization_type\": \"BSO\",\n        \"location_state\": \"UNKN\",\n        \"source\": \"CA\",\n        \"raw_data\": {\n          \"incident_details\": \"...\",\n          \"information_affected\": \"...\",\n          \"source_url\": \"https://oag.ca.gov/ecrime/databreach/reports/sb24-618283\",\n          \"notification_url_original\": \"https://oag.ca.gov/system/files/Template%20Individual%20Notice.pdf\",\n          \"pdf_contents_cleaned\": \"...\"\n        }\n      }\n    ],\n    \"total\": 90278,\n    \"limit\": 10,\n    \"offset\": 0\n  }\n}\n</code></pre>","tags":["Data","OSINT"]},{"location":"api/breach-spider/#record-fields","title":"Record Fields","text":"Field Type Description <code>id</code> string Unique record UUID <code>organization</code> string Normalised organization name <code>reported_date</code> string Date reported to AG (<code>YYYY-MM-DD</code>) <code>breach_date</code> string Date breach occurred <code>breach_type</code> string <code>HACK</code>, <code>PHYS</code>, <code>PORT</code>, <code>STAT</code>, <code>DISC</code>, <code>UNKN</code> <code>total_affected</code> int / null Number of individuals affected <code>organization_type</code> string <code>BSO</code>, <code>BSF</code>, <code>MED</code>, <code>EDU</code>, <code>GOV</code>, <code>NGO</code>, <code>BSR</code>, <code>UNKN</code> <code>location_state</code> string US state code or <code>UNKN</code> <code>source</code> string Source AG state code <code>raw_data</code> object Full API response with incident details, PDF text, URLs","tags":["Data","OSINT"]},{"location":"api/breach-spider/#organization-types","title":"Organization Types","text":"Code Description Approx. Count <code>BSO</code> Other Businesses ~25,267 <code>BSF</code> Financial Services ~21,333 <code>MED</code> Healthcare ~17,201 <code>UNKN</code> Unknown ~9,683 <code>BSR</code> Retail ~5,677 <code>EDU</code> Education ~4,849 <code>NGO</code> Non-Profit ~3,973 <code>GOV</code> Government ~2,579","tags":["Data","OSINT"]},{"location":"api/breach-spider/#get-v1breachesspiderstats-run-statistics","title":"GET <code>/v1/breaches/spider/stats</code> -- Run Statistics","text":"<p>Get detailed statistics from the last spider run.</p>","tags":["Data","OSINT"]},{"location":"api/breach-spider/#example_2","title":"Example","text":"<pre><code>curl \"http://localhost:8001/v1/breaches/spider/stats\"\n</code></pre>","tags":["Data","OSINT"]},{"location":"api/breach-spider/#response_3","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"total_api_calls\": 974,\n    \"total_records_seen\": 93759,\n    \"unique_records\": 90278,\n    \"rate_limits_hit\": 0,\n    \"proxy_rotations\": 47,\n    \"errors\": [],\n    \"by_partition\": {\n      \"state=CA\": 4424,\n      \"state=NY\": 2835,\n      \"state=TX\": 2703,\n      \"state=UNKN, type=BSF\": 1678,\n      \"state=UNKN, type=BSO\": 3962,\n      \"state=UNKN, type=MED\": 3538\n    },\n    \"proxy_stats\": {\n      \"total_proxies\": 100,\n      \"usage_by_ip\": {\"82.23.56.249\": 4412},\n      \"total_records_via_proxy\": 138809\n    }\n  }\n}\n</code></pre>","tags":["Data","OSINT"]},{"location":"api/breach-spider/#get-v1breachesspiderexport-export-records","title":"GET <code>/v1/breaches/spider/export</code> -- Export Records","text":"<p>Export all collected records as a JSON or CSV download.</p>","tags":["Data","OSINT"]},{"location":"api/breach-spider/#query-parameters_3","title":"Query Parameters","text":"Parameter Type Default Description <code>format</code> string <code>json</code> Export format: <code>json</code> or <code>csv</code> <p>CSV columns: <code>id</code>, <code>organization</code>, <code>reported_date</code>, <code>breach_date</code>, <code>breach_type</code>, <code>total_affected</code>, <code>organization_type</code>, <code>location_state</code>, <code>source</code></p>","tags":["Data","OSINT"]},{"location":"api/breach-spider/#examples_2","title":"Examples","text":"CSVJSON <pre><code>curl \"http://localhost:8001/v1/breaches/spider/export?format=csv\" -o breach_records.csv\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/breaches/spider/export?format=json\" -o breach_records.json\n</code></pre>","tags":["Data","OSINT"]},{"location":"api/breach-spider/#delete-v1breachesspiderrecords-clear-records","title":"DELETE <code>/v1/breaches/spider/records</code> -- Clear Records","text":"<p>Clear all stored breach records, stats, and status. Use before starting a fresh extraction.</p> <pre><code>curl -X DELETE \"http://localhost:8001/v1/breaches/spider/records\"\n</code></pre> <pre><code>{\n  \"success\": true,\n  \"message\": \"All breach records cleared\"\n}\n</code></pre>","tags":["Data","OSINT"]},{"location":"api/breach-spider/#get-v1breachesdatabasestats-live-database-stats","title":"GET <code>/v1/breaches/database/stats</code> -- Live Database Stats","text":"<p>Fetch live statistics directly from the Data Breach Chronology database. Opens a Camoufox browser to get the current record count.</p> <pre><code>curl \"http://localhost:8001/v1/breaches/database/stats\"\n</code></pre> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"total_records\": 90562,\n    \"organization_types\": [\n      {\"type\": \"BSO\", \"count\": 25267},\n      {\"type\": \"BSF\", \"count\": 21333},\n      {\"type\": \"MED\", \"count\": 17201}\n    ],\n    \"fetched_at\": \"2026-02-11T17:00:00.000000\"\n  }\n}\n</code></pre>","tags":["Data","OSINT"]},{"location":"api/breach-spider/#post-v1breachesspiderrun-deprecated","title":"POST <code>/v1/breaches/spider/run</code> (deprecated)","text":"<p>Deprecated</p> <p>This legacy endpoint is superseded by <code>/v1/breaches/spider/parallel</code>. Use the parallel endpoint for all new integrations.</p>","tags":["Data","OSINT"]},{"location":"api/breach-spider/#typical-workflow","title":"Typical Workflow","text":"<p>The recommended workflow is: clear previous data, start extraction, poll status, review stats, and export.</p> <pre><code># 1. Clear any previous data\ncurl -X DELETE \"http://localhost:8001/v1/breaches/spider/records\"\n\n# 2. Start extraction\ncurl -X POST \"http://localhost:8001/v1/breaches/spider/parallel?strategy=states\"\n\n# 3. Monitor progress (poll every 30s)\nwatch -n 30 'curl -s http://localhost:8001/v1/breaches/spider/status | python3 -m json.tool'\n\n# 4. Check stats when complete\ncurl \"http://localhost:8001/v1/breaches/spider/stats\"\n\n# 5. Export as CSV\ncurl \"http://localhost:8001/v1/breaches/spider/export?format=csv\" -o breaches.csv\n\n# 6. Query specific records\ncurl \"http://localhost:8001/v1/breaches/spider/records?state=CA&amp;org_type=MED&amp;limit=50\"\n</code></pre>","tags":["Data","OSINT"]},{"location":"api/breach-spider/#python-integration-example","title":"Python Integration Example","text":"<pre><code>import requests\nimport time\n\nBASE = \"http://localhost:8001\"\n\n# Start extraction\nrequests.post(f\"{BASE}/v1/breaches/spider/parallel\", params={\"strategy\": \"states\"})\n\n# Poll until complete\nwhile True:\n    status = requests.get(f\"{BASE}/v1/breaches/spider/status\").json()[\"data\"]\n    print(f\"Status: {status['status']}, Records: {status.get('records_collected', 0)}\")\n    if status[\"status\"] in (\"completed\", \"failed\"):\n        break\n    time.sleep(30)\n\n# Get all healthcare breaches\nrecords = requests.get(f\"{BASE}/v1/breaches/spider/records\", params={\n    \"org_type\": \"MED\",\n    \"limit\": 1000,\n}).json()[\"data\"]\n\nprint(f\"Healthcare breaches: {records['total']}\")\nfor r in records[\"records\"][:5]:\n    print(f\"  {r['reported_date']} | {r['organization']} | {r['breach_type']}\")\n</code></pre>","tags":["Data","OSINT"]},{"location":"api/content/","title":"Content Endpoints","text":"<p>The Content API provides powerful tools for extracting, transforming, and anonymizing web content. From converting web pages to clean text or markdown, to generating screenshots, discovering CSS/XPath selectors, and stripping PII from documents -- these endpoints form a complete content processing pipeline.</p>","tags":["Data"]},{"location":"api/content/#capabilities-overview","title":"Capabilities Overview","text":"Capability Endpoint Description Text Extraction <code>POST /v1/content/extract</code> Extract clean text from web pages and PDFs Batch Extraction <code>POST /v1/content/extract/batch</code> Queue bulk URL extraction as async jobs Screenshots <code>GET /v1/content/screenshot</code> Generate visual previews via multiple services Selector Discovery <code>GET /v1/content/selectors</code> Extract XPath, CSS, and Playwright selectors Markdown Conversion <code>GET /v1/content/markdown</code> Convert web pages to clean markdown PII Anonymization <code>POST /v1/content/anonymize</code> Detect and anonymize personally identifiable information URL Anonymization <code>POST /v1/content/anonymize/url</code> Extract content from a URL and anonymize PII in one step PII Entity Types <code>GET /v1/content/pii/entities</code> List all supported PII entity types","tags":["Data"]},{"location":"api/content/#post-v1contentextract","title":"<code>POST /v1/content/extract</code>","text":"<p>Extract text content from a URL. Removes HTML boilerplate and returns the main content along with NLP-extracted entities and an optional LLM summary.</p> <p>Best For</p> <p>Converting web pages to clean text for analysis, storage, or processing.</p> <p>Target Audience</p> <ul> <li>Data engineers -- Building content pipelines and datasets</li> <li>Researchers -- Extracting article text for analysis</li> <li>AI/ML teams -- Preparing training data from web sources</li> <li>Content aggregators -- Building read-it-later or RSS-like services</li> <li>SEO analysts -- Extracting and analyzing competitor content</li> </ul> <p>Key features:</p> <ul> <li>Removes HTML boilerplate and extracts main content</li> <li>Handles PDFs via MinerU integration</li> <li>Automatic NLP entity extraction (people, orgs, locations)</li> <li>Optional LLM summary for quick understanding</li> <li>SSRF-protected (blocks internal network access)</li> </ul>","tags":["Data"]},{"location":"api/content/#parameters","title":"Parameters","text":"<p>Request Body:</p> Field Type Required Description <code>url</code> string Yes URL to extract (web page or PDF) <code>options</code> object No Additional extraction options <p>Query Parameters:</p> Name Type Default Description <code>llm</code> boolean <code>false</code> Add LLM-generated summary","tags":["Data"]},{"location":"api/content/#examples","title":"Examples","text":"curlcurl with LLM summarycurl (PDF)Python <pre><code>curl -X POST http://localhost:8001/v1/content/extract \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\": \"https://example.com\"}'\n</code></pre> <pre><code>curl -X POST \"http://localhost:8001/v1/content/extract?llm=true\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\": \"https://arxiv.org/abs/2301.07041\"}'\n</code></pre> <pre><code>curl -X POST http://localhost:8001/v1/content/extract \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\": \"https://example.com/report.pdf\"}'\n</code></pre> <pre><code>import requests\n\nurls = [\n    \"https://blog.example.com/post-1\",\n    \"https://blog.example.com/post-2\",\n]\n\nfor url in urls:\n    response = requests.post(\n        \"http://localhost:8001/v1/content/extract\",\n        json={\"url\": url}\n    )\n    data = response.json()[\"data\"]\n\n    save_to_database({\n        \"url\": url,\n        \"content\": data[\"content\"],\n        \"entities\": data[\"entities\"],\n        \"extracted_at\": data[\"meta\"][\"timestamp\"]\n    })\n</code></pre>","tags":["Data"]},{"location":"api/content/#response","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"url\": \"https://example.com\",\n    \"domain\": \"example.com\",\n    \"content_length\": 127, // (1)\n    \"content\": \"Example Domain This domain is for use in illustrative examples in documents. You may use this domain in literature without prior coordination or asking for permission. More information...\", // (2)\n    \"is_pdf\": false, // (3)\n    \"entities\": [ // (4)\n      {\"text\": \"Example Domain\", \"label\": \"ORG\", \"description\": \"Organization\"}\n    ],\n    \"llm_summary\": \"This is an example domain used for documentation purposes.\" // (5)\n  },\n  \"meta\": {\n    \"request_id\": \"d8d1dcb4-e942-4659-9b2c-06d4d30b7097\",\n    \"timestamp\": \"2026-01-31T03:13:07.679544Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 103\n  }\n}\n</code></pre> <ol> <li>Character count of the extracted text. Useful for estimating token usage if feeding content into an LLM.</li> <li>The extracted main content with HTML boilerplate removed. Truncated to 10,000 characters maximum. For full-length extraction, use the batch endpoint.</li> <li>Indicates whether the source URL was a PDF. When <code>true</code>, extraction was handled by the MinerU PDF processing service instead of HTML parsing.</li> <li>NLP entities automatically extracted from the content using spaCy. Includes people, organizations, locations, dates, and monetary amounts.</li> <li>LLM-generated summary of the page content. Only populated when <code>llm=true</code> is passed as a query parameter. Adds ~1-3 seconds of latency.</li> </ol>","tags":["Data"]},{"location":"api/content/#response-fields","title":"Response Fields","text":"Field Type Description <code>url</code> string Requested URL <code>domain</code> string Domain extracted from URL <code>content_length</code> integer Character count of extracted content <code>content</code> string Extracted text content (max 10,000 chars) <code>is_pdf</code> boolean Whether the URL is a PDF document <code>entities</code> array Extracted NLP entities (organization, people, locations, etc.) <code>llm_summary</code> string LLM-generated summary (only if <code>llm=true</code>)","tags":["Data"]},{"location":"api/content/#post-v1contentextractbatch","title":"<code>POST /v1/content/extract/batch</code>","text":"<p>Queue batch URL extraction as an async job. Returns immediately with a job ID for status tracking. Supports webhook notifications on completion.</p> <p>Best For</p> <p>Processing many URLs without blocking -- ideal for data pipelines.</p> <p>Target Audience</p> <ul> <li>Data engineers -- Building ETL pipelines for web content</li> <li>Researchers -- Extracting content from large URL lists</li> <li>Archivists -- Bulk preserving web content</li> <li>ML engineers -- Preparing training datasets from web sources</li> </ul> <p>Key features:</p> <ul> <li>Non-blocking: returns immediately with job ID</li> <li>Parallel processing with automatic rate limiting (semaphore-controlled)</li> <li>Webhook notifications when job completes</li> <li>Idempotency keys prevent duplicate processing</li> <li>HMAC-signed webhooks for security</li> </ul>","tags":["Data"]},{"location":"api/content/#parameters_1","title":"Parameters","text":"<p>Request Body:</p> Field Type Required Description <code>urls</code> array Yes List of URLs to extract <code>callback_url</code> string No Webhook URL for completion notification <code>callback_secret</code> string No HMAC secret for webhook signature verification <p>Headers:</p> Name Description <code>Idempotency-Key</code> Prevent duplicate job creation","tags":["Data"]},{"location":"api/content/#examples_1","title":"Examples","text":"curlcurl with webhookCheck job statusPython <pre><code>curl -X POST http://localhost:8001/v1/content/extract/batch \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Idempotency-Key: batch-job-123\" \\\n  -d '{\"urls\": [\"https://example.com\", \"https://example.org\"]}'\n</code></pre> <pre><code>curl -X POST http://localhost:8001/v1/content/extract/batch \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"urls\": [\"https://example.com\", \"https://example.org\"],\n    \"callback_url\": \"https://your-server.com/webhook\",\n    \"callback_secret\": \"your-secret-key\"\n  }'\n</code></pre> <pre><code>JOB_ID=\"e31f008a-303f-4ec2-a994-e26cf4ce677a\"\ncurl \"http://localhost:8001/v1/jobs/$JOB_ID\"\n</code></pre> <pre><code>import requests\nimport time\n\n# Submit batch\nresponse = requests.post(\n    \"http://localhost:8001/v1/content/extract/batch\",\n    json={\"urls\": research_urls},  # List of 100+ URLs\n    headers={\"Idempotency-Key\": f\"research-batch-{session_id}\"}\n)\n\njob_id = response.json()[\"data\"][\"job_id\"]\n\n# Poll for completion (or use webhook)\nwhile True:\n    status = requests.get(\n        f\"http://localhost:8001/v1/jobs/{job_id}\"\n    ).json()\n    if status[\"data\"][\"status\"] == \"completed\":\n        results = status[\"data\"][\"results\"]\n        break\n    time.sleep(5)\n</code></pre>","tags":["Data"]},{"location":"api/content/#response_1","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"job_id\": \"e31f008a-303f-4ec2-a994-e26cf4ce677a\",\n    \"status\": \"pending\",\n    \"total_urls\": 2,\n    \"status_url\": \"/v1/jobs/e31f008a-303f-4ec2-a994-e26cf4ce677a\"\n  },\n  \"meta\": {\n    \"request_id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\",\n    \"timestamp\": \"2026-01-31T03:00:00.000000Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 5\n  }\n}\n</code></pre>","tags":["Data"]},{"location":"api/content/#response-fields_1","title":"Response Fields","text":"Field Type Description <code>job_id</code> string Unique job identifier for status tracking <code>status</code> string Job status: <code>pending</code>, <code>processing</code>, <code>completed</code>, <code>failed</code> <code>total_urls</code> integer Number of URLs queued <code>status_url</code> string Convenience URL for checking job status","tags":["Data"]},{"location":"api/content/#get-v1contentscreenshot","title":"<code>GET /v1/content/screenshot</code>","text":"<p>Generate screenshot URLs for a webpage using multiple screenshot services (thum.io, screenshotmachine, microlink). Returns URLs directly -- no image storage required on your server.</p> <p>Best For</p> <p>Generating visual previews of web pages for thumbnails, archives, or monitoring.</p> <p>Target Audience</p> <ul> <li>Product managers -- Creating visual documentation of web pages</li> <li>QA engineers -- Capturing visual state for bug reports</li> <li>Archivists -- Preserving visual snapshots of websites</li> <li>Marketers -- Creating social media previews</li> <li>Developers -- Building link preview features</li> </ul>","tags":["Data"]},{"location":"api/content/#parameters_2","title":"Parameters","text":"Name Type Default Description <code>url</code> string -- URL to screenshot (required) <code>width</code> integer <code>1280</code> Screenshot width in pixels <code>height</code> integer <code>800</code> Screenshot height in pixels","tags":["Data"]},{"location":"api/content/#examples_2","title":"Examples","text":"curl (default)curl (Full HD)curl (Mobile)JavaScript <pre><code>curl \"http://localhost:8001/v1/content/screenshot?url=https://example.com\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/content/screenshot?url=https://example.com&amp;width=1920&amp;height=1080\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/content/screenshot?url=https://example.com&amp;width=375&amp;height=812\"\n</code></pre> <pre><code>async function getLinkPreview(url) {\n  const response = await fetch(\n    `http://localhost:8001/v1/content/screenshot?url=${encodeURIComponent(url)}`\n  );\n  const data = await response.json();\n\n  // Use thum.io by default, fall back to others if rate limited\n  return {\n    thumbnail: data.data.screenshots.thum_io,\n    fallback: data.data.screenshots.microlink\n  };\n}\n</code></pre>","tags":["Data"]},{"location":"api/content/#response_2","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"url\": \"https://example.com\",\n    \"screenshots\": {\n      \"thum_io\": \"https://image.thum.io/get/width/1920/crop/1080/https://example.com\",\n      \"screenshotmachine\": \"https://api.screenshotmachine.com?url=...\",\n      \"microlink\": \"https://api.microlink.io?url=...&amp;screenshot=true\"\n    },\n    \"note\": \"Use these URLs to fetch screenshots. Some services may have rate limits.\"\n  },\n  \"meta\": {\n    \"request_id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\",\n    \"timestamp\": \"2026-01-31T03:00:00.000000Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 1\n  }\n}\n</code></pre>","tags":["Data"]},{"location":"api/content/#response-fields_2","title":"Response Fields","text":"Field Type Description <code>url</code> string The requested URL <code>screenshots</code> object Screenshot URLs from multiple services <code>screenshots.thum_io</code> string Screenshot URL from thum.io <code>screenshots.screenshotmachine</code> string Screenshot URL from screenshotmachine <code>screenshots.microlink</code> string Screenshot URL from microlink <code>note</code> string Usage note about rate limits","tags":["Data"]},{"location":"api/content/#get-v1contentselectors","title":"<code>GET /v1/content/selectors</code>","text":"<p>Extract XPath, CSS, and Playwright selectors from a webpage using browser automation. Supports semantic search to find elements by description (e.g., \"login button\", \"search box\").</p> <p>Best For</p> <p>Building web scrapers, automation scripts, and testing frameworks.</p> <p>Target Audience</p> <ul> <li>Automation engineers -- Finding reliable selectors for Playwright/Selenium</li> <li>QA engineers -- Generating selectors for test automation</li> <li>Data engineers -- Building web scraping pipelines</li> <li>RPA developers -- Creating robotic process automation workflows</li> <li>Web scraping teams -- Identifying stable element locators</li> </ul> <p>Key features:</p> <ul> <li>Semantic search: Find elements by description (\"login button\", \"search box\")</li> <li>Quality ratings: Know if a selector is stable (<code>GOOD</code>) or fragile (<code>POOR</code>)</li> <li>Multiple selector types: XPath, CSS, and Playwright locators</li> <li>Bounding boxes: Know exact element positions for click automation</li> <li>Browser-rendered: Sees JavaScript-rendered content (not just raw HTML)</li> </ul>","tags":["Data"]},{"location":"api/content/#parameters_3","title":"Parameters","text":"Name Type Default Description <code>url</code> string -- URL to analyze (required) <code>q</code> string -- Semantic search for elements (e.g., \"login button\") <code>full</code> boolean <code>false</code> Include full absolute XPath/CSS <code>limit</code> integer <code>20</code> Maximum elements to return (1-100)","tags":["Data"]},{"location":"api/content/#examples_3","title":"Examples","text":"curl (semantic search)curl (form inputs)curl (full XPath)Python (Web Scraping)bash (jq processing) <pre><code>curl \"http://localhost:8001/v1/content/selectors?url=https://example.com&amp;q=link&amp;limit=5\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/content/selectors?url=https://example.com&amp;q=input+field\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/content/selectors?url=https://example.com&amp;q=navigation&amp;full=true\"\n</code></pre> <pre><code>import requests\nfrom playwright.sync_api import sync_playwright\n\n# Find the best selector for the \"Add to Cart\" button\nresponse = requests.get(\n    \"http://localhost:8001/v1/content/selectors\",\n    params={\n        \"url\": \"https://shop.example.com/product/123\",\n        \"q\": \"add to cart button\"\n    }\n)\n\nmatches = response.json()[\"data\"][\"matches\"]\n\n# Use the highest quality selector\nbest_selector = None\nfor match in matches:\n    for sel in match[\"selectors\"]:\n        if sel[\"quality\"] == \"GOOD\":\n            best_selector = sel[\"selector\"]\n            break\n\n# Use in Playwright\nwith sync_playwright() as p:\n    browser = p.chromium.launch()\n    page = browser.new_page()\n    page.goto(\"https://shop.example.com/product/123\")\n    page.click(best_selector)\n</code></pre> <pre><code># Generate selectors for a form page\ncurl \"http://localhost:8001/v1/content/selectors?url=https://app.example.com/signup&amp;q=form\" | \\\n  jq '.data.matches[] | {element: .name, xpath: .xpath, quality: .xpath_quality}'\n</code></pre>","tags":["Data"]},{"location":"api/content/#response_3","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"url\": \"https://example.com\",\n    \"title\": \"Example Domain\",\n    \"total_elements\": 2,\n    \"query\": \"link\",\n    \"matches\": [\n      {\n        \"id\": 0,\n        \"tag\": \"a\",\n        \"role\": null,\n        \"name\": \"More information...\",\n        \"text\": \"More information...\",\n        \"xpath\": \"//a[contains(text(), 'More information')]\",\n        \"xpath_quality\": \"OK\",\n        \"css\": null,\n        \"css_quality\": null,\n        \"full_xpath\": \"/html/body/div/p/a\",\n        \"full_css\": \"body &gt; div &gt; p &gt; a\",\n        \"selectors\": [\n          {\n            \"selector\": \"//a[contains(text(), 'More information')]\",\n            \"type\": \"xpath\",\n            \"quality\": \"OK\",\n            \"description\": \"text content\"\n          },\n          {\n            \"selector\": \"getByText('More information...')\",\n            \"type\": \"playwright\",\n            \"quality\": \"OK\",\n            \"description\": \"text content\"\n          },\n          {\n            \"selector\": \"(//a)[1]\",\n            \"type\": \"xpath\",\n            \"quality\": \"POOR\",\n            \"description\": \"positional index (fragile)\"\n          }\n        ],\n        \"aria_label\": null,\n        \"classes\": [],\n        \"attributes\": {\n          \"href\": \"https://www.iana.org/domains/example\"\n        },\n        \"bounding_box\": {\n          \"x\": 256,\n          \"y\": 206.078125,\n          \"width\": 91.25,\n          \"height\": 19\n        },\n        \"ancestor_id\": null,\n        \"ancestor_path\": null\n      }\n    ],\n    \"screenshot_base64\": null,\n    \"processing_time_ms\": 1610.89\n  },\n  \"meta\": {\n    \"request_id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\",\n    \"timestamp\": \"2026-01-31T03:00:00.000000Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 1611\n  }\n}\n</code></pre>","tags":["Data"]},{"location":"api/content/#match-object-fields","title":"Match Object Fields","text":"Field Type Description <code>id</code> integer Element index <code>tag</code> string HTML tag name <code>role</code> string ARIA role (if present) <code>name</code> string Accessible name <code>text</code> string Text content <code>xpath</code> string Best XPath selector <code>xpath_quality</code> string Quality rating: <code>GOOD</code>, <code>OK</code>, <code>POOR</code> <code>css</code> string CSS selector (if available) <code>css_quality</code> string CSS quality rating <code>full_xpath</code> string Absolute XPath from root <code>full_css</code> string Absolute CSS selector <code>selectors</code> array All available selectors with quality ratings <code>aria_label</code> string ARIA label attribute <code>classes</code> array CSS classes on element <code>attributes</code> object All HTML attributes <code>bounding_box</code> object Position and size (<code>x</code>, <code>y</code>, <code>width</code>, <code>height</code>) <code>ancestor_id</code> integer Parent element ID (if applicable) <code>ancestor_path</code> string Path to ancestor","tags":["Data"]},{"location":"api/content/#get-v1contentmarkdown","title":"<code>GET /v1/content/markdown</code>","text":"<p>Convert a webpage to clean markdown. Uses the reader-lm model for intelligent extraction that removes ads, navigation, and clutter while preserving document structure.</p> <p>Best For</p> <p>Preparing web content for LLMs, documentation, or offline reading.</p> <p>Target Audience</p> <ul> <li>AI/ML engineers -- Preparing clean text for LLM context windows</li> <li>Documentation teams -- Importing web content into docs</li> <li>Researchers -- Creating clean, portable versions of articles</li> <li>Content migrators -- Moving content between platforms</li> <li>Read-it-later apps -- Building offline reading features</li> </ul> <p>Key features:</p> <ul> <li>Removes ads, navigation, and clutter -- keeps main content</li> <li>Preserves document structure (headings, lists, code blocks)</li> <li>Optional link/image inclusion for different use cases</li> <li>Uses reader-lm model for intelligent extraction</li> <li>Output is LLM-ready (clean context for RAG, summarization)</li> </ul>","tags":["Data"]},{"location":"api/content/#parameters_4","title":"Parameters","text":"Name Type Default Description <code>url</code> string -- URL to convert (required) <code>include_links</code> boolean <code>true</code> Include hyperlinks in output <code>include_images</code> boolean <code>false</code> Include image references","tags":["Data"]},{"location":"api/content/#examples_4","title":"Examples","text":"curlcurl (no links)curl (with images)Python (RAG Pipeline)bash (save to file) <pre><code>curl \"http://localhost:8001/v1/content/markdown?url=https://example.com\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/content/markdown?url=https://docs.example.com/guide&amp;include_links=false\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/content/markdown?url=https://example.com&amp;include_images=true\"\n</code></pre> <pre><code>import requests\n\ndef prepare_for_rag(urls):\n    documents = []\n    for url in urls:\n        response = requests.get(\n            \"http://localhost:8001/v1/content/markdown\",\n            params={\n                \"url\": url,\n                \"include_links\": False,   # Cleaner for embeddings\n                \"include_images\": False\n            }\n        )\n        data = response.json()[\"data\"]\n        documents.append({\n            \"url\": url,\n            \"title\": data[\"title\"],\n            \"content\": data[\"markdown\"],\n            \"char_count\": data[\"content_length\"]\n        })\n    return documents\n\ndocs = prepare_for_rag(article_urls)\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/content/markdown?url=https://library.docs.com/api\" \\\n  | jq -r '.data.markdown' &gt; api-docs.md\n</code></pre>","tags":["Data"]},{"location":"api/content/#response_4","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"url\": \"https://example.com\",\n    \"title\": \"Example Domain\",\n    \"markdown\": \"# Example Domain\\n\\nThis domain is for use in illustrative examples...\\n\\n[More information...](https://www.iana.org/domains/example)\",\n    \"content_length\": 180,\n    \"method\": \"reader_lm\"\n  },\n  \"meta\": {\n    \"request_id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\",\n    \"timestamp\": \"2026-01-31T03:00:00.000000Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 2500\n  }\n}\n</code></pre>","tags":["Data"]},{"location":"api/content/#response-fields_3","title":"Response Fields","text":"Field Type Description <code>url</code> string The requested URL <code>title</code> string Extracted page title <code>markdown</code> string Clean markdown content <code>content_length</code> integer Character count of markdown output <code>method</code> string Extraction method used (e.g., <code>reader_lm</code>)","tags":["Data"]},{"location":"api/content/#post-v1contentanonymize","title":"<code>POST /v1/content/anonymize</code>","text":"<p>Anonymize PII (Personally Identifiable Information) in text using Microsoft Presidio. Detects and anonymizes names, emails, phone numbers, credit cards, and many more entity types.</p> <p>Best For</p> <p>Data privacy compliance, anonymizing logs, redacting sensitive data before sharing or storage.</p>","tags":["Data"]},{"location":"api/content/#parameters_5","title":"Parameters","text":"Name Type Required Default Description <code>text</code> string Yes -- Text to anonymize <code>entities</code> array No all Entity types to detect (detects all if omitted) <code>default_operator</code> string No <code>replace</code> Anonymization operator: <code>replace</code>, <code>redact</code>, <code>mask</code>, <code>hash</code> <code>score_threshold</code> float No <code>0.5</code> Confidence threshold 0-1 <code>return_entities</code> boolean No <code>true</code> Include detected entities in response","tags":["Data"]},{"location":"api/content/#anonymization-operators","title":"Anonymization Operators","text":"Operator Behavior Example Output <code>replace</code> Replace with entity type label <code>&lt;PERSON&gt;</code> <code>redact</code> Remove completely (empty) <code>mask</code> Partially mask <code>****@email.com</code> <code>hash</code> SHA256 hash <code>a1b2c3d4e5f6...</code>","tags":["Data"]},{"location":"api/content/#examples_5","title":"Examples","text":"curlcurl (specific entities)curl (mask operator) <pre><code>curl -X POST http://localhost:8001/v1/content/anonymize \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"text\": \"Contact John at john@example.com\"}'\n</code></pre> <pre><code>curl -X POST http://localhost:8001/v1/content/anonymize \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"text\": \"John Smith'\\''s email is john@example.com and phone is 555-123-4567\",\n    \"entities\": [\"PERSON\", \"EMAIL_ADDRESS\", \"PHONE_NUMBER\"],\n    \"default_operator\": \"replace\",\n    \"score_threshold\": 0.5,\n    \"return_entities\": true\n  }'\n</code></pre> <pre><code>curl -X POST http://localhost:8001/v1/content/anonymize \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"text\": \"Contact John at john@example.com\",\n    \"default_operator\": \"mask\"\n  }'\n</code></pre>","tags":["Data"]},{"location":"api/content/#response_5","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"text\": \"Contact &lt;PERSON&gt; at &lt;EMAIL_ADDRESS&gt;\",\n    \"original_length\": 32,\n    \"anonymized_length\": 37,\n    \"entities_detected\": 2,\n    \"entity_counts\": {\"PERSON\": 1, \"EMAIL_ADDRESS\": 1},\n    \"entities\": [\n      {\"entity_type\": \"PERSON\", \"text\": \"John\", \"start\": 8, \"end\": 12, \"score\": 0.85},\n      {\"entity_type\": \"EMAIL_ADDRESS\", \"text\": \"john@example.com\", \"start\": 16, \"end\": 32, \"score\": 0.95}\n    ]\n  }\n}\n</code></pre>","tags":["Data"]},{"location":"api/content/#response-fields_4","title":"Response Fields","text":"Field Type Description <code>text</code> string Anonymized text <code>original_length</code> integer Character count of original text <code>anonymized_length</code> integer Character count after anonymization <code>entities_detected</code> integer Total number of PII entities found <code>entity_counts</code> object Count of each entity type detected <code>entities</code> array Detailed list of detected entities with positions and confidence scores","tags":["Data"]},{"location":"api/content/#post-v1contentanonymizeurl","title":"<code>POST /v1/content/anonymize/url</code>","text":"<p>Extract content from a URL and anonymize PII in a single step. Combines the extraction and anonymization pipelines.</p>","tags":["Data"]},{"location":"api/content/#parameters_6","title":"Parameters","text":"Name Type Required Default Description <code>url</code> string Yes -- URL to extract and anonymize <code>default_operator</code> string No <code>replace</code> Anonymization operator <code>score_threshold</code> float No <code>0.5</code> Confidence threshold 0-1","tags":["Data"]},{"location":"api/content/#example","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/content/anonymize/url \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"url\": \"https://example.com/page-with-pii\",\n    \"default_operator\": \"replace\",\n    \"score_threshold\": 0.5\n  }'\n</code></pre>","tags":["Data"]},{"location":"api/content/#get-v1contentpiientities","title":"<code>GET /v1/content/pii/entities</code>","text":"<p>List all supported PII entity types that can be used with the anonymization endpoints.</p>","tags":["Data"]},{"location":"api/content/#example_1","title":"Example","text":"<pre><code>curl http://localhost:8001/v1/content/pii/entities\n</code></pre>","tags":["Data"]},{"location":"api/content/#response_6","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"entities\": [\n      {\"type\": \"PERSON\", \"description\": \"Person names\"},\n      {\"type\": \"EMAIL_ADDRESS\", \"description\": \"Email addresses\"},\n      {\"type\": \"PHONE_NUMBER\", \"description\": \"Phone numbers\"},\n      {\"type\": \"CREDIT_CARD\", \"description\": \"Credit card numbers\"},\n      {\"type\": \"US_SSN\", \"description\": \"US Social Security Numbers\"},\n      {\"type\": \"IP_ADDRESS\", \"description\": \"IP addresses\"},\n      {\"type\": \"LOCATION\", \"description\": \"Location/address\"},\n      {\"type\": \"DATE_TIME\", \"description\": \"Dates and times\"},\n      {\"type\": \"NRP\", \"description\": \"Nationality, religion, political group\"},\n      {\"type\": \"MEDICAL_LICENSE\", \"description\": \"Medical license numbers\"},\n      {\"type\": \"URL\", \"description\": \"URLs\"},\n      {\"type\": \"US_BANK_NUMBER\", \"description\": \"US bank account numbers\"},\n      {\"type\": \"US_DRIVER_LICENSE\", \"description\": \"US driver's license\"},\n      {\"type\": \"US_PASSPORT\", \"description\": \"US passport numbers\"},\n      {\"type\": \"IBAN_CODE\", \"description\": \"International bank account numbers\"},\n      {\"type\": \"CRYPTO\", \"description\": \"Cryptocurrency addresses\"}\n    ]\n  }\n}\n</code></pre>","tags":["Data"]},{"location":"api/content/#supported-entity-types","title":"Supported Entity Types","text":"Entity Type Description <code>PERSON</code> Person names <code>EMAIL_ADDRESS</code> Email addresses <code>PHONE_NUMBER</code> Phone numbers <code>CREDIT_CARD</code> Credit card numbers <code>US_SSN</code> US Social Security Numbers <code>IP_ADDRESS</code> IP addresses <code>LOCATION</code> Location / address <code>DATE_TIME</code> Dates and times <code>NRP</code> Nationality, religion, political group <code>MEDICAL_LICENSE</code> Medical license numbers <code>URL</code> URLs <code>US_BANK_NUMBER</code> US bank account numbers <code>US_DRIVER_LICENSE</code> US driver's license numbers <code>US_PASSPORT</code> US passport numbers <code>IBAN_CODE</code> International bank account numbers <code>CRYPTO</code> Cryptocurrency addresses","tags":["Data"]},{"location":"api/crawler/","title":"Web Crawler","text":"<p>Advanced web crawling with Human Behavior Simulation (HBS) using the Camoufox anti-detect browser engine.</p>","tags":["Data"]},{"location":"api/crawler/#overview","title":"Overview","text":"<p>The crawler endpoints provide full-featured web scraping with JavaScript rendering, bot-detection evasion, and structured content extraction. Every request is executed inside a real Firefox browser instance powered by Camoufox, which patches dozens of browser fingerprinting vectors.</p> <p>Human Behavior Simulation (HBS)</p> <p>HBS makes automated browsing indistinguishable from a real user by injecting:</p> <ul> <li>Mouse movements -- realistic bezier-curve paths across the viewport</li> <li>Scrolling -- variable-speed scroll events with pauses</li> <li>Typing delays -- per-keystroke jitter that mimics human cadence</li> <li>Session warmup -- visiting unrelated pages before the target to build a natural referrer chain</li> </ul>","tags":["Data"]},{"location":"api/crawler/#key-features","title":"Key Features","text":"Feature Description JavaScript rendering Full browser engine -- not headless Chrome stubs HBS anti-detection Mouse, scroll, typing, warmup, referrer spoofing, WebRTC blocking Crawl modes Single page, domain, subdomain, recursive Browsing modes <code>fast</code>, <code>normal</code>, <code>human</code>, <code>stealth</code>, <code>amazon</code> Bot-detection retry Exponential backoff when a challenge page is detected Content extraction Text, links, images, metadata Enhanced extraction Custom CSS / XPath / regex selectors, spaCy keyword extraction, language detection Screenshot capture Optional full-page screenshot per crawled URL Proxy support Datacenter and residential proxy pass-through","tags":["Data"]},{"location":"api/crawler/#endpoints","title":"Endpoints","text":"","tags":["Data"]},{"location":"api/crawler/#post-v1crawlerpage-crawl-single-page","title":"POST <code>/v1/crawler/page</code> -- Crawl Single Page","text":"<p>Crawl a single page with full JavaScript rendering and optional enhanced extraction.</p>","tags":["Data"]},{"location":"api/crawler/#request-body","title":"Request Body","text":"<pre><code>{\n  \"url\": \"https://www.amazon.com/s?k=shoes\",\n  \"browsing_mode\": \"normal\",\n  \"timeout\": 30000,\n  \"extract_content\": true,\n  \"capture_screenshot\": false,\n  \"warmup_browse\": false,\n  \"selectors\": [\n    {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"css\"},\n    {\"name\": \"prices\", \"selector\": \".price\", \"type\": \"css\", \"multiple\": true},\n    {\"name\": \"description\", \"selector\": \"//meta[@name='description']/@content\", \"type\": \"xpath\"}\n  ],\n  \"extract_keywords\": true,\n  \"keyword_count\": 10,\n  \"detect_language\": true\n}\n</code></pre>","tags":["Data"]},{"location":"api/crawler/#browsing-modes","title":"Browsing Modes","text":"Mode Delays Description <code>fast</code> 0.1 -- 0.3 s Minimal delays, fastest but most detectable <code>normal</code> 0.5 -- 1.5 s Good balance of speed and stealth <code>human</code> 1.5 -- 4 s Full HBS with mouse, scroll, typing <code>stealth</code> 3 -- 7 s Maximum anti-detection <code>amazon</code> 2 -- 5 s For heavily protected sites; includes warmup","tags":["Data"]},{"location":"api/crawler/#enhanced-extraction-options","title":"Enhanced Extraction Options","text":"Parameter Type Description <code>selectors</code> array Custom CSS / XPath / regex selectors for targeted extraction <code>extract_keywords</code> bool Extract keywords from page content (uses spaCy NLP) <code>keyword_count</code> int Number of keywords to extract (1-50, default: 10) <code>detect_language</code> bool Detect page language (uses langdetect)","tags":["Data"]},{"location":"api/crawler/#selector-configuration","title":"Selector Configuration","text":"<pre><code>{\n  \"name\": \"field_name\",\n  \"selector\": \"h1.title\",\n  \"type\": \"css\",\n  \"attribute\": \"href\",\n  \"multiple\": true\n}\n</code></pre> Field Required Description <code>name</code> yes Output field name <code>selector</code> yes CSS selector, XPath expression, or regex pattern <code>type</code> yes <code>css</code>, <code>xpath</code>, or <code>regex</code> <code>attribute</code> no Extract an attribute instead of inner text <code>multiple</code> no Return all matches instead of first only (default: <code>false</code>)","tags":["Data"]},{"location":"api/crawler/#example","title":"Example","text":"curlPython <pre><code>curl -X POST http://localhost:8001/v1/crawler/page \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"url\": \"https://www.amazon.com/s?k=shoes\",\n    \"browsing_mode\": \"normal\",\n    \"extract_content\": true,\n    \"extract_keywords\": true,\n    \"detect_language\": true\n  }'\n</code></pre> <pre><code>import requests\n\nresp = requests.post(\"http://localhost:8001/v1/crawler/page\", json={\n    \"url\": \"https://www.amazon.com/s?k=shoes\",\n    \"browsing_mode\": \"normal\",\n    \"extract_content\": True,\n    \"extract_keywords\": True,\n    \"detect_language\": True,\n})\ndata = resp.json()[\"data\"]\nprint(data[\"title\"], \"-- links:\", data[\"links_count\"])\n</code></pre>","tags":["Data"]},{"location":"api/crawler/#response","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"url\": \"https://www.amazon.com/s?k=shoes\",\n    \"final_url\": \"https://www.amazon.com/s?k=shoes\",\n    \"status\": \"success\",\n    \"depth\": 0,\n    \"title\": \"Amazon.com : shoes\",\n    \"content_text\": \"Skip to main content...\",\n    \"links_count\": 723,\n    \"images_count\": 63,\n    \"links\": [\"https://...\"],\n    \"metadata\": {\"description\": \"...\"},\n    \"elapsed_ms\": 3500,\n    \"extracted\": {\n      \"title\": \"Amazon.com: shoes\",\n      \"prices\": [\"$49.99\", \"$89.99\", \"$129.00\"],\n      \"description\": \"Shop shoes at Amazon...\"\n    },\n    \"keywords\": [\n      {\"text\": \"shoes\", \"score\": 0.95},\n      {\"text\": \"running\", \"score\": 0.87},\n      {\"text\": \"athletic\", \"score\": 0.82}\n    ],\n    \"language\": {\n      \"code\": \"en\",\n      \"name\": \"English\",\n      \"confidence\": 0.99\n    }\n  }\n}\n</code></pre>","tags":["Data"]},{"location":"api/crawler/#post-v1crawlercrawl-multi-page-crawl","title":"POST <code>/v1/crawler/crawl</code> -- Multi-Page Crawl","text":"<p>Start a multi-page crawl job with recursive link following.</p>","tags":["Data"]},{"location":"api/crawler/#request-body_1","title":"Request Body","text":"<pre><code>{\n  \"url\": \"https://example.com\",\n  \"mode\": \"domain\",\n  \"browsing_mode\": \"human\",\n  \"max_depth\": 2,\n  \"max_pages\": 20,\n  \"min_delay\": 1.0,\n  \"max_delay\": 3.0,\n  \"extract_content\": true,\n  \"capture_screenshots\": false,\n  \"follow_external\": false\n}\n</code></pre>","tags":["Data"]},{"location":"api/crawler/#crawl-modes","title":"Crawl Modes","text":"Mode Scope <code>single</code> Only the starting URL <code>domain</code> Stay within the same domain (allows subdomains) <code>subdomain</code> Stay within the exact subdomain only <code>recursive</code> Follow all links (use with caution) <p>Recursive Mode</p> <p>The <code>recursive</code> mode follows all outbound links regardless of domain. Always set <code>max_pages</code> and <code>max_depth</code> to avoid runaway crawls.</p>","tags":["Data"]},{"location":"api/crawler/#example_1","title":"Example","text":"curlPython <pre><code>curl -X POST http://localhost:8001/v1/crawler/crawl \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"url\": \"https://example.com\",\n    \"mode\": \"domain\",\n    \"browsing_mode\": \"human\",\n    \"max_depth\": 2,\n    \"max_pages\": 20\n  }'\n</code></pre> <pre><code>import requests\n\nresp = requests.post(\"http://localhost:8001/v1/crawler/crawl\", json={\n    \"url\": \"https://example.com\",\n    \"mode\": \"domain\",\n    \"browsing_mode\": \"human\",\n    \"max_depth\": 2,\n    \"max_pages\": 20,\n})\nresult = resp.json()[\"data\"]\nprint(f\"Crawled {result['pages_crawled']} pages in {result['elapsed_seconds']}s\")\n</code></pre>","tags":["Data"]},{"location":"api/crawler/#response_1","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"start_url\": \"https://example.com\",\n    \"mode\": \"domain\",\n    \"pages_crawled\": 15,\n    \"pages_failed\": 1,\n    \"pages_skipped\": 4,\n    \"total_links_found\": 342,\n    \"unique_domains\": 1,\n    \"elapsed_seconds\": 45.2,\n    \"stopped_reason\": \"max_pages\",\n    \"results\": [\"...\"]\n  }\n}\n</code></pre>","tags":["Data"]},{"location":"api/crawler/#get-v1crawlermodes-list-available-modes","title":"GET <code>/v1/crawler/modes</code> -- List Available Modes","text":"<p>List available crawl and browsing modes with descriptions.</p>","tags":["Data"]},{"location":"api/crawler/#example_2","title":"Example","text":"<pre><code>curl http://localhost:8001/v1/crawler/modes\n</code></pre>","tags":["Data"]},{"location":"api/crawler/#response_2","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"crawl_modes\": [\n      {\"name\": \"single\", \"description\": \"Only crawl the starting URL\"},\n      {\"name\": \"domain\", \"description\": \"Stay within same domain\"},\n      {\"name\": \"subdomain\", \"description\": \"Stay within exact subdomain\"},\n      {\"name\": \"recursive\", \"description\": \"Follow all links\"}\n    ],\n    \"browsing_modes\": [\n      {\"name\": \"fast\", \"description\": \"Minimal delays (0.1-0.3s)\"},\n      {\"name\": \"normal\", \"description\": \"Standard delays (0.5-1.5s)\"},\n      {\"name\": \"human\", \"description\": \"Full HBS simulation (1.5-4s)\"},\n      {\"name\": \"stealth\", \"description\": \"Maximum anti-detection (3-7s)\"},\n      {\"name\": \"amazon\", \"description\": \"For heavily protected sites (2-5s)\"}\n    ],\n    \"features\": [\n      \"Camoufox anti-detect Firefox browser\",\n      \"Human-like bezier curve mouse movements\",\n      \"Session warmup browsing\",\n      \"Referrer chain spoofing\",\n      \"WebRTC blocking\"\n    ],\n    \"proxy_note\": \"Datacenter IPs may require residential proxies\"\n  }\n}\n</code></pre>","tags":["Data"]},{"location":"api/crawler/#anti-detection-features","title":"Anti-Detection Features","text":"<p>The crawler ships with multiple layers of anti-detection that work together to avoid bot fingerprinting.</p> Layer Technique Browser engine Camoufox -- patched Firefox with fingerprint randomisation Mouse movements Bezier-curve paths with variable speed and overshoot Scrolling Human-like variable-speed scroll events Typing Per-keystroke jitter matching human cadence Session warmup Visits unrelated pages to build a natural referrer chain Referrer spoofing Constructs plausible referrer headers WebRTC blocking Prevents real-IP leaks through WebRTC Retry logic Exponential backoff on bot-detection challenge pages","tags":["Data"]},{"location":"api/crawler/#using-proxies","title":"Using Proxies","text":"<p>For datacenter IPs or heavily protected sites, add proxy configuration to any crawler request body:</p> <pre><code>{\n  \"url\": \"https://www.amazon.com/s?k=shoes\",\n  \"browsing_mode\": \"amazon\",\n  \"proxy\": {\n    \"server\": \"http://residential.proxy.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n  }\n}\n</code></pre> <p>When Do You Need a Proxy?</p> <p>Residential IPs (home networks) typically do not need proxies. The proxy option is for users running from datacenter or cloud environments where the IP reputation is low.</p>","tags":["Data"]},{"location":"api/crypto/","title":"Crypto Wallet Analysis","text":"<p>The Crypto API provides multi-chain cryptocurrency wallet tracking, balance monitoring, and transaction history analysis.</p> <p>Supported Blockchains</p> Chain Network ID Bitcoin <code>bitcoin</code> Ethereum <code>ethereum</code> Solana <code>solana</code> Polygon <code>polygon</code> BNB Chain <code>bnb</code> XRP Ledger <code>xrp</code> Dogecoin <code>dogecoin</code>","tags":["Crypto","OSINT"]},{"location":"api/crypto/#endpoints","title":"Endpoints","text":"","tags":["Crypto","OSINT"]},{"location":"api/crypto/#list-supported-networks","title":"List Supported Networks","text":"<p><code>GET /v1/crypto/networks</code></p> <p>List all supported blockchain networks.</p> <pre><code>curl http://localhost:8001/v1/crypto/networks\n</code></pre> Response <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"networks\": [\"bitcoin\", \"ethereum\", \"solana\", \"polygon\"]\n  }\n}\n</code></pre>","tags":["Crypto","OSINT"]},{"location":"api/crypto/#list-tracked-wallets","title":"List Tracked Wallets","text":"<p><code>GET /v1/crypto/wallets</code></p> <p>List all wallets currently being tracked.</p> <pre><code>curl http://localhost:8001/v1/crypto/wallets\n</code></pre>","tags":["Crypto","OSINT"]},{"location":"api/crypto/#add-wallet","title":"Add Wallet","text":"<p><code>POST /v1/crypto/wallets</code></p> <p>Add a new wallet address to track.</p> <p>Request Body:</p> Field Type Description <code>address</code> string Wallet address (required) <code>network</code> string Blockchain network (required) <code>label</code> string Human-readable label EthereumBitcoinSolanaPython <pre><code>curl -X POST http://localhost:8001/v1/crypto/wallets \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"address\": \"0x742d35Cc6634C0532925a3b844Bc9e7595f2bD18\",\n    \"network\": \"ethereum\",\n    \"label\": \"Main Wallet\"\n  }'\n</code></pre> <pre><code>curl -X POST http://localhost:8001/v1/crypto/wallets \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"address\": \"bc1qxy2kgdygjrsqtzq2n0yrf2493p83kkfjhx0wlh\",\n    \"network\": \"bitcoin\",\n    \"label\": \"Cold Storage\"\n  }'\n</code></pre> <pre><code>curl -X POST http://localhost:8001/v1/crypto/wallets \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"address\": \"7xKXtg2CW87d97TXJSDpbD5jBkheTqA83TZRuJosgAsU\",\n    \"network\": \"solana\",\n    \"label\": \"SOL Wallet\"\n  }'\n</code></pre> <pre><code>import requests\n\ndef track_wallets(wallets):\n    \"\"\"Track multiple wallets across different chains.\"\"\"\n    results = []\n    for wallet in wallets:\n        response = requests.post(\n            'http://localhost:8001/v1/crypto/wallets',\n            json={\n                'address': wallet['address'],\n                'network': wallet['network'],\n                'label': wallet.get('label', '')\n            }\n        )\n        results.append(response.json())\n    return results\n\n# Track wallets across chains\ntrack_wallets([\n    {'address': '0x742d...', 'network': 'ethereum', 'label': 'ETH Main'},\n    {'address': 'bc1q...', 'network': 'bitcoin', 'label': 'BTC Cold'},\n    {'address': '7xKX...', 'network': 'solana', 'label': 'SOL DeFi'},\n])\n</code></pre>","tags":["Crypto","OSINT"]},{"location":"api/crypto/#get-wallet-details","title":"Get Wallet Details","text":"<p><code>GET /v1/crypto/wallets/{wallet_id}</code></p> <p>Retrieve detailed information about a tracked wallet.</p> <pre><code>curl http://localhost:8001/v1/crypto/wallets/w_abc123\n</code></pre>","tags":["Crypto","OSINT"]},{"location":"api/crypto/#remove-wallet","title":"Remove Wallet","text":"<p><code>DELETE /v1/crypto/wallets/{wallet_id}</code></p> <p>Stop tracking a wallet and remove it from the system.</p> <pre><code>curl -X DELETE http://localhost:8001/v1/crypto/wallets/w_abc123\n</code></pre>","tags":["Crypto","OSINT"]},{"location":"api/crypto/#get-wallet-balance","title":"Get Wallet Balance","text":"<p><code>GET /v1/crypto/wallets/{wallet_id}/balance</code></p> <p>Retrieve the current balance for a tracked wallet.</p> cURLPython <pre><code>curl http://localhost:8001/v1/crypto/wallets/w_abc123/balance\n</code></pre> <pre><code>import requests\n\ndef get_portfolio_balances(wallet_ids):\n    \"\"\"Get balances for all tracked wallets.\"\"\"\n    portfolio = []\n    for wid in wallet_ids:\n        resp = requests.get(\n            f'http://localhost:8001/v1/crypto/wallets/{wid}/balance'\n        )\n        data = resp.json()['data']\n        portfolio.append(data)\n    return portfolio\n</code></pre>","tags":["Crypto","OSINT"]},{"location":"api/crypto/#get-wallet-transactions","title":"Get Wallet Transactions","text":"<p><code>GET /v1/crypto/wallets/{wallet_id}/transactions</code></p> <p>Retrieve transaction history for a tracked wallet.</p> cURLPython <pre><code>curl http://localhost:8001/v1/crypto/wallets/w_abc123/transactions\n</code></pre> <pre><code>import requests\n\ndef analyze_transactions(wallet_id):\n    \"\"\"Fetch and analyze wallet transactions.\"\"\"\n    resp = requests.get(\n        f'http://localhost:8001/v1/crypto/wallets/{wallet_id}/transactions'\n    )\n    txns = resp.json()['data']['transactions']\n\n    # Summarize\n    total_in = sum(t['value'] for t in txns if t['direction'] == 'in')\n    total_out = sum(t['value'] for t in txns if t['direction'] == 'out')\n\n    return {\n        'total_received': total_in,\n        'total_sent': total_out,\n        'net_flow': total_in - total_out,\n        'transaction_count': len(txns)\n    }\n</code></pre>","tags":["Crypto","OSINT"]},{"location":"api/darkweb/","title":"Dark Web Monitoring","text":"<p>Monitor dark web sources for threats including ransomware groups, forums, and paste sites.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#overview","title":"Overview","text":"<p>The dark web monitoring endpoints provide continuous threat intelligence from three major source categories: ransomware leak sites, underground forums, and paste sites. All Tor-based sources are accessed through the built-in Tor proxy.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#key-features","title":"Key Features","text":"Feature Description Ransomware tracking 45+ active groups with victim feeds Forum monitoring Dark web forum crawling with Tor support Paste site monitoring Credential and data leak detection Keyword alerting Webhook notifications on keyword matches Pattern matching Regex-based detection for emails, API keys, credit cards, SSNs, private keys","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#ransomware-tracking","title":"Ransomware Tracking","text":"","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkwebransomwaregroups-list-groups","title":"GET <code>/v1/darkweb/ransomware/groups</code> -- List Groups","text":"<p>List known ransomware groups with activity statistics.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#example","title":"Example","text":"<pre><code>curl http://localhost:8001/v1/darkweb/ransomware/groups\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#response","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"groups\": [\n      {\n        \"name\": \"lockbit\",\n        \"aliases\": [\"LockBit 3.0\", \"LockBit Black\"],\n        \"first_seen\": \"2019-09-01\",\n        \"status\": \"active\",\n        \"victim_count\": 1500,\n        \"leak_site\": \"http://lockbit...onion\"\n      }\n    ],\n    \"total\": 45\n  }\n}\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkwebransomwarevictims-recent-victims","title":"GET <code>/v1/darkweb/ransomware/victims</code> -- Recent Victims","text":"<p>Get recent ransomware victims across all tracked groups.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#query-parameters","title":"Query Parameters","text":"Parameter Type Default Description <code>limit</code> int 50 Maximum results <code>offset</code> int 0 Pagination offset <code>group</code> string -- Filter by group name <code>country</code> string -- Filter by country code","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#example_1","title":"Example","text":"curlPython <pre><code>curl \"http://localhost:8001/v1/darkweb/ransomware/victims?group=lockbit&amp;limit=10\"\n</code></pre> <pre><code>import requests\n\nresp = requests.get(\"http://localhost:8001/v1/darkweb/ransomware/victims\", params={\n    \"group\": \"lockbit\",\n    \"limit\": 10,\n})\nfor victim in resp.json()[\"data\"][\"victims\"]:\n    print(f\"{victim['posted_date']} | {victim['organization']} | {victim['sector']}\")\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#response_1","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"victims\": [\n      {\n        \"organization\": \"Example Corp\",\n        \"domain\": \"example.com\",\n        \"group\": \"lockbit\",\n        \"country\": \"US\",\n        \"sector\": \"Technology\",\n        \"posted_date\": \"2024-01-15\",\n        \"deadline\": \"2024-01-30\",\n        \"data_size\": \"500GB\",\n        \"status\": \"published\"\n      }\n    ]\n  }\n}\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkwebransomwarecheckdomain-domain-check","title":"GET <code>/v1/darkweb/ransomware/check/{domain}</code> -- Domain Check","text":"<p>Check if a specific domain has been hit by ransomware.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#example_2","title":"Example","text":"<pre><code>curl http://localhost:8001/v1/darkweb/ransomware/check/example.com\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#response_2","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"domain\": \"example.com\",\n    \"found\": true,\n    \"incidents\": [\n      {\n        \"group\": \"lockbit\",\n        \"posted_date\": \"2024-01-15\",\n        \"status\": \"published\"\n      }\n    ]\n  }\n}\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#post-v1darkwebransomwaresearch-search-victims","title":"POST <code>/v1/darkweb/ransomware/search</code> -- Search Victims","text":"<p>Search the ransomware victim database with filters.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#request-body","title":"Request Body","text":"<pre><code>{\n  \"query\": \"healthcare\",\n  \"groups\": [\"lockbit\", \"alphv\"],\n  \"date_from\": \"2024-01-01\",\n  \"date_to\": \"2024-12-31\"\n}\n</code></pre> Parameter Type Required Description <code>query</code> string yes Search term (matches org name, sector, domain) <code>groups</code> array no Filter to specific ransomware groups <code>date_from</code> string no Start date (<code>YYYY-MM-DD</code>) <code>date_to</code> string no End date (<code>YYYY-MM-DD</code>)","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#example_3","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/darkweb/ransomware/search \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"healthcare\", \"groups\": [\"lockbit\", \"alphv\"], \"date_from\": \"2024-01-01\"}'\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkwebransomwarestatistics-trends-statistics","title":"GET <code>/v1/darkweb/ransomware/statistics</code> -- Trends &amp; Statistics","text":"<p>Get ransomware trends and aggregate statistics.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#example_4","title":"Example","text":"<pre><code>curl http://localhost:8001/v1/darkweb/ransomware/statistics\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#response_3","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"total_victims_30d\": 450,\n    \"top_groups\": [\n      {\"name\": \"lockbit\", \"count\": 120},\n      {\"name\": \"alphv\", \"count\": 85}\n    ],\n    \"top_sectors\": [\n      {\"sector\": \"Healthcare\", \"count\": 65},\n      {\"sector\": \"Technology\", \"count\": 58}\n    ],\n    \"top_countries\": [\n      {\"country\": \"US\", \"count\": 180},\n      {\"country\": \"UK\", \"count\": 45}\n    ]\n  }\n}\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#ransomware-trackers","title":"Ransomware Trackers","text":"<p>Create persistent trackers that monitor for specific organizations and notify via webhook.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#post-v1darkwebransomwaretrackercreate-create-tracker","title":"POST <code>/v1/darkweb/ransomware/tracker/create</code> -- Create Tracker","text":"<pre><code>{\n  \"name\": \"Our Company Monitor\",\n  \"domains\": [\"ourcompany.com\", \"oursubsidiary.com\"],\n  \"keywords\": [\"Our Company\", \"OurCompany Inc\"],\n  \"notify_webhook\": \"https://hooks.slack.com/...\",\n  \"check_interval_hours\": 6\n}\n</code></pre> Parameter Type Required Description <code>name</code> string yes Human-readable tracker name <code>domains</code> array yes Domains to watch <code>keywords</code> array no Additional keywords to match <code>notify_webhook</code> string no Webhook URL for alerts <code>check_interval_hours</code> int no Check frequency (default: 6)","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkwebransomwaretrackerlist-list-trackers","title":"GET <code>/v1/darkweb/ransomware/tracker/list</code> -- List Trackers","text":"<p>List all configured ransomware trackers.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#post-v1darkwebransomwaretrackertracker_idrun-run-tracker","title":"POST <code>/v1/darkweb/ransomware/tracker/{tracker_id}/run</code> -- Run Tracker","text":"<p>Manually trigger a tracker check.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#delete-v1darkwebransomwaretrackertracker_id-delete-tracker","title":"DELETE <code>/v1/darkweb/ransomware/tracker/{tracker_id}</code> -- Delete Tracker","text":"<p>Remove a ransomware tracker.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#forum-monitoring","title":"Forum Monitoring","text":"<p>Monitor dark web forums for threat intelligence.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkwebforumlist-list-forums","title":"GET <code>/v1/darkweb/forum/list</code> -- List Forums","text":"<p>List configured dark web forums.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#example_5","title":"Example","text":"<pre><code>curl http://localhost:8001/v1/darkweb/forum/list\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#response_4","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"forums\": [\n      {\n        \"id\": \"forum_1\",\n        \"name\": \"Example Forum\",\n        \"url\": \"http://example...onion\",\n        \"status\": \"active\",\n        \"last_crawled\": \"2024-01-15T10:30:00Z\"\n      }\n    ]\n  }\n}\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#post-v1darkwebforumadd-add-forum","title":"POST <code>/v1/darkweb/forum/add</code> -- Add Forum","text":"<p>Add a new forum to monitoring.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#request-body_1","title":"Request Body","text":"<pre><code>{\n  \"name\": \"New Forum\",\n  \"url\": \"http://newforum...onion\",\n  \"selectors\": {\n    \"thread_list\": \".thread-item\",\n    \"thread_title\": \".title\",\n    \"thread_author\": \".author\",\n    \"thread_date\": \".date\"\n  }\n}\n</code></pre> Parameter Type Required Description <code>name</code> string yes Forum display name <code>url</code> string yes Onion URL <code>selectors</code> object yes CSS selectors for thread extraction","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkwebforumselectors-default-selectors","title":"GET <code>/v1/darkweb/forum/selectors</code> -- Default Selectors","text":"<p>Get default CSS selectors for common forum software (phpBB, XenForo, etc.).</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#post-v1darkwebforumforum_idcapture-capture-content","title":"POST <code>/v1/darkweb/forum/{forum_id}/capture</code> -- Capture Content","text":"<p>Capture the current state of a forum's thread listing.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#delete-v1darkwebforumforum_id-remove-forum","title":"DELETE <code>/v1/darkweb/forum/{forum_id}</code> -- Remove Forum","text":"<p>Remove a forum from monitoring.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#forum-monitors","title":"Forum Monitors","text":"<p>Create keyword-based monitors that scan forum posts and alert via webhook.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#post-v1darkwebforummonitorcreate-create-monitor","title":"POST <code>/v1/darkweb/forum/monitor/create</code> -- Create Monitor","text":"<pre><code>{\n  \"name\": \"Credential Leak Monitor\",\n  \"forums\": [\"forum_1\", \"forum_2\"],\n  \"keywords\": [\"database\", \"leak\", \"credentials\"],\n  \"notify_webhook\": \"https://hooks.slack.com/...\"\n}\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkwebforummonitorlist-list-monitors","title":"GET <code>/v1/darkweb/forum/monitor/list</code> -- List Monitors","text":"<p>List all forum keyword monitors.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#post-v1darkwebforummonitormonitor_idrun-run-monitor","title":"POST <code>/v1/darkweb/forum/monitor/{monitor_id}/run</code> -- Run Monitor","text":"<p>Manually trigger a forum monitor check.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#delete-v1darkwebforummonitormonitor_id-delete-monitor","title":"DELETE <code>/v1/darkweb/forum/monitor/{monitor_id}</code> -- Delete Monitor","text":"<p>Remove a forum monitor.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#paste-site-monitoring","title":"Paste Site Monitoring","text":"<p>Search and monitor paste sites for leaked credentials and sensitive data.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#post-v1darkwebpastesearch-search-pastes","title":"POST <code>/v1/darkweb/paste/search</code> -- Search Pastes","text":"<p>Search paste sites for leaked data using keywords and pattern matchers.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#request-body_2","title":"Request Body","text":"<pre><code>{\n  \"query\": \"example.com\",\n  \"patterns\": [\"email\", \"password\", \"api_key\"],\n  \"date_from\": \"2024-01-01\",\n  \"limit\": 100\n}\n</code></pre> Parameter Type Required Description <code>query</code> string yes Search term (domain, keyword, etc.) <code>patterns</code> array no Pattern matchers to apply (see table below) <code>date_from</code> string no Start date (<code>YYYY-MM-DD</code>) <code>limit</code> int no Maximum results (default: 100)","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#example_6","title":"Example","text":"curlPython <pre><code>curl -X POST http://localhost:8001/v1/darkweb/paste/search \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"example.com\", \"patterns\": [\"email\", \"password\", \"api_key\"]}'\n</code></pre> <pre><code>import requests\n\nresp = requests.post(\"http://localhost:8001/v1/darkweb/paste/search\", json={\n    \"query\": \"example.com\",\n    \"patterns\": [\"email\", \"password\", \"api_key\"],\n})\nfor paste in resp.json()[\"data\"][\"pastes\"]:\n    print(f\"{paste['site']} | {paste['title']} | matches: {paste['matches']}\")\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#response_5","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"pastes\": [\n      {\n        \"id\": \"abc123\",\n        \"site\": \"pastebin\",\n        \"title\": \"Database dump\",\n        \"preview\": \"user@example.com:password123...\",\n        \"matches\": [\"email\", \"password\"],\n        \"posted_date\": \"2024-01-10\",\n        \"url\": \"https://pastebin.com/abc123\"\n      }\n    ],\n    \"total\": 15\n  }\n}\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkwebpastepatterns-available-patterns","title":"GET <code>/v1/darkweb/paste/patterns</code> -- Available Patterns","text":"<p>Get the list of built-in pattern matchers for paste monitoring.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#example_7","title":"Example","text":"<pre><code>curl http://localhost:8001/v1/darkweb/paste/patterns\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#response_6","title":"Response","text":"Pattern Description <code>email</code> Email addresses <code>password</code> Password patterns <code>api_key</code> API keys (AWS, GitHub, etc.) <code>credit_card</code> Credit card numbers <code>ssn</code> Social Security Numbers <code>private_key</code> RSA / SSH private keys","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#paste-monitors","title":"Paste Monitors","text":"<p>Create persistent paste monitoring jobs that check for new leaks on a schedule.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#post-v1darkwebpastemonitorcreate-create-monitor","title":"POST <code>/v1/darkweb/paste/monitor/create</code> -- Create Monitor","text":"<pre><code>{\n  \"name\": \"Company Leak Monitor\",\n  \"keywords\": [\"@ourcompany.com\", \"ourcompany.internal\"],\n  \"patterns\": [\"email\", \"password\", \"api_key\"],\n  \"notify_webhook\": \"https://hooks.slack.com/...\",\n  \"check_interval_hours\": 1\n}\n</code></pre> Parameter Type Required Description <code>name</code> string yes Monitor display name <code>keywords</code> array yes Keywords to search for <code>patterns</code> array no Pattern matchers to apply <code>notify_webhook</code> string no Webhook URL for alerts <code>check_interval_hours</code> int no Check frequency (default: 1)","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkwebpastemonitorlist-list-monitors","title":"GET <code>/v1/darkweb/paste/monitor/list</code> -- List Monitors","text":"<p>List all paste monitors.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#post-v1darkwebpastemonitormonitor_idrun-run-monitor","title":"POST <code>/v1/darkweb/paste/monitor/{monitor_id}/run</code> -- Run Monitor","text":"<p>Manually trigger a paste monitor check.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#delete-v1darkwebpastemonitormonitor_id-delete-monitor","title":"DELETE <code>/v1/darkweb/paste/monitor/{monitor_id}</code> -- Delete Monitor","text":"<p>Remove a paste monitor.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#ransomware-sites","title":"Ransomware Sites","text":"","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkwebsites-list-ransomware-sites","title":"GET <code>/v1/darkweb/sites</code> -- List Ransomware Sites","text":"<p>List tracked ransomware leak sites with their current status. Data sourced from ransomwatch and other pluggable sources.</p> <p>Parameters:</p> Name Type Default Description <code>group</code> string - Filter by ransomware group <code>status</code> string - Filter by site status <code>source</code> string - Filter by data source <code>limit</code> integer 100 Maximum results <code>offset</code> integer 0 Pagination offset <pre><code>curl \"http://localhost:8001/v1/darkweb/sites?group=lockbit&amp;limit=20\"\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkwebsitessite_id-get-site-details","title":"GET <code>/v1/darkweb/sites/{site_id}</code> -- Get Site Details","text":"<p>Get details for a specific ransomware site including status and history.</p> <pre><code>curl http://localhost:8001/v1/darkweb/sites/abc123\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkwebsitesgroups-list-ransomware-groups","title":"GET <code>/v1/darkweb/sites/groups</code> -- List Ransomware Groups","text":"<p>List ransomware groups with per-group statistics including site counts and uptime.</p> Name Type Default Description <code>limit</code> integer 100 Maximum results <code>offset</code> integer 0 Pagination offset <pre><code>curl http://localhost:8001/v1/darkweb/sites/groups\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkwebsitessources-list-data-sources","title":"GET <code>/v1/darkweb/sites/sources</code> -- List Data Sources","text":"<p>List all registered ransomware data sources with their status.</p> <pre><code>curl http://localhost:8001/v1/darkweb/sites/sources\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkwebsitesstats-site-statistics","title":"GET <code>/v1/darkweb/sites/stats</code> -- Site Statistics","text":"<p>Get aggregate ransomware site statistics: total tracked, online/offline counts, sites by group, historical uptime.</p> <pre><code>curl http://localhost:8001/v1/darkweb/sites/stats\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#post-v1darkwebsitescheck-check-site-status","title":"POST <code>/v1/darkweb/sites/check</code> -- Check Site Status","text":"<p>Probe a ransomware site via Tor. Returns online/offline status, response time, captcha detection, and JavaScript requirements.</p> <pre><code>curl -X POST http://localhost:8001/v1/darkweb/sites/check \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\": \"http://example.onion\"}'\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#post-v1darkwebsitesrefresh-refresh-site-status","title":"POST <code>/v1/darkweb/sites/refresh</code> -- Refresh Site Status","text":"<p>Bulk-check site availability. Limited to prevent overwhelming the Tor network.</p> Name Type Default Description <code>limit</code> integer 10 Maximum sites to check <code>group</code> string - Filter by group <pre><code>curl -X POST \"http://localhost:8001/v1/darkweb/sites/refresh?limit=10&amp;group=lockbit\"\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#post-v1darkwebsitesscrape-scrape-onion-site","title":"POST <code>/v1/darkweb/sites/scrape</code> -- Scrape .onion Site","text":"<p>Scrape content from an .onion site via Tor using aiohttp-socks. Returns HTML content, extracted text, page title, metadata, all links, and images.</p> <pre><code>curl -X POST http://localhost:8001/v1/darkweb/sites/scrape \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\": \"http://example.onion\"}'\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#post-v1darkwebsitesbulk-scrape-bulk-scrape-sites","title":"POST <code>/v1/darkweb/sites/bulk-scrape</code> -- Bulk Scrape Sites","text":"<p>Scrape multiple .onion sites in parallel with rate limiting. Extracts victim/company data from HTML and compares with upstream feeds.</p> <pre><code>curl -X POST http://localhost:8001/v1/darkweb/sites/bulk-scrape \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"urls\": [\"http://site1.onion\", \"http://site2.onion\"]}'\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#post-v1darkwebsitesscreenshot-capture-site-screenshot","title":"POST <code>/v1/darkweb/sites/screenshot</code> -- Capture Site Screenshot","text":"<p>Capture a screenshot of an .onion site via Tor using Camoufox browser. Screenshots are saved to <code>/app/data/screenshots/</code>.</p> <pre><code>curl -X POST http://localhost:8001/v1/darkweb/sites/screenshot \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\": \"http://example.onion\"}'\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#post-v1darkwebsitesscreenshotsbulk-bulk-screenshots","title":"POST <code>/v1/darkweb/sites/screenshots/bulk</code> -- Bulk Screenshots","text":"<p>Capture screenshots of multiple .onion sites in parallel with concurrency limits.</p> <pre><code>curl -X POST http://localhost:8001/v1/darkweb/sites/screenshots/bulk \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"urls\": [\"http://site1.onion\", \"http://site2.onion\"]}'\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#post-v1darkwebsitesextract-victims-extract-victims-from-html","title":"POST <code>/v1/darkweb/sites/extract-victims</code> -- Extract Victims from HTML","text":"<p>Parse HTML using group-specific patterns to extract victim data: company names, websites/domains, countries, data sizes, publication dates.</p> <pre><code>curl -X POST \"http://localhost:8001/v1/darkweb/sites/extract-victims?html=&lt;html&gt;...&lt;/html&gt;&amp;group_name=lockbit\"\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkwebsitesexportonion-export-onion-urls","title":"GET <code>/v1/darkweb/sites/export/onion</code> -- Export .onion URLs","text":"<p>Export all ransomware .onion addresses aggregated from all sources.</p> Name Type Default Description <code>available_only</code> boolean false Only include online sites <code>format</code> string json Export format: <code>json</code>, <code>csv</code>, <code>txt</code> <pre><code>curl \"http://localhost:8001/v1/darkweb/sites/export/onion?format=txt&amp;available_only=true\"\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkwebsitesstream-stream-status-changes-sse","title":"GET <code>/v1/darkweb/sites/stream</code> -- Stream Status Changes (SSE)","text":"<p>Stream ransomware site status changes via Server-Sent Events. Emits events when sites go online/offline.</p> <pre><code>const evtSource = new EventSource(\"http://localhost:8001/v1/darkweb/sites/stream\");\nevtSource.onmessage = (event) =&gt; {\n  const data = JSON.parse(event.data);\n  console.log(`${data.group}: ${data.status}`);\n};\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#change-detection","title":"Change Detection","text":"","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#post-v1darkwebsitesdetect-change-detect-site-change","title":"POST <code>/v1/darkweb/sites/detect-change</code> -- Detect Site Change","text":"<p>Detect if a site's content has changed since last check. Compares content hash with stored hash. If changed, records the change in history, emits SSE event, and stores new hash.</p> <pre><code>curl -X POST http://localhost:8001/v1/darkweb/sites/detect-change \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\": \"http://example.onion\"}'\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#post-v1darkwebsitesdetect-changesbulk-bulk-change-detection","title":"POST <code>/v1/darkweb/sites/detect-changes/bulk</code> -- Bulk Change Detection","text":"<p>Check multiple sites for content changes. Returns summary of sites that changed (with diff info), sites unchanged, first-time checks (baseline established), and failed checks.</p> <pre><code>curl -X POST http://localhost:8001/v1/darkweb/sites/detect-changes/bulk \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"urls\": [\"http://site1.onion\", \"http://site2.onion\"]}'\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkwebsiteschangesrecent-recent-changes","title":"GET <code>/v1/darkweb/sites/changes/recent</code> -- Recent Changes","text":"<p>Get chronologically sorted list of recent content changes across all monitored ransomware leak sites. Useful for alerting dashboards.</p> Name Type Default Description <code>limit</code> integer 100 Maximum results <pre><code>curl \"http://localhost:8001/v1/darkweb/sites/changes/recent?limit=50\"\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkwebsiteschangeshistoryurl-site-change-history","title":"GET <code>/v1/darkweb/sites/changes/history/{url}</code> -- Site Change History","text":"<p>Get change history for a specific site: old/new content hashes, timestamps, content length differences.</p> Name Type Default Description <code>url</code> string (required) Site URL <code>limit</code> integer 50 Maximum results <pre><code>curl \"http://localhost:8001/v1/darkweb/sites/changes/history/http%3A%2F%2Fexample.onion\"\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#victims","title":"Victims","text":"","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkwebvictims-list-ransomware-victims","title":"GET <code>/v1/darkweb/victims</code> -- List Ransomware Victims","text":"<p>List companies claimed by ransomware groups on their leak sites. Data sourced from ransomwatch and ransomlook.</p> <p>Parameters:</p> Name Type Default Description <code>group</code> string - Filter by ransomware group <code>days</code> string - Filter by time period <code>limit</code> integer 100 Maximum results <code>offset</code> integer 0 Pagination offset <pre><code>curl \"http://localhost:8001/v1/darkweb/victims?group=lockbit&amp;days=30&amp;limit=50\"\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkwebvictimssearch-search-victims","title":"GET <code>/v1/darkweb/victims/search</code> -- Search Victims","text":"<p>Search ransomware victims by company name to find if a specific company has been targeted.</p> Name Type Default Description <code>q</code> string (required) Search query <code>limit</code> integer 50 Maximum results <pre><code>curl \"http://localhost:8001/v1/darkweb/victims/search?q=acme+corp\"\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkwebvictimsstats-victim-statistics","title":"GET <code>/v1/darkweb/victims/stats</code> -- Victim Statistics","text":"<p>Get aggregate victim statistics: total victims in period, top ransomware groups by victim count, daily victim counts.</p> Name Type Default Description <code>days</code> integer 30 Time period in days <pre><code>curl \"http://localhost:8001/v1/darkweb/victims/stats?days=90\"\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#data-sync-feeds","title":"Data Sync &amp; Feeds","text":"","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkwebfeeds-list-ransomware-feeds","title":"GET <code>/v1/darkweb/feeds</code> -- List Ransomware Feeds","text":"<p>List all raw data feed URLs for ransomware intelligence. These are the upstream sources where data is pulled from.</p> <pre><code>curl http://localhost:8001/v1/darkweb/feeds\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#post-v1darkwebsync-sync-all-data","title":"POST <code>/v1/darkweb/sync</code> -- Sync All Data","text":"<p>Sync ALL ransomware data from upstream sources (ransomware.live, ransomwatch, ransomlook) to local Valkey storage.</p> <pre><code>curl -X POST http://localhost:8001/v1/darkweb/sync\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#post-v1darkwebmerge-merge-new-from-upstream","title":"POST <code>/v1/darkweb/merge</code> -- Merge New from Upstream","text":"<p>Check upstream for NEW URLs and merge them into local database without replacing existing data. Finds URLs/victims not present locally and appends them.</p> <pre><code>curl -X POST http://localhost:8001/v1/darkweb/merge\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkwebdiff-diff-with-upstream","title":"GET <code>/v1/darkweb/diff</code> -- Diff with Upstream","text":"<p>Check what's NEW in upstream sources vs the local database. Compares local Valkey data with ransomware.live, ransomwatch, ransomlook and shows missing URLs/victims. Does NOT modify data.</p> <pre><code>curl http://localhost:8001/v1/darkweb/diff\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#local-storage","title":"Local Storage","text":"<p>Local-First</p> <p>Local endpoints return data from Valkey storage with no external API calls. Run <code>/v1/darkweb/sync</code> first to populate local data.</p>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkweblocalstatus-local-sync-status","title":"GET <code>/v1/darkweb/local/status</code> -- Local Sync Status","text":"<p>Shows when data was last synced and counts of local sites/victims.</p> <pre><code>curl http://localhost:8001/v1/darkweb/local/status\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkweblocalsites-local-sites","title":"GET <code>/v1/darkweb/local/sites</code> -- Local Sites","text":"<p>Get ransomware sites from local storage.</p> Name Type Default Description <code>onion_only</code> boolean false Only return .onion sites <code>limit</code> integer 1000 Maximum results <pre><code>curl \"http://localhost:8001/v1/darkweb/local/sites?onion_only=true\"\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkweblocalvictims-local-victims","title":"GET <code>/v1/darkweb/local/victims</code> -- Local Victims","text":"<p>Get ransomware victims from local storage.</p> Name Type Default Description <code>limit</code> integer 1000 Maximum results <pre><code>curl \"http://localhost:8001/v1/darkweb/local/victims?limit=500\"\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#get-v1darkweblocalonion-local-onion-urls","title":"GET <code>/v1/darkweb/local/onion</code> -- Local .onion URLs","text":"<p>Get all .onion URLs from local storage.</p> Name Type Default Description <code>format</code> string json Export format: <code>json</code>, <code>txt</code> <pre><code>curl \"http://localhost:8001/v1/darkweb/local/onion?format=txt\"\n</code></pre>","tags":["Dark Web","OSINT"]},{"location":"api/darkweb/#use-cases","title":"Use Cases","text":"Audience Purpose Security teams Monitor for leaked credentials and data breaches Threat intelligence Track ransomware campaigns targeting your industry Brand protection Detect mentions of your organization on the dark web Incident response Quickly identify if your organization has been compromised","tags":["Dark Web","OSINT"]},{"location":"api/driftnet/","title":"Driftnet OSINT Endpoints","text":"<p>The Driftnet endpoints provide comprehensive internet intelligence via the Driftnet.io API. With 25 endpoints spanning DNS, WHOIS, IP intelligence, scanning, certificate transparency, and vulnerability data, Driftnet offers deep OSINT capabilities for security research and threat intelligence.</p>","tags":["OSINT"]},{"location":"api/driftnet/#configuration","title":"Configuration","text":"<p>API Key Required</p> <p>All Driftnet endpoints require a <code>DRIFTNET_API_KEY</code> to be configured.</p> Environment Variable<pre><code># Add to .env or environment variables\nDRIFTNET_API_KEY=your_api_key_here\n</code></pre> <p>Base URL</p> <p>All endpoints use the base URL <code>http://localhost:8001</code>.</p>","tags":["OSINT"]},{"location":"api/driftnet/#endpoint-categories","title":"Endpoint Categories","text":"Category Endpoints Description DNS 4 Forward/reverse DNS, MX, TXT records Domain 3 WHOIS, reverse WHOIS, comprehensive intel IP 7 WHOIS, BGP, abuse, routes, reverse lookup Scanning 5 Protocol scans, ports, JARM, favicons Certificate Transparency 2 CT log search, subdomain enumeration Multi-Source 1 Cross-source correlation Reference 1 CVE lookups Suggestions 2 Autocomplete for domains and scan values","tags":["OSINT"]},{"location":"api/driftnet/#dns-endpoints","title":"DNS Endpoints","text":"","tags":["OSINT"]},{"location":"api/driftnet/#get-v1driftnetdnsforward","title":"GET /v1/driftnet/dns/forward","text":"<p>Forward DNS lookups (A/AAAA records) with historical data support.</p> <p>Best For</p> <p>Finding IP addresses for hostnames, tracking DNS history over time, and correlating hostnames to IPs.</p> <p>Parameters:</p> Name Type Description <code>host</code> string Hostname to lookup <code>ip</code> string IP address or CIDR to search <code>nameserver</code> string Filter by nameserver <code>summarize</code> string Field to summarize (e.g., 'ip', 'host') <code>from</code> string Start date (YYYY-MM-DD) <code>to</code> string End date (YYYY-MM-DD) <code>page</code> integer Result page (0-indexed) curl <pre><code># Find all IPs that have hosted google.com\ncurl \"http://localhost:8001/v1/driftnet/dns/forward?host=google.com\"\n\n# Find all domains resolving to a specific IP\ncurl \"http://localhost:8001/v1/driftnet/dns/forward?ip=8.8.8.8\"\n\n# Summarize unique IPs for a domain\ncurl \"http://localhost:8001/v1/driftnet/dns/forward?host=example.com&amp;summarize=ip\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"page\": 0,\n    \"pages\": 134,\n    \"result_count\": 13330,\n    \"results\": [\n      {\n        \"date\": \"2026-02-02\",\n        \"items\": [\n          {\"type\": \"host\", \"value\": \"mail.google.com\"},\n          {\"type\": \"ip\", \"value\": \"209.85.128.238\", \"context\": \"dns-a\"},\n          {\"type\": \"host\", \"value\": \"ns4.google.com\", \"context\": \"dns-ns\"}\n        ]\n      }\n    ]\n  }\n}\n</code></pre> <p>Response Fields:</p> Field Type Description <code>page</code> integer Current page number (0-indexed) <code>pages</code> integer Total number of pages <code>result_count</code> integer Total result count <code>results[].date</code> string Date of the DNS record observation <code>results[].items</code> array DNS record items with type, value, and context","tags":["OSINT"]},{"location":"api/driftnet/#get-v1driftnetdnsreverse","title":"GET /v1/driftnet/dns/reverse","text":"<p>Reverse DNS (PTR) lookups for finding hostnames associated with IP addresses.</p> <p>Best For</p> <p>Identifying infrastructure ownership, verifying PTR records, and discovering related infrastructure.</p> <p>Parameters:</p> Name Type Description <code>ip</code> string IP address to lookup PTR <code>host</code> string PTR hostname to search <code>from</code> string Start date <code>to</code> string End date <code>page</code> integer Result page <pre><code>curl \"http://localhost:8001/v1/driftnet/dns/reverse?ip=8.8.8.8\"\n</code></pre>","tags":["OSINT"]},{"location":"api/driftnet/#get-v1driftnetdnsmx","title":"GET /v1/driftnet/dns/mx","text":"<p>MX record lookups for finding mail servers associated with domains.</p> <p>Best For</p> <p>Email infrastructure mapping, verifying MX configuration, and identifying email service providers.</p> <p>Parameters:</p> Name Type Description <code>host</code> string Domain to lookup MX records <code>nameserver</code> string Filter by nameserver <code>from</code> string Start date <code>to</code> string End date <code>page</code> integer Result page <pre><code>curl \"http://localhost:8001/v1/driftnet/dns/mx?host=google.com\"\n</code></pre>","tags":["OSINT"]},{"location":"api/driftnet/#get-v1driftnetdnstxt","title":"GET /v1/driftnet/dns/txt","text":"<p>TXT record lookups (SPF, DKIM, DMARC, and other verification records).</p> <p>Best For</p> <p>Analyzing email security configuration, checking SPF/DKIM/DMARC posture, and finding domain verification tokens.</p> <p>Parameters:</p> Name Type Description <code>host</code> string Domain to lookup TXT records <code>txt</code> string TXT content to search (e.g., 'v=spf1') <code>prefix</code> boolean Treat txt as prefix match <code>from</code> string Start date <code>to</code> string End date <code>page</code> integer Result page By DomainBy TXT Content <pre><code>curl \"http://localhost:8001/v1/driftnet/dns/txt?host=google.com\"\n</code></pre> <pre><code># Find all domains with specific SPF configuration\ncurl \"http://localhost:8001/v1/driftnet/dns/txt?txt=v=spf1&amp;prefix=true\"\n</code></pre>","tags":["OSINT"]},{"location":"api/driftnet/#domain-endpoints","title":"Domain Endpoints","text":"","tags":["OSINT"]},{"location":"api/driftnet/#get-v1driftnetdomainwhois","title":"GET /v1/driftnet/domain/whois","text":"<p>Domain WHOIS lookup returning structured registration data.</p> <p>Best For</p> <p>Researching domain ownership, registration dates, registrar information, and brand protection.</p> <p>Parameters:</p> Name Type Description <code>domain</code> string Domain name to lookup (required) <code>extract_apex</code> boolean Extract apex domain from subdomain Basic LookupApex Extraction <pre><code>curl \"http://localhost:8001/v1/driftnet/domain/whois?domain=google.com\"\n</code></pre> <pre><code># Extract apex domain from subdomain\ncurl \"http://localhost:8001/v1/driftnet/domain/whois?domain=mail.google.com&amp;extract_apex=true\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"domain\": {\n      \"name\": \"google.com\",\n      \"created\": \"1997-09-15T04:00:00Z\",\n      \"expires\": \"2028-09-14T04:00:00Z\",\n      \"registrar_name\": \"MarkMonitor Inc.\",\n      \"nameservers\": [\"ns1.google.com\", \"ns2.google.com\", \"ns3.google.com\", \"ns4.google.com\"],\n      \"status\": \"clientDeleteProhibited,clientTransferProhibited,clientUpdateProhibited\"\n    }\n  }\n}\n</code></pre> <p>Response Fields:</p> Field Type Description <code>domain.name</code> string Domain name <code>domain.created</code> string Registration date (ISO 8601) <code>domain.expires</code> string Expiration date (ISO 8601) <code>domain.registrar_name</code> string Registrar name <code>domain.nameservers</code> array Authoritative nameservers <code>domain.status</code> string Domain status codes","tags":["OSINT"]},{"location":"api/driftnet/#get-v1driftnetdomainreverse","title":"GET /v1/driftnet/domain/reverse","text":"<p>Reverse WHOIS search -- find domains by registrant information.</p> <p>Best For</p> <p>Mapping threat actor infrastructure, finding all domains owned by an entity, and brand protection.</p> <p>Parameters:</p> Name Type Description <code>name</code> string Registrant name <code>email</code> string Registrant email <code>phone</code> string Registrant phone <code>nameserver</code> string Nameserver <code>phrase</code> string Search phrase in any field <code>prefix</code> boolean Treat as prefix match <code>created_from</code> string Domain creation start date <code>created_to</code> string Domain creation end date <code>page</code> integer Result page By NameBy EmailBy Nameserver <pre><code>curl \"http://localhost:8001/v1/driftnet/domain/reverse?name=Google\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/driftnet/domain/reverse?email=admin@example.com\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/driftnet/domain/reverse?nameserver=ns1.google.com\"\n</code></pre>","tags":["OSINT"]},{"location":"api/driftnet/#get-v1driftnetdomainintel","title":"GET /v1/driftnet/domain/intel","text":"<p>Comprehensive domain intelligence combining WHOIS, DNS, MX, TXT, and certificate data in a single call.</p> <p>Best For</p> <p>Full domain investigation, rapid domain triage, and comprehensive domain profiling.</p> <p>Parameters:</p> Name Type Description <code>domain</code> string Domain to analyze (required) <pre><code>curl \"http://localhost:8001/v1/driftnet/domain/intel?domain=cloudflare.com\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"domain\": \"cloudflare.com\",\n    \"whois\": {},\n    \"dns\": {},\n    \"mx\": {},\n    \"txt\": {},\n    \"certs\": {}\n  }\n}\n</code></pre>","tags":["OSINT"]},{"location":"api/driftnet/#ip-intelligence-endpoints","title":"IP Intelligence Endpoints","text":"","tags":["OSINT"]},{"location":"api/driftnet/#get-v1driftnetipwhois","title":"GET /v1/driftnet/ip/whois","text":"<p>IP WHOIS lookup for finding ownership, network blocks, and allocation information.</p> <p>Best For</p> <p>IP investigation, network allocation research, and infrastructure mapping.</p> <p>Parameters:</p> Name Type Description <code>ip</code> string IP address to lookup (required) <pre><code>curl \"http://localhost:8001/v1/driftnet/ip/whois?ip=8.8.8.8\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"net\": {\n      \"cidr\": \"8.8.8.0/24\",\n      \"net_name\": \"GOGL\",\n      \"registry\": \"ARIN\"\n    },\n    \"org\": {\n      \"name\": \"Google LLC\",\n      \"address\": [\"1600 Amphitheatre Parkway\", \"Mountain View\", \"CA\", \"94043\", \"US\"]\n    }\n  }\n}\n</code></pre> <p>Response Fields:</p> Field Type Description <code>net.cidr</code> string Network CIDR block <code>net.net_name</code> string Network name <code>net.registry</code> string Regional Internet Registry (ARIN, RIPE, APNIC, etc.) <code>org.name</code> string Organization name <code>org.address</code> array Organization address components","tags":["OSINT"]},{"location":"api/driftnet/#get-v1driftnetipsummary","title":"GET /v1/driftnet/ip/summary","text":"<p>Quick IP summary with ownership information.</p> <p>Best For</p> <p>Fast lookup of IP ownership without full WHOIS details.</p> <p>Parameters:</p> Name Type Description <code>ip</code> string IP address (required) <pre><code>curl \"http://localhost:8001/v1/driftnet/ip/summary?ip=8.8.8.8\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"domain\": {\"apex_domain\": \"dns.google\", \"cidr\": \"8.8.8.8/32\", \"context\": \"rdns\"},\n    \"entity\": {\"cidr\": \"8.8.8.0/24\", \"context\": \"bgpasn\", \"name\": \"Google LLC\"}\n  }\n}\n</code></pre>","tags":["OSINT"]},{"location":"api/driftnet/#get-v1driftnetipabuse","title":"GET /v1/driftnet/ip/abuse","text":"<p>Get abuse contact information for an IP address.</p> <p>Best For</p> <p>Finding contacts for abuse reports, coordinating takedowns, and reporting malicious activity.</p> <p>Parameters:</p> Name Type Description <code>ip</code> string IP address (required) <code>restrict</code> string Restrict to 'whois' or 'bgp' <pre><code>curl \"http://localhost:8001/v1/driftnet/ip/abuse?ip=8.8.8.8\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"cidr\": \"8.8.8.0/24\",\n    \"emails\": [\"network-abuse@google.com\"],\n    \"contexts\": [\"bgpasn\", \"net\"]\n  }\n}\n</code></pre>","tags":["OSINT"]},{"location":"api/driftnet/#get-v1driftnetipbgp","title":"GET /v1/driftnet/ip/bgp","text":"<p>BGP/Prefix WHOIS for AS information, BGP routing data, and CIDR allocations.</p> <p>Best For</p> <p>AS information lookups, BGP routing analysis, and network allocation research.</p> <p>Parameters:</p> Name Type Description <code>ip</code> string IP address <code>asn</code> string AS number (e.g., 'AS15169') <pre><code>curl \"http://localhost:8001/v1/driftnet/ip/bgp?ip=8.8.8.8\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"as_name\": \"GOOGLE\",\n    \"as_number\": \"AS15169\",\n    \"cidr\": \"8.8.8.0/24\",\n    \"name\": \"Google LLC\",\n    \"registry\": \"ARIN\"\n  }\n}\n</code></pre> <p>Response Fields:</p> Field Type Description <code>as_name</code> string Autonomous System name <code>as_number</code> string AS number <code>cidr</code> string Network CIDR block <code>name</code> string Organization name <code>registry</code> string Regional Internet Registry","tags":["OSINT"]},{"location":"api/driftnet/#get-v1driftnetiproutes","title":"GET /v1/driftnet/ip/routes","text":"<p>BGP routing information for an IP or ASN.</p> <p>Parameters:</p> Name Type Description <code>ip</code> string IP address <code>asn</code> string AS number <pre><code>curl \"http://localhost:8001/v1/driftnet/ip/routes?ip=8.8.8.8\"\n</code></pre>","tags":["OSINT"]},{"location":"api/driftnet/#get-v1driftnetipreverse","title":"GET /v1/driftnet/ip/reverse","text":"<p>Reverse IP WHOIS search to find IP ranges owned by an organization.</p> <p>Best For</p> <p>Discovering all IP ranges owned by an organization, asset inventory, and infrastructure mapping.</p> <p>Parameters:</p> Name Type Description <code>name</code> string Organization name <code>domain</code> string Associated domain <code>email</code> string Contact email <code>address</code> string Physical address <code>phone</code> string Phone number <code>outer_only</code> boolean Only return outer ranges <code>context</code> string Filter by 'whois' or 'bgp' <code>page</code> integer Result page <pre><code>curl \"http://localhost:8001/v1/driftnet/ip/reverse?name=Google\"\n</code></pre>","tags":["OSINT"]},{"location":"api/driftnet/#get-v1driftnetipintel","title":"GET /v1/driftnet/ip/intel","text":"<p>Comprehensive IP intelligence combining WHOIS, BGP, abuse contacts, and port scan data in one call.</p> <p>Best For</p> <p>Complete IP investigation, rapid IP triage, and bulk IP analysis.</p> <p>Parameters:</p> Name Type Description <code>ip</code> string IP address to analyze (required) <pre><code>curl \"http://localhost:8001/v1/driftnet/ip/intel?ip=8.8.8.8\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"ip\": \"8.8.8.8\",\n    \"summary\": {},\n    \"whois\": {},\n    \"abuse\": {},\n    \"ports\": {},\n    \"bgp\": {}\n  }\n}\n</code></pre>","tags":["OSINT"]},{"location":"api/driftnet/#scanning-endpoints","title":"Scanning Endpoints","text":"","tags":["OSINT"]},{"location":"api/driftnet/#get-v1driftnetscanprotocols","title":"GET /v1/driftnet/scan/protocols","text":"<p>Protocol scan data for finding hosts by server software, HTTP headers, page titles, and more.</p> <p>Best For</p> <p>Finding vulnerable services, tracking C2 infrastructure, and service discovery.</p> <p>Parameters:</p> Name Type Description <code>ip</code> string IP address or CIDR <code>query</code> string Free-text search <code>field</code> string Field search (e.g., 'server:nginx') <code>expression</code> string Complex query expression <code>filter</code> string Post-filter results <code>most_recent</code> boolean Dedupe to most recent <code>summarize</code> string Field to summarize <code>summary_limit</code> integer Max summary values <code>from</code> string Start date <code>to</code> string End date <code>page</code> integer Result page By SoftwareBy TitleBy IP <pre><code># Find all IPs running nginx\ncurl \"http://localhost:8001/v1/driftnet/scan/protocols?field=server:nginx\"\n</code></pre> <pre><code># Find Apache servers with specific title\ncurl \"http://localhost:8001/v1/driftnet/scan/protocols?field=title:admin&amp;query=apache\"\n</code></pre> <pre><code># Get scan data for specific IP\ncurl \"http://localhost:8001/v1/driftnet/scan/protocols?ip=8.8.8.8\"\n</code></pre>","tags":["OSINT"]},{"location":"api/driftnet/#get-v1driftnetscanports","title":"GET /v1/driftnet/scan/ports","text":"<p>Open ports summary with honeypot detection.</p> <p>Best For</p> <p>Quick view of open ports for an IP or CIDR range.</p> <p>Parameters:</p> Name Type Description <code>ip</code> string IP address or CIDR (required) <pre><code>curl \"http://localhost:8001/v1/driftnet/scan/ports?ip=8.8.8.8\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"other\": 0,\n    \"values\": {\"443\": 31, \"53\": 10, \"853\": 10}\n  }\n}\n</code></pre> <p>Response Fields:</p> Field Type Description <code>other</code> integer Count of ports not in top values <code>values</code> object Port numbers mapped to observation counts","tags":["OSINT"]},{"location":"api/driftnet/#get-v1driftnetscandomains","title":"GET /v1/driftnet/scan/domains","text":"<p>Domain/vhost scan data for finding virtual hosts and HTTP service data.</p> <p>Best For</p> <p>Finding virtual hosts, HTTP service data for domains, and infrastructure correlation.</p> <p>Parameters:</p> Name Type Description <code>host</code> string Hostname to search <code>ip</code> string IP address <code>query</code> string Free-text search <code>field</code> string Field search (e.g., 'title:admin') <code>summarize</code> string Field to summarize <code>from</code> string Start date <code>to</code> string End date <code>page</code> integer Result page <pre><code>curl \"http://localhost:8001/v1/driftnet/scan/domains?host=github.com\"\n</code></pre>","tags":["OSINT"]},{"location":"api/driftnet/#get-v1driftnetscanjarm","title":"GET /v1/driftnet/scan/jarm","text":"<p>JARM fingerprint lookups for tracking C2 infrastructure and identifying server implementations.</p> <p>Best For</p> <p>Tracking malware C2 servers, server fingerprinting, and infrastructure correlation. Same JARM hash indicates similar server configuration.</p> <p>Parameters:</p> Name Type Description <code>ip</code> string IP address <code>hash</code> string JARM hash to search <code>summarize</code> string Field to summarize (e.g., 'ip') <code>summary_limit</code> integer Max summary values <code>from</code> string Start date <code>to</code> string End date <code>page</code> integer Result page By IPBy Hash <pre><code>curl \"http://localhost:8001/v1/driftnet/scan/jarm?ip=8.8.8.8\"\n</code></pre> <pre><code># Find all IPs with same JARM hash\ncurl \"http://localhost:8001/v1/driftnet/scan/jarm?hash=29d29d00029d29d00042d43d00041d2aa5ce6a70de7ba95aef77a77b00a0af\"\n</code></pre>","tags":["OSINT"]},{"location":"api/driftnet/#get-v1driftnetscanfavicons","title":"GET /v1/driftnet/scan/favicons","text":"<p>Favicon hash lookups for finding related infrastructure by favicon hash.</p> <p>Best For</p> <p>Tracking phishing infrastructure, finding impersonation sites, and infrastructure correlation. Same favicon likely indicates same or related infrastructure.</p> <p>Parameters:</p> Name Type Description <code>sha1</code> string SHA1 hash <code>sha256</code> string SHA256 hash <code>md5</code> string MD5 hash <code>murmur3</code> integer MurmurHash3 (Shodan-compatible format) <code>summarize</code> string Field to summarize <code>from</code> string Start date <code>to</code> string End date <code>page</code> integer Result page <pre><code># Find IPs with same favicon (MurmurHash3 format for Shodan compatibility)\ncurl \"http://localhost:8001/v1/driftnet/scan/favicons?murmur3=-1123257478\"\n</code></pre> <p>Supported Hash Formats</p> <p>Multiple hash formats are supported: SHA1, SHA256, MD5, and MurmurHash3. The MurmurHash3 format is compatible with Shodan's <code>http.favicon.hash</code> filter.</p>","tags":["OSINT"]},{"location":"api/driftnet/#certificate-transparency-endpoints","title":"Certificate Transparency Endpoints","text":"","tags":["OSINT"]},{"location":"api/driftnet/#get-v1driftnetctlog","title":"GET /v1/driftnet/ct/log","text":"<p>Certificate Transparency log searches for finding certificates issued for domains.</p> <p>Best For</p> <p>Certificate reconnaissance, subdomain enumeration via CT logs, and certificate inventory/compliance.</p> <p>Parameters:</p> Name Type Description <code>field</code> string Field search (e.g., 'host:example.com') (required) <code>prefix</code> boolean Treat as prefix match (for wildcards) <code>valid_filter</code> string 'valid-from' or 'valid-to' <code>summarize</code> string Field to summarize (e.g., 'host') <code>summary_limit</code> integer Max summary values <code>from</code> string Start date <code>to</code> string End date <code>page</code> integer Result page Find CertificatesEnumerate SubdomainsBy Organization <pre><code># Find all certificates for a domain\ncurl \"http://localhost:8001/v1/driftnet/ct/log?field=host:github.com\"\n</code></pre> <pre><code># Enumerate subdomains using CT logs\ncurl \"http://localhost:8001/v1/driftnet/ct/log?field=host:example.com&amp;prefix=true&amp;summarize=host\"\n</code></pre> <pre><code># Find certificates by organization\ncurl \"http://localhost:8001/v1/driftnet/ct/log?field=subject:Google\"\n</code></pre>","tags":["OSINT"]},{"location":"api/driftnet/#get-v1driftnetsubdomains","title":"GET /v1/driftnet/subdomains","text":"<p>Enumerate subdomains using CT logs and DNS combined for comprehensive subdomain discovery.</p> <p>Best For</p> <p>Attack surface discovery, asset inventory, scope enumeration for penetration testing and bug bounties.</p> <p>Parameters:</p> Name Type Description <code>domain</code> string Apex domain to enumerate (required) <code>limit</code> integer Max subdomains (default: 1000) <pre><code>curl \"http://localhost:8001/v1/driftnet/subdomains?domain=github.com&amp;limit=100\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"domain\": \"github.com\",\n    \"ct_subdomains\": {},\n    \"dns_subdomains\": {}\n  }\n}\n</code></pre> <p>Response Fields:</p> Field Type Description <code>domain</code> string The queried apex domain <code>ct_subdomains</code> object Subdomains discovered from Certificate Transparency logs <code>dns_subdomains</code> object Subdomains discovered from DNS data","tags":["OSINT"]},{"location":"api/driftnet/#multi-source-intelligence","title":"Multi-Source Intelligence","text":"","tags":["OSINT"]},{"location":"api/driftnet/#get-v1driftnetmultisummary","title":"GET /v1/driftnet/multi/summary","text":"<p>Multi-facet summary across all data types for rapid target assessment.</p> <p>Best For</p> <p>Quick overview of all data related to a target, rapid triage, and initial reconnaissance.</p> <p>Parameters:</p> Name Type Description <code>field</code> string Field search (e.g., 'host:example.com') <code>expression</code> string Query expression <code>summary_limit</code> integer Max summary values <code>timeout</code> integer Timeout in seconds (default: 30) <pre><code>curl \"http://localhost:8001/v1/driftnet/multi/summary?field=ip:8.8.8.8\"\n</code></pre>","tags":["OSINT"]},{"location":"api/driftnet/#reference-data","title":"Reference Data","text":"","tags":["OSINT"]},{"location":"api/driftnet/#get-v1driftnetcve","title":"GET /v1/driftnet/cve","text":"<p>CVE information lookup with CVSS scoring, EPSS probability, and KEV status.</p> <p>Best For</p> <p>Vulnerability research, patch prioritization, and impact assessment.</p> <p>Parameters:</p> Name Type Description <code>id</code> string CVE ID (e.g., 'CVE-2021-44228') (required) Log4ShellRecent CVE <pre><code>curl \"http://localhost:8001/v1/driftnet/cve?id=CVE-2021-44228\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/driftnet/cve?id=CVE-2024-1234\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"id\": \"CVE-2021-44228\",\n    \"cvss_score\": \"10.0\",\n    \"cvss_version\": \"3.1\",\n    \"descriptions\": {\n      \"en\": \"Apache Log4j2 2.0-beta9 through 2.15.0 JNDI features...\"\n    },\n    \"epss_score\": \"0.94358\",\n    \"epss_percentile\": \"0.9996\",\n    \"kev\": {\n      \"date_added\": \"2021-12-10\",\n      \"due_date\": \"2021-12-24\",\n      \"known_ransomware_campaign_use\": \"Known\"\n    }\n  }\n}\n</code></pre> <p>Response Fields:</p> Field Type Description <code>id</code> string CVE identifier <code>cvss_score</code> string CVSS base score (0.0-10.0) <code>cvss_version</code> string CVSS version used for scoring <code>descriptions</code> object CVE descriptions by language <code>epss_score</code> string EPSS probability of exploitation (0-1) <code>epss_percentile</code> string EPSS percentile ranking <code>kev.date_added</code> string Date added to CISA KEV catalog <code>kev.due_date</code> string Required remediation date <code>kev.known_ransomware_campaign_use</code> string Whether used in ransomware campaigns <p>Scoring Systems</p> <ul> <li>CVSS: Common Vulnerability Scoring System -- measures severity (0-10)</li> <li>EPSS: Exploit Prediction Scoring System -- predicts likelihood of exploitation (0-1)</li> <li>KEV: CISA Known Exploited Vulnerabilities -- indicates active exploitation in the wild</li> </ul>","tags":["OSINT"]},{"location":"api/driftnet/#suggestion-endpoints","title":"Suggestion Endpoints","text":"","tags":["OSINT"]},{"location":"api/driftnet/#get-v1driftnetsuggestdomains","title":"GET /v1/driftnet/suggest/domains","text":"<p>Domain autocomplete suggestions for building search interfaces.</p> <p>Parameters:</p> Name Type Description <code>prefix</code> string Search prefix (required) <code>limit</code> integer Max suggestions (default: 20) <pre><code>curl \"http://localhost:8001/v1/driftnet/suggest/domains?prefix=goo&amp;limit=5\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"result_count\": 5,\n    \"results\": [\n      {\"domain_display\": \"google.com\", \"subdomain_count\": 14191},\n      {\"domain_display\": \"good50.com\", \"subdomain_count\": 27955}\n    ]\n  }\n}\n</code></pre> <p>Response Fields:</p> Field Type Description <code>result_count</code> integer Number of suggestions returned <code>results[].domain_display</code> string Domain name <code>results[].subdomain_count</code> integer Number of known subdomains","tags":["OSINT"]},{"location":"api/driftnet/#get-v1driftnetsuggestscan","title":"GET /v1/driftnet/suggest/scan","text":"<p>Scan value autocomplete suggestions for building scan search interfaces.</p> <p>Parameters:</p> Name Type Description <code>prefix</code> string Search prefix (required) <code>type</code> string Value type (e.g., 'port-tcp') <code>limit</code> integer Max suggestions (default: 10) <pre><code>curl \"http://localhost:8001/v1/driftnet/suggest/scan?prefix=nginx&amp;limit=5\"\n</code></pre>","tags":["OSINT"]},{"location":"api/intelligence/","title":"Intelligence Endpoints","text":"<p>The Intelligence API provides a comprehensive suite of endpoints for security research, threat intelligence, domain reconnaissance, and vulnerability assessment. These endpoints combine local analysis capabilities with integrations to external intelligence platforms.</p>","tags":["OSINT","Security"]},{"location":"api/intelligence/#overview","title":"Overview","text":"Category Endpoints API Keys Required Domain Intelligence DNS, WHOIS, Domain Info, Subdomains, Tech Detection No Security Analysis Security Headers, TLS/SSL Audit No Threat Intelligence Multi-source Threat Check, IP Reputation Optional (enhanced with keys) Social &amp; Entity Social Profile Search, Entity Graph Extraction No TLS Fingerprinting JARM, JA3S, JA4, JA4S, Lookup No Censys Host Search, Host Details, Certificate Search Yes Vulners CVE Search, Exploit Search, Product Vulns Yes SSL Labs SSL Grade, Full Analysis No abuse.ch URLhaus, Malware Bazaar, ThreatFox No GreyNoise IP Noise/RIOT Check No (Community API) AlienVault OTX IP/Domain/Hash Indicators, Pulses Yes <p>Base URL</p> <p>All endpoints use the base URL <code>http://localhost:8001</code>.</p>","tags":["OSINT","Security"]},{"location":"api/intelligence/#breach-intelligence","title":"Breach Intelligence","text":"","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelbreach","title":"GET /v1/intel/breach","text":"<p>Search for data breach notifications across state AG offices, SEC filings, and news sources.</p> <p>Best For</p> <p>Researching security incidents, compliance monitoring, third-party risk assessment, and cyber insurance underwriting.</p> <p>Parameters:</p> Name Type Description <code>company</code> string Company name <code>year</code> integer Year of breach <code>state</code> string US state code (CA, NY, etc.) <code>breach_type</code> string Type: ransomware, phishing, etc. <code>industry</code> string Industry: healthcare, finance, etc. <code>include_ransomware</code> boolean Include ransomware sources <code>limit</code> integer Maximum results (default: 30) <code>cursor</code> string Pagination cursor <code>fields</code> string Fields to return curlPython <pre><code>curl \"http://localhost:8001/v1/intel/breach?company=Acme&amp;year=2024&amp;industry=healthcare\"\n</code></pre> <pre><code>import requests\n\ndef assess_vendor_breaches(vendor_name):\n    response = requests.get('http://localhost:8001/v1/intel/breach', params={\n        'company': vendor_name,\n        'limit': 50\n    })\n    data = response.json()['data']\n\n    return {\n        'vendor': vendor_name,\n        'breach_count': data['total'],\n        'incidents': [\n            {\n                'title': r['title'],\n                'url': r['url'],\n                'date': r.get('published_date')\n            }\n            for r in data['results']\n        ]\n    }\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"query\": \"\\\"data breach\\\" OR \\\"security incident\\\" \\\"Acme\\\" 2024 (HIPAA OR healthcare)\",\n    \"filters\": {\n      \"company\": \"Acme\",\n      \"year\": 2024,\n      \"state\": null,\n      \"breach_type\": null,\n      \"industry\": \"healthcare\"\n    },\n    \"results\": [\n      {\n        \"title\": \"Acme Healthcare Data Breach Notification\",\n        \"url\": \"https://...\",\n        \"domain\": \"...\",\n        \"snippet\": \"...\",\n        \"engines\": [\"google\", \"brave\"],\n        \"score\": 15.5,\n        \"entities\": []\n      }\n    ],\n    \"total\": 5,\n    \"engines_used\": [\"google\", \"brave\", \"duckduckgo\"]\n  },\n  \"meta\": {\n    \"request_id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\",\n    \"timestamp\": \"2026-01-31T03:00:00.000000Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 3500\n  }\n}\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/intelligence/#domain-intelligence","title":"Domain Intelligence","text":"","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1inteldns","title":"GET /v1/intel/dns","text":"<p>Perform DNS lookup for a domain, returning IP addresses and CNAME records.</p> <p>Best For</p> <p>Quick domain resolution checks, infrastructure reconnaissance, and CDN detection.</p> <p>Parameters:</p> Name Type Description <code>domain</code> string Domain to lookup (required) curlShell Loop <pre><code>curl \"http://localhost:8001/v1/intel/dns?domain=github.com\"\n</code></pre> <pre><code># Check multiple domains at once\nfor domain in api.example.com www.example.com mail.example.com; do\n  echo \"=== $domain ===\"\n  curl -s \"http://localhost:8001/v1/intel/dns?domain=$domain\" | jq '.data'\ndone\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"domain\": \"google.com\",\n    \"records\": {\n      \"hostname\": \"google.com\",\n      \"aliases\": []\n    },\n    \"ip_addresses\": [\n      \"142.250.139.113\",\n      \"142.250.139.138\",\n      \"142.250.139.102\"\n    ],\n    \"success\": true\n  },\n  \"meta\": {\n    \"request_id\": \"cab415ba-1185-4bc5-ae4a-77a9121dbb63\",\n    \"timestamp\": \"2026-01-31T03:12:56.054325Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 1\n  }\n}\n</code></pre> <p>Response Fields:</p> Field Type Description <code>domain</code> string Queried domain <code>records.hostname</code> string Canonical hostname <code>records.aliases</code> array DNS aliases (CNAME records) <code>ip_addresses</code> array Resolved IPv4 addresses <code>success</code> boolean Whether DNS lookup succeeded <code>error</code> string Error message (only if <code>success=false</code>)","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelwhois","title":"GET /v1/intel/whois","text":"<p>Get WHOIS information for a domain, parsed into structured JSON.</p> <p>Best For</p> <p>Domain ownership research, expiration monitoring, and brand protection.</p> <p>Parameters:</p> Name Type Description <code>domain</code> string Domain to lookup (required) curlPython <pre><code>curl \"http://localhost:8001/v1/intel/whois?domain=google.com\"\n</code></pre> <pre><code>import requests\nfrom datetime import datetime, timedelta\n\ndef check_domain_expiration(domain):\n    response = requests.get('http://localhost:8001/v1/intel/whois', params={'domain': domain})\n    data = response.json()['data']\n\n    if not data['success']:\n        return {'domain': domain, 'error': data.get('error')}\n\n    expiration = data['parsed'].get('expiration_date')\n    if expiration:\n        exp_date = datetime.fromisoformat(expiration.replace('Z', '+00:00'))\n        days_until = (exp_date - datetime.now(exp_date.tzinfo)).days\n        return {\n            'domain': domain,\n            'expires': expiration,\n            'days_until': days_until,\n            'needs_renewal': days_until &lt; 30\n        }\n    return {'domain': domain, 'expiration': 'unknown'}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"domain\": \"example.com\",\n    \"raw\": \"   Domain Name: EXAMPLE.COM\\r\\n   ...\",\n    \"parsed\": {\n      \"registrar\": \"RESERVED-Internet Assigned Numbers Authority\",\n      \"creation_date\": \"1995-08-14T04:00:00Z\",\n      \"expiration_date\": \"2026-08-13T04:00:00Z\",\n      \"updated_date\": \"2026-01-16T18:26:50Z\",\n      \"name_servers\": [\n        \"elliott.ns.cloudflare.com\",\n        \"hera.ns.cloudflare.com\"\n      ],\n      \"status\": [\n        \"clientDeleteProhibited https://icann.org/epp#clientDeleteProhibited\",\n        \"clientTransferProhibited https://icann.org/epp#clientTransferProhibited\",\n        \"clientUpdateProhibited https://icann.org/epp#clientUpdateProhibited\"\n      ]\n    },\n    \"success\": true\n  },\n  \"meta\": {\n    \"request_id\": \"2e543e91-e49b-4a8b-8982-2a8a90d9bece\",\n    \"timestamp\": \"2026-01-31T03:31:41.367571Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 47\n  }\n}\n</code></pre> <p>Parsed WHOIS Fields:</p> Field Type Description <code>registrar</code> string Domain registrar name <code>creation_date</code> string Domain registration date (ISO 8601) <code>expiration_date</code> string Domain expiration date (ISO 8601) <code>updated_date</code> string Last update date (ISO 8601) <code>name_servers</code> array List of authoritative name servers <code>status</code> array Domain status flags with EPP codes <code>registrant_org</code> string Registrant organization (if available) <code>registrant_country</code> string Registrant country (if available)","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1inteldomain","title":"GET /v1/intel/domain","text":"<p>Get comprehensive domain intelligence combining DNS, HTTP headers, and indexing data in a single call.</p> <p>Best For</p> <p>Full domain reconnaissance, competitive technical analysis, and security assessment.</p> <p>Parameters:</p> Name Type Description <code>domain</code> string Domain to analyze (required) curlPythonjq <pre><code>curl \"http://localhost:8001/v1/intel/domain?domain=example.com\"\n</code></pre> <pre><code>import requests\n\ndef analyze_competitor(domain):\n    response = requests.get('http://localhost:8001/v1/intel/domain', params={'domain': domain})\n    data = response.json()['data']\n\n    return {\n        'domain': domain,\n        'ips': data['ip_addresses'],\n        'server': data.get('http', {}).get('server'),\n        'powered_by': data.get('http', {}).get('x_powered_by'),\n        'uses_cdn': len(data.get('dns', {}).get('aliases', [])) &gt; 0,\n        'indexed_pages': data.get('indexed_pages'),\n        'final_url': data.get('final_url')\n    }\n</code></pre> <pre><code># Quick security posture check\ncurl \"http://localhost:8001/v1/intel/domain?domain=target.com\" | jq '{\n  ips: .data.ip_addresses,\n  status: .data.http.status_code,\n  server: .data.http.server,\n  redirects_to: .data.final_url\n}'\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"domain\": \"github.com\",\n    \"dns\": {\n      \"hostname\": \"github.com\",\n      \"aliases\": []\n    },\n    \"ip_addresses\": [\"140.82.114.3\"],\n    \"http\": {\n      \"status_code\": 200,\n      \"server\": \"github.com\",\n      \"content_type\": \"text/html; charset=utf-8\",\n      \"x_powered_by\": null\n    },\n    \"final_url\": \"https://github.com\",\n    \"indexed_pages\": null\n  },\n  \"meta\": {\n    \"request_id\": \"bd0c7df8-ad99-41f8-919c-96814ca670a0\",\n    \"timestamp\": \"2026-01-31T03:13:24.874816Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 10105\n  }\n}\n</code></pre> <p>Response Fields:</p> Field Type Description <code>domain</code> string Queried domain <code>dns.hostname</code> string Canonical hostname <code>dns.aliases</code> array DNS aliases (CNAMEs) <code>dns.error</code> string DNS lookup error (if failed) <code>ip_addresses</code> array Resolved IP addresses <code>http.status_code</code> integer HTTP response status <code>http.server</code> string Server header value <code>http.content_type</code> string Content-Type header <code>http.x_powered_by</code> string X-Powered-By header (if present) <code>http.error</code> string HTTP request error (if failed) <code>final_url</code> string Final URL after redirects <code>indexed_pages</code> integer Approximate indexed pages (from search)","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelsubdomains","title":"GET /v1/intel/subdomains","text":"<p>Enumerate subdomains using search engines (passive enumeration -- no direct scanning of target).</p> <p>Best For</p> <p>Attack surface discovery, finding forgotten/orphaned subdomains, and bug bounty reconnaissance.</p> <p>Parameters:</p> Name Type Default Description <code>domain</code> string - Domain to enumerate (required) <code>limit</code> integer 50 Maximum results (1-200) <code>cursor</code> string - Pagination cursor curlPython <pre><code>curl \"http://localhost:8001/v1/intel/subdomains?domain=google.com&amp;limit=10\"\n</code></pre> <pre><code>import requests\n\ndef enumerate_subdomains(domain):\n    response = requests.get('http://localhost:8001/v1/intel/subdomains', params={\n        'domain': domain,\n        'limit': 200\n    })\n    data = response.json()['data']\n\n    categories = {\n        'api': [], 'dev': [], 'admin': [], 'mail': [], 'other': []\n    }\n\n    for sub in data['subdomains']:\n        if any(x in sub for x in ['api', 'rest', 'graphql']):\n            categories['api'].append(sub)\n        elif any(x in sub for x in ['dev', 'staging', 'test', 'beta']):\n            categories['dev'].append(sub)\n        elif any(x in sub for x in ['admin', 'portal', 'dashboard']):\n            categories['admin'].append(sub)\n        elif any(x in sub for x in ['mail', 'smtp', 'imap']):\n            categories['mail'].append(sub)\n        else:\n            categories['other'].append(sub)\n\n    return {'domain': domain, 'total': data['total'], 'categories': categories}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"domain\": \"google.com\",\n    \"subdomains\": [\n      \"accounts.google.com\",\n      \"mail.google.com\",\n      \"drive.google.com\",\n      \"docs.google.com\",\n      \"maps.google.com\"\n    ],\n    \"total\": 25\n  },\n  \"meta\": {\n    \"request_id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\",\n    \"timestamp\": \"2026-01-31T03:00:00.000000Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 3000\n  }\n}\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/intelligence/#social-entity-intelligence","title":"Social &amp; Entity Intelligence","text":"","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelsocial","title":"GET /v1/intel/social","text":"<p>Search for a username across social media platforms.</p> <p>Best For</p> <p>OSINT investigations, background research, brand impersonation detection, and identity correlation.</p> <p>Parameters:</p> Name Type Description <code>username</code> string Username to search (required) <code>platforms</code> string Comma-separated: twitter, linkedin, github, instagram, facebook, reddit, youtube <code>limit</code> integer Maximum results (default: 20) <code>cursor</code> string Pagination cursor <code>fields</code> string Fields to return curlPython <pre><code>curl \"http://localhost:8001/v1/intel/social?username=johndoe&amp;platforms=github,twitter\"\n</code></pre> <pre><code>import requests\n\ndef find_social_profiles(username, platforms=None):\n    params = {'username': username, 'limit': 50}\n    if platforms:\n        params['platforms'] = ','.join(platforms)\n\n    response = requests.get('http://localhost:8001/v1/intel/social', params=params)\n    data = response.json()['data']\n\n    profiles = {}\n    for result in data['results']:\n        domain = result['domain']\n        if domain not in profiles:\n            profiles[domain] = []\n        profiles[domain].append({\n            'title': result['title'],\n            'url': result['url']\n        })\n    return profiles\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"username\": \"johndoe\",\n    \"query\": \"\\\"johndoe\\\" (site:github.com OR site:twitter.com)\",\n    \"results\": [\n      {\n        \"title\": \"johndoe (John Doe) - GitHub\",\n        \"url\": \"https://github.com/johndoe\",\n        \"domain\": \"github.com\",\n        \"snippet\": \"johndoe has 50 repositories...\",\n        \"engines\": [\"google\", \"brave\"]\n      }\n    ],\n    \"total\": 5\n  },\n  \"meta\": {\n    \"request_id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\",\n    \"timestamp\": \"2026-01-31T03:00:00.000000Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 2500\n  }\n}\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelentities","title":"GET /v1/intel/entities","text":"<p>Extract entity relationship graph from search results using NLP.</p> <p>Best For</p> <p>Discovering connections between people, organizations, and places. Ready for visualization with D3.js, vis.js, or Cytoscape.</p> <p>Parameters:</p> Name Type Required Default Description <code>q</code> string Yes - Search query for entity extraction <code>limit</code> integer No 30 Number of search results to analyze (5-100) <code>max_entities</code> integer No 50 Maximum entities in graph (10-100) curlPythonJavaScript (D3.js) <pre><code>curl \"http://localhost:8001/v1/intel/entities?q=Tesla+company&amp;limit=20\"\n</code></pre> <pre><code>import requests\nimport networkx as nx\n\ndef build_knowledge_graph(query):\n    response = requests.get('http://localhost:8001/v1/intel/entities', params={\n        'q': query,\n        'limit': 50,\n        'max_entities': 50\n    })\n    graph_data = response.json()['data']['graph']\n\n    G = nx.Graph()\n    for node in graph_data['nodes']:\n        G.add_node(node['id'], label=node['label'],\n                   count=node['count'], description=node['description'])\n    for edge in graph_data['edges']:\n        G.add_edge(edge['source'], edge['target'], weight=edge['weight'])\n    return G\n</code></pre> <pre><code>fetch('http://localhost:8001/v1/intel/entities?q=cryptocurrency+exchange+regulation')\n  .then(r =&gt; r.json())\n  .then(data =&gt; {\n    const graph = data.data.graph;\n    const simulation = d3.forceSimulation(graph.nodes)\n      .force('link', d3.forceLink(graph.edges).id(d =&gt; d.id))\n      .force('charge', d3.forceManyBody().strength(-100))\n      .force('center', d3.forceCenter(width/2, height/2));\n  });\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"query\": \"Tesla company\",\n    \"graph\": {\n      \"nodes\": [\n        {\"id\": \"tesla\", \"label\": \"ORG\", \"count\": 15, \"description\": \"Organization\"},\n        {\"id\": \"elon musk\", \"label\": \"PERSON\", \"count\": 12, \"description\": \"Person\"},\n        {\"id\": \"austin\", \"label\": \"GPE\", \"count\": 5, \"description\": \"Location\"}\n      ],\n      \"edges\": [\n        {\"source\": \"tesla\", \"target\": \"elon musk\", \"weight\": 10},\n        {\"source\": \"tesla\", \"target\": \"austin\", \"weight\": 4}\n      ],\n      \"total_entities\": 31,\n      \"total_relationships\": 15,\n      \"available\": true\n    },\n    \"sources_analyzed\": 20,\n    \"results_url\": \"/v1/search?q=Tesla+company\"\n  }\n}\n</code></pre> <p>Graph Structure</p> <ul> <li>nodes: Entity nodes with id, label (NER type), occurrence count, and description</li> <li>edges: Relationships between entities based on co-occurrence (weight &gt;= 2)</li> <li>Use with D3.js, vis.js, or Cytoscape for visualization</li> </ul>","tags":["OSINT","Security"]},{"location":"api/intelligence/#technology-detection","title":"Technology Detection","text":"","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1inteltech","title":"GET /v1/intel/tech","text":"<p>Detect technologies used by a website. No external APIs required -- all detection is done locally with 50+ technology signatures.</p> <p>Best For</p> <p>Fingerprinting targets for vulnerability assessment, competitive intelligence, and lead qualification.</p> <p>Parameters:</p> Name Type Description <code>url</code> string URL to analyze (required) curlPython <pre><code>curl \"http://localhost:8001/v1/intel/tech?url=https://wordpress.org\"\n</code></pre> <pre><code>import requests\n\ndef analyze_tech_stack(url):\n    response = requests.get('http://localhost:8001/v1/intel/tech', params={'url': url})\n    data = response.json()['data']\n\n    by_category = {}\n    for tech in data['technologies']:\n        cat = tech['category']\n        if cat not in by_category:\n            by_category[cat] = []\n        by_category[cat].append({\n            'name': tech['name'],\n            'confidence': tech['confidence'],\n            'version': tech.get('version')\n        })\n    return {'url': data['url'], 'stack': by_category, 'implied': data['implied']}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"url\": \"https://wordpress.org\",\n    \"technologies\": [\n      {\"name\": \"WordPress\", \"confidence\": 100, \"version\": \"6.4\", \"category\": \"CMS\", \"website\": \"https://wordpress.org\"},\n      {\"name\": \"nginx\", \"confidence\": 100, \"version\": \"1.24\", \"category\": \"Web Server\", \"website\": \"https://nginx.org\"},\n      {\"name\": \"PHP\", \"confidence\": 80, \"version\": null, \"category\": \"Programming Language\", \"website\": \"https://www.php.net\"},\n      {\"name\": \"jQuery\", \"confidence\": 60, \"version\": \"3.7.1\", \"category\": \"JavaScript Library\", \"website\": \"https://jquery.com\"}\n    ],\n    \"implied\": [\"MySQL\"],\n    \"detection_methods\": [\"headers\", \"meta_tags\", \"scripts\", \"url_patterns\"],\n    \"total_detected\": 4\n  }\n}\n</code></pre> <p>Supported Technology Categories:</p> Category Examples CMS WordPress, Drupal, Joomla, Ghost E-commerce Shopify, Magento, WooCommerce JavaScript Framework React, Vue.js, Angular, Next.js, Nuxt.js CSS Framework Bootstrap, Tailwind CSS Backend Framework Laravel, Django, Rails, Express, FastAPI Web Server nginx, Apache, IIS, LiteSpeed CDN Cloudflare, CloudFront, Fastly, Akamai Analytics Google Analytics, Google Tag Manager, Hotjar Hosting Vercel, Netlify, Heroku, AWS","tags":["OSINT","Security"]},{"location":"api/intelligence/#security-analysis","title":"Security Analysis","text":"","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelsecurity-headers","title":"GET /v1/intel/security-headers","text":"<p>Analyze HTTP security headers and get a security score (0-100) with a letter grade (A+ to F).</p> <p>Best For</p> <p>Auditing website security configuration, validating security header deployment, and compliance verification.</p> <p>Parameters:</p> Name Type Description <code>url</code> string URL to analyze (required) curlPython <pre><code>curl \"http://localhost:8001/v1/intel/security-headers?url=https://github.com\"\n</code></pre> <pre><code>import requests\n\ndef audit_security_headers(urls):\n    results = []\n    for url in urls:\n        response = requests.get('http://localhost:8001/v1/intel/security-headers', params={'url': url})\n        data = response.json()['data']\n        results.append({\n            'url': data['url'],\n            'score': data['score'],\n            'grade': data['grade'],\n            'missing': len(data['missing_headers']),\n            'issues': sum(len(h['issues']) for h in data['headers'].values() if h['present'])\n        })\n    return sorted(results, key=lambda x: x['score'], reverse=True)\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"url\": \"https://github.com\",\n    \"score\": 63,\n    \"grade\": \"D\",\n    \"headers\": {\n      \"content-security-policy\": {\n        \"present\": true,\n        \"value\": \"default-src 'none'; base-uri 'self'...\",\n        \"score\": 17,\n        \"max_score\": 20,\n        \"issues\": [\"Contains 'unsafe-inline' which weakens XSS protection\"],\n        \"recommendations\": []\n      },\n      \"strict-transport-security\": {\n        \"present\": true,\n        \"value\": \"max-age=31536000; includeSubdomains; preload\",\n        \"score\": 15,\n        \"max_score\": 15,\n        \"issues\": [],\n        \"recommendations\": []\n      },\n      \"x-frame-options\": {\n        \"present\": true,\n        \"value\": \"deny\",\n        \"score\": 10,\n        \"max_score\": 10,\n        \"issues\": [],\n        \"recommendations\": []\n      }\n    },\n    \"missing_headers\": [\"permissions-policy\", \"cross-origin-opener-policy\"],\n    \"recommendations\": [\n      \"Add Permissions-Policy header to restrict browser features\",\n      \"Add Cross-Origin-Opener-Policy for cross-origin isolation\"\n    ]\n  }\n}\n</code></pre> <p>Headers Analyzed:</p> Header Max Score Description Content-Security-Policy 20 XSS and injection protection Strict-Transport-Security 15 HTTPS enforcement X-Frame-Options 10 Clickjacking protection X-Content-Type-Options 10 MIME sniffing prevention X-XSS-Protection 5 Legacy XSS filter Referrer-Policy 10 Referrer leakage control Permissions-Policy 10 Browser feature restrictions Cross-Origin-Opener-Policy 5 Browsing context isolation Cross-Origin-Embedder-Policy 5 Cross-origin embedding control Cross-Origin-Resource-Policy 5 Resource sharing control Cache-Control 5 Sensitive content caching <p>Grading Scale:</p> Grade Score Range A+ 95-100 A 90-94 B 80-89 C 70-79 D 60-69 F Below 60","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1inteltls","title":"GET /v1/intel/tls","text":"<p>Get TLS/SSL certificate and connection information for a domain.</p> <p>Best For</p> <p>Auditing TLS configuration, monitoring certificate expiration, and verifying encryption standards.</p> <p>Parameters:</p> Name Type Default Description <code>domain</code> string - Domain to check (required) <code>port</code> integer 443 Port number (1-65535) curlPython <pre><code>curl \"http://localhost:8001/v1/intel/tls?domain=github.com\"\n</code></pre> <pre><code>import requests\n\ndef check_cert_expiration(domains):\n    alerts = []\n    for domain in domains:\n        response = requests.get('http://localhost:8001/v1/intel/tls', params={'domain': domain})\n        data = response.json()['data']\n        cert = data['certificate']\n\n        if cert['is_expired']:\n            alerts.append({'domain': domain, 'status': 'EXPIRED', 'days': cert['days_until_expiry']})\n        elif cert['days_until_expiry'] &lt; 30:\n            alerts.append({'domain': domain, 'status': 'CRITICAL', 'days': cert['days_until_expiry']})\n        elif cert['days_until_expiry'] &lt; 90:\n            alerts.append({'domain': domain, 'status': 'WARNING', 'days': cert['days_until_expiry']})\n    return alerts\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"domain\": \"github.com\",\n    \"port\": 443,\n    \"tls_version\": \"TLSv1.3\",\n    \"cipher_suite\": \"TLS_AES_128_GCM_SHA256\",\n    \"certificate\": {\n      \"subject\": \"CN=github.com\",\n      \"issuer\": \"CN=DigiCert TLS RSA SHA256 2020 CA1,O=DigiCert Inc,C=US\",\n      \"serial_number\": \"0x0123456789abcdef\",\n      \"not_before\": \"2025-03-15T00:00:00+00:00\",\n      \"not_after\": \"2026-03-15T23:59:59+00:00\",\n      \"is_expired\": false,\n      \"days_until_expiry\": 180,\n      \"san\": [\"github.com\", \"www.github.com\"],\n      \"key_size\": 256,\n      \"key_algorithm\": \"ECDSA\",\n      \"signature_algorithm\": \"SHA256-RSA\"\n    },\n    \"certificate_chain\": [\n      {\"subject\": \"CN=github.com\", \"issuer\": \"CN=DigiCert TLS RSA SHA256 2020 CA1...\"}\n    ],\n    \"issues\": []\n  }\n}\n</code></pre> <p>Security Issues Detected</p> <ul> <li>Expired certificates</li> <li>Certificates expiring within 30/90 days</li> <li>Deprecated TLS versions (TLSv1.0, TLSv1.1)</li> <li>Weak cipher suites (RC4, DES, 3DES, NULL, EXPORT)</li> <li>Small key sizes (RSA &lt; 2048, ECDSA &lt; 256)</li> <li>Weak signature algorithms (SHA1)</li> </ul>","tags":["OSINT","Security"]},{"location":"api/intelligence/#threat-intelligence","title":"Threat Intelligence","text":"","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelthreat","title":"GET /v1/intel/threat","text":"<p>Aggregate threat intelligence from multiple external APIs to check if a URL or domain is malicious.</p> <p>Best For</p> <p>Investigating suspicious URLs, triaging potential threats, checking links in suspicious emails, and assessing IOCs.</p> <p>External APIs Used (optional, enhances results)</p> Service Free Tier VirusTotal 500 requests/day Google Safe Browsing 10,000 requests/day AbuseIPDB 1,000 checks/day Shodan 100 credits/month Hybrid Analysis 50 submissions/day Configuration (.env)<pre><code>VIRUSTOTAL_API_KEY=your_key_here\nGOOGLE_SAFE_BROWSING_KEY=your_key_here\nABUSEIPDB_API_KEY=your_key_here\nSHODAN_API_KEY=your_key_here\nHYBRID_ANALYSIS_API_KEY=your_key_here\n</code></pre> <p>Parameters:</p> Name Type Description <code>url</code> string URL to analyze (required) curlPython <pre><code>curl \"http://localhost:8001/v1/intel/threat?url=https://example.com\"\n</code></pre> <pre><code>import requests\nimport re\n\ndef scan_email_links(email_body):\n    urls = re.findall(r'https?://[^\\s&lt;&gt;\"{}|\\\\^`\\[\\]]+', email_body)\n    results = []\n    for url in urls:\n        response = requests.get('http://localhost:8001/v1/intel/threat', params={'url': url})\n        data = response.json()['data']\n        results.append({\n            'url': url,\n            'verdict': data['verdict'],\n            'confidence': data['confidence'],\n            'safe': data['verdict'] == 'safe'\n        })\n    return results\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"target\": \"https://example.com\",\n    \"target_type\": \"url\",\n    \"verdict\": \"safe\",\n    \"confidence\": 85,\n    \"sources\": {\n      \"virustotal\": {\n        \"available\": true,\n        \"malicious\": 0,\n        \"suspicious\": 0,\n        \"clean\": 70,\n        \"scan_date\": \"2026-02-01T10:00:00Z\",\n        \"permalink\": \"https://www.virustotal.com/gui/url/...\"\n      },\n      \"google_safe_browsing\": {\n        \"available\": true,\n        \"threats\": []\n      },\n      \"abuseipdb\": {\n        \"available\": true,\n        \"confidence_score\": 0,\n        \"total_reports\": 0,\n        \"is_tor\": false\n      },\n      \"shodan\": {\n        \"available\": true,\n        \"ports\": [80, 443],\n        \"vulns\": [],\n        \"org\": \"Example Inc\"\n      }\n    },\n    \"recommendations\": [\n      \"No threats detected from available sources\",\n      \"Always verify URLs before entering credentials\"\n    ],\n    \"scan_timestamp\": \"2026-02-02T12:00:00+00:00\"\n  }\n}\n</code></pre> <p>Verdict Values:</p> Verdict Description <code>safe</code> No threats detected, high confidence <code>suspicious</code> Some indicators of risk, exercise caution <code>malicious</code> Multiple sources flag as dangerous <code>unknown</code> Insufficient data to determine","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelip","title":"GET /v1/intel/ip","text":"<p>Get IP address reputation, geolocation, and infrastructure details.</p> <p>Best For</p> <p>Investigating suspicious IPs in logs, triaging alerts, fraud prevention, and gathering IOC intelligence.</p> <p>External APIs Used</p> <ul> <li>AbuseIPDB: Abuse reports and confidence score</li> <li>Shodan: Open ports, services, vulnerabilities</li> </ul> <p>Parameters:</p> Name Type Description <code>ip</code> string IP address to check (required, IPv4 or IPv6) curlPython <pre><code>curl \"http://localhost:8001/v1/intel/ip?ip=8.8.8.8\"\n</code></pre> <pre><code>import requests\n\ndef analyze_suspicious_ips(ip_list):\n    results = []\n    for ip in ip_list:\n        response = requests.get('http://localhost:8001/v1/intel/ip', params={'ip': ip})\n        data = response.json()['data']\n\n        risk_score = 0\n        risk_factors = []\n\n        if data.get('abuse') and data['abuse'].get('confidence_score', 0) &gt; 50:\n            risk_score += 30\n            risk_factors.append(f\"High abuse score: {data['abuse']['confidence_score']}%\")\n        if data.get('abuse', {}).get('is_tor'):\n            risk_score += 20\n            risk_factors.append(\"Tor exit node\")\n        if data.get('shodan', {}).get('vulns'):\n            risk_score += 25\n            risk_factors.append(f\"Known vulnerabilities: {len(data['shodan']['vulns'])}\")\n\n        results.append({\n            'ip': ip, 'risk_score': risk_score,\n            'risk_factors': risk_factors,\n            'org': data.get('org'),\n            'country': data.get('geolocation', {}).get('country_code')\n        })\n    return sorted(results, key=lambda x: x['risk_score'], reverse=True)\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"ip\": \"8.8.8.8\",\n    \"ip_version\": 4,\n    \"is_private\": false,\n    \"geolocation\": {\n      \"country\": \"United States\",\n      \"country_code\": \"US\",\n      \"region\": null,\n      \"city\": null\n    },\n    \"reputation\": {\n      \"abuse_score\": 0,\n      \"total_reports\": 22,\n      \"is_tor_exit\": false,\n      \"has_vulnerabilities\": false,\n      \"open_ports\": 2\n    },\n    \"abuse\": {\n      \"available\": true,\n      \"confidence_score\": 0,\n      \"total_reports\": 22,\n      \"last_reported\": \"2026-01-15T10:30:00Z\",\n      \"is_tor\": false,\n      \"isp\": \"Google LLC\",\n      \"domain\": \"google.com\",\n      \"country_code\": \"US\"\n    },\n    \"shodan\": {\n      \"available\": true,\n      \"ports\": [53, 443],\n      \"services\": [\"DNS\", \"HTTPS\"],\n      \"vulns\": [],\n      \"org\": \"Google LLC\",\n      \"asn\": \"AS15169\",\n      \"isp\": \"Google LLC\"\n    },\n    \"reverse_dns\": \"dns.google\",\n    \"asn\": \"AS15169\",\n    \"org\": \"Google LLC\",\n    \"isp\": \"Google LLC\"\n  }\n}\n</code></pre> <p>IP Types Handled</p> <ul> <li>Public IPv4: Full analysis with external APIs</li> <li>Public IPv6: Full analysis with external APIs</li> <li>Private IPs: Detected and flagged (192.168.x.x, 10.x.x.x, etc.)</li> <li>Reserved IPs: Properly identified</li> </ul>","tags":["OSINT","Security"]},{"location":"api/intelligence/#tls-fingerprinting","title":"TLS Fingerprinting","text":"<p>TLS fingerprinting endpoints identify servers by their TLS handshake behavior. Useful for tracking C2 infrastructure, detecting malware, and correlating server deployments.</p>","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelfingerprint","title":"GET /v1/intel/fingerprint","text":"<p>Get complete TLS fingerprint with JARM and JA3S.</p> <p>Parameters:</p> Name Type Description <code>domain</code> string Domain to fingerprint (required) <code>port</code> integer Port number (default: 443) <code>timeout</code> integer Timeout in seconds (default: 20) <code>include_lookup</code> boolean Include database lookup (default: true) <pre><code>curl \"http://localhost:8001/v1/intel/fingerprint?domain=example.com\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"domain\": \"example.com\",\n    \"port\": 443,\n    \"jarm\": \"29d29d15d29d29d00029d29d29d29de1a3c0dfa92ed20d5b4c29faba9dcc89\",\n    \"ja3s\": \"771,49195,65281-0-11-35-23\",\n    \"ja3s_hash\": \"eb1d94daa7e0344597e756a1fb6e7054\",\n    \"tls_version\": \"TLSv1.3\",\n    \"cipher_suite\": \"TLS_AES_256_GCM_SHA384\",\n    \"identification\": {\n      \"name\": \"Cloudflare\",\n      \"category\": \"cdn\"\n    }\n  }\n}\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelfingerprintjarm","title":"GET /v1/intel/fingerprint/jarm","text":"<p>Get JARM fingerprint for a TLS server. JARM sends 10 specially crafted TLS Client Hello packets and creates a 62-character fingerprint.</p> <p>Best For</p> <p>Identifying server software, detecting malware C2 (Cobalt Strike, Metasploit).</p> <p>Parameters:</p> Name Type Description <code>domain</code> string Domain to fingerprint (required) <code>port</code> integer Port number (default: 443) <code>timeout</code> integer Timeout in seconds (default: 20) <pre><code>curl \"http://localhost:8001/v1/intel/fingerprint/jarm?domain=example.com\"\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelfingerprintja3s","title":"GET /v1/intel/fingerprint/ja3s","text":"<p>Get JA3S fingerprint for a TLS server (server response fingerprint).</p> <p>Parameters:</p> Name Type Description <code>domain</code> string Domain to fingerprint (required) <code>port</code> integer Port number (default: 443) <code>timeout</code> integer Timeout in seconds (default: 10) <pre><code>curl \"http://localhost:8001/v1/intel/fingerprint/ja3s?domain=example.com\"\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelfingerprintja4","title":"GET /v1/intel/fingerprint/ja4","text":"<p>Get JA4 fingerprint (next-generation client fingerprint). JA4 improves on JA3 with better resistance to randomization.</p> <p>Parameters:</p> Name Type Description <code>domain</code> string Domain to fingerprint (required) <code>port</code> integer Port number (default: 443) <code>timeout</code> integer Timeout in seconds (default: 10) <pre><code>curl \"http://localhost:8001/v1/intel/fingerprint/ja4?domain=example.com\"\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelfingerprintja4s","title":"GET /v1/intel/fingerprint/ja4s","text":"<p>Get JA4S server fingerprint.</p> <p>Parameters:</p> Name Type Description <code>domain</code> string Domain to fingerprint (required) <code>port</code> integer Port number (default: 443) <code>timeout</code> integer Timeout in seconds (default: 10)","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelfingerprintlookup","title":"GET /v1/intel/fingerprint/lookup","text":"<p>Look up a JARM fingerprint in the known fingerprint database.</p> <p>Parameters:</p> Name Type Description <code>jarm</code> string 62-character JARM fingerprint (required) <pre><code>curl \"http://localhost:8001/v1/intel/fingerprint/lookup?jarm=29d29d15d29d29d00029d29d29d29de1a3c0dfa92ed20d5b4c29faba9dcc89\"\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/intelligence/#censys-integration","title":"Censys Integration","text":"<p>API Keys Required</p> <p>All Censys endpoints require <code>CENSYS_API_ID</code> and <code>CENSYS_API_SECRET</code> environment variables.</p> Configuration (.env)<pre><code>CENSYS_API_ID=your_api_id_here\nCENSYS_API_SECRET=your_api_secret_here\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelcensyshosts","title":"GET /v1/intel/censys/hosts","text":"<p>Search for hosts using Censys Search Language.</p> <p>Parameters:</p> Name Type Description <code>query</code> string Censys Search Language query (required) <code>per_page</code> integer Results per page (default: 25) <code>pages</code> integer Number of pages (default: 1) <code>virtual_hosts</code> string EXCLUDE, INCLUDE, or ONLY (default: EXCLUDE) <pre><code>curl \"http://localhost:8001/v1/intel/censys/hosts?query=services.http.response.html_title:Login\"\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelcensyshostsip","title":"GET /v1/intel/censys/hosts/{ip}","text":"<p>Get detailed host information from Censys for a specific IP.</p> <p>Parameters:</p> Name Type Description <code>ip</code> string IP address (path parameter) <code>at_time</code> string RFC3339 timestamp for historical data","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelcensyscerts","title":"GET /v1/intel/censys/certs","text":"<p>Search SSL/TLS certificates in the Censys database.</p> <p>Parameters:</p> Name Type Description <code>query</code> string Certificate search query (required) <code>per_page</code> integer Results per page (default: 25) <pre><code>curl \"http://localhost:8001/v1/intel/censys/certs?query=parsed.subject.common_name:example.com\"\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelcensyscertsfingerprinthosts","title":"GET /v1/intel/censys/certs/{fingerprint}/hosts","text":"<p>Find hosts presenting a specific certificate.</p> <p>Parameters:</p> Name Type Description <code>fingerprint</code> string SHA-256 fingerprint (path parameter) <code>per_page</code> integer Max results (default: 100)","tags":["OSINT","Security"]},{"location":"api/intelligence/#vulners-integration","title":"Vulners Integration","text":"<p>API Key Required</p> <p>All Vulners endpoints require a <code>VULNERS_API_KEY</code> environment variable.</p> Configuration (.env)<pre><code>VULNERS_API_KEY=your_api_key_here\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelvulnerssearch","title":"GET /v1/intel/vulners/search","text":"<p>Search CVEs using the Vulners database with Lucene-style queries.</p> <p>Parameters:</p> Name Type Description <code>query</code> string Lucene-style search query (required) <code>limit</code> integer Maximum results (default: 20) <code>skip</code> integer Pagination offset (default: 0) <pre><code>curl \"http://localhost:8001/v1/intel/vulners/search?query=apache\"\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelvulnersexploits","title":"GET /v1/intel/vulners/exploits","text":"<p>Search for exploits in the Vulners database.</p> <p>Parameters:</p> Name Type Description <code>query</code> string Search query (required) <code>limit</code> integer Maximum results (default: 20)","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelvulnerscvecve_id","title":"GET /v1/intel/vulners/cve/{cve_id}","text":"<p>Get detailed CVE information from Vulners.</p> <p>Parameters:</p> Name Type Description <code>cve_id</code> string CVE ID (path parameter, e.g., CVE-2021-44228)","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelvulnerscvecve_idexploits","title":"GET /v1/intel/vulners/cve/{cve_id}/exploits","text":"<p>Find exploits for a specific CVE.</p> <p>Parameters:</p> Name Type Description <code>cve_id</code> string CVE ID (path parameter) <code>limit</code> integer Maximum exploits (default: 50)","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelvulnersproduct","title":"GET /v1/intel/vulners/product","text":"<p>Search vulnerabilities by product and version.</p> <p>Parameters:</p> Name Type Description <code>product</code> string Product name (required) <code>version</code> string Product version <code>severity</code> string CRITICAL, HIGH, MEDIUM, LOW <code>limit</code> integer Maximum results (default: 20) <pre><code>curl \"http://localhost:8001/v1/intel/vulners/product?product=apache&amp;version=2.4.49\"\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/intelligence/#ssl-labs-integration","title":"SSL Labs Integration","text":"<p>SSL Labs endpoints provide deep SSL/TLS analysis. No API key is required but requests are rate limited.</p>","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelssllabsinfo","title":"GET /v1/intel/ssllabs/info","text":"<p>Get SSL Labs API status and rate limits.</p> <pre><code>curl \"http://localhost:8001/v1/intel/ssllabs/info\"\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelssllabsanalyze","title":"GET /v1/intel/ssllabs/analyze","text":"<p>Start or retrieve an SSL/TLS analysis (non-blocking).</p> <p>Parameters:</p> Name Type Description <code>host</code> string Domain to analyze (required) <code>start_new</code> boolean Force new scan (default: false) <code>from_cache</code> boolean Use cached results (default: true) <code>max_age</code> integer Max cache age in hours (default: 24) <code>publish</code> boolean Publish results on SSL Labs (default: false) <pre><code>curl \"http://localhost:8001/v1/intel/ssllabs/analyze?host=example.com\"\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/intelligence/#post-v1intelssllabsanalyzefull","title":"POST /v1/intel/ssllabs/analyze/full","text":"<p>Start analysis and wait for completion (blocking call).</p> <p>Parameters:</p> Name Type Description <code>host</code> string Domain to analyze (required) <code>max_wait</code> integer Max wait time in seconds (default: 300) <code>poll_interval</code> integer Poll interval in seconds (default: 10)","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelssllabsgrade","title":"GET /v1/intel/ssllabs/grade","text":"<p>Get SSL grade summary from cache.</p> <p>Parameters:</p> Name Type Description <code>host</code> string Domain to check (required) <code>max_age</code> integer Max cache age in hours (default: 24)","tags":["OSINT","Security"]},{"location":"api/intelligence/#abusech-integration","title":"abuse.ch Integration","text":"<p>The abuse.ch endpoints provide access to URLhaus, Malware Bazaar, and ThreatFox databases. No API keys are required -- these are free community services.</p>","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelurlhausurl","title":"GET /v1/intel/urlhaus/url","text":"<p>Check a URL against the abuse.ch URLhaus database for known malicious URLs.</p> <p>Parameters:</p> Name Type Description <code>url</code> string URL to check (required) <pre><code>curl \"http://localhost:8001/v1/intel/urlhaus/url?url=http://example.com/malware\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"query_status\": \"ok\",\n    \"url\": \"http://example.com/malware\",\n    \"url_status\": \"online\",\n    \"threat\": \"malware_download\",\n    \"tags\": [\"elf\", \"mirai\"],\n    \"date_added\": \"2026-01-15\",\n    \"payloads\": []\n  }\n}\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelurlhaushost","title":"GET /v1/intel/urlhaus/host","text":"<p>Check a host against the URLhaus database.</p> <p>Parameters:</p> Name Type Description <code>host</code> string Hostname or IP to check (required)","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelurlhausrecent","title":"GET /v1/intel/urlhaus/recent","text":"<p>Get recent malicious URLs from the URLhaus feed.</p> <p>Parameters:</p> Name Type Description <code>limit</code> integer Maximum URLs (default: 100)","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelmalwarebazaarhash","title":"GET /v1/intel/malwarebazaar/hash","text":"<p>Look up a file hash in the Malware Bazaar database.</p> <p>Parameters:</p> Name Type Description <code>hash</code> string MD5, SHA1, or SHA256 hash (required) <pre><code>curl \"http://localhost:8001/v1/intel/malwarebazaar/hash?hash=ed01ebfbc9eb5bbea545af4d01bf5f1071661840480439c6e5babe8e080e41aa\"\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelmalwarebazaartag","title":"GET /v1/intel/malwarebazaar/tag","text":"<p>Search Malware Bazaar samples by tag.</p> <p>Parameters:</p> Name Type Description <code>tag</code> string Tag to search (e.g., \"ransomware\") (required) <code>limit</code> integer Maximum results (default: 50)","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelmalwarebazaarsignature","title":"GET /v1/intel/malwarebazaar/signature","text":"<p>Search Malware Bazaar by malware signature/family name.</p> <p>Parameters:</p> Name Type Description <code>signature</code> string Malware family name (required) <code>limit</code> integer Maximum results (default: 50)","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelmalwarebazaarrecent","title":"GET /v1/intel/malwarebazaar/recent","text":"<p>Get recent malware samples from Malware Bazaar.</p> <p>Parameters:</p> Name Type Description <code>limit</code> integer Maximum samples (default: 100)","tags":["OSINT","Security"]},{"location":"api/intelligence/#post-v1intelthreatfoxioc","title":"POST /v1/intel/threatfox/ioc","text":"<p>Look up an Indicator of Compromise (IOC) in the ThreatFox database.</p> <p>Request Body:</p> <pre><code>{\n  \"ioc\": \"http://example.com/malware.exe\"\n}\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelthreatfoxmalware","title":"GET /v1/intel/threatfox/malware","text":"<p>Search ThreatFox by malware family.</p> <p>Parameters:</p> Name Type Description <code>malware</code> string Malware family name (required) <code>limit</code> integer Maximum IOCs (default: 100) <pre><code>curl \"http://localhost:8001/v1/intel/threatfox/malware?malware=Cobalt%20Strike\"\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelthreatfoxrecent","title":"GET /v1/intel/threatfox/recent","text":"<p>Get recent IOCs from ThreatFox.</p> <p>Parameters:</p> Name Type Description <code>days</code> integer Days to look back (default: 1) <code>limit</code> integer Maximum IOCs (default: 100)","tags":["OSINT","Security"]},{"location":"api/intelligence/#greynoise-integration","title":"GreyNoise Integration","text":"<p>GreyNoise classifies internet-wide scan traffic. Uses the Community API (free tier, rate limited).</p>","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelgreynoiseip","title":"GET /v1/intel/greynoise/ip","text":"<p>Look up an IP address in the GreyNoise database.</p> <p>Parameters:</p> Name Type Description <code>ip</code> string IP address to check (required) <pre><code>curl \"http://localhost:8001/v1/intel/greynoise/ip?ip=8.8.8.8\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"ip\": \"8.8.8.8\",\n    \"noise\": false,\n    \"riot\": true,\n    \"classification\": \"benign\",\n    \"name\": \"Google DNS\",\n    \"link\": \"https://viz.greynoise.io/ip/8.8.8.8\"\n  }\n}\n</code></pre> <p>GreyNoise Classifications</p> Classification Description <code>noise</code> Background internet scanning <code>riot</code> Benign services (CDNs, cloud providers) <code>malicious</code> Known malicious activity <code>benign</code> Known safe sources <code>unknown</code> Not enough data","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelgreynoiseriot","title":"GET /v1/intel/greynoise/riot","text":"<p>Check if an IP is in the GreyNoise RIOT database (benign services like CDNs, cloud providers).</p> <p>Parameters:</p> Name Type Description <code>ip</code> string IP address to check (required)","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelgreynoisequick","title":"GET /v1/intel/greynoise/quick","text":"<p>Quick noise check for an IP -- returns only whether the IP is seen scanning the internet.</p> <p>Parameters:</p> Name Type Description <code>ip</code> string IP address to check (required)","tags":["OSINT","Security"]},{"location":"api/intelligence/#alienvault-otx-integration","title":"AlienVault OTX Integration","text":"<p>API Key Required</p> <p>All OTX endpoints require an <code>OTX_API_KEY</code> environment variable.</p> Configuration (.env)<pre><code>OTX_API_KEY=your_api_key_here\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelotxindicatorip","title":"GET /v1/intel/otx/indicator/ip","text":"<p>Look up an IP indicator in AlienVault OTX.</p> <p>Parameters:</p> Name Type Description <code>ip</code> string IP address to look up (required)","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelotxindicatordomain","title":"GET /v1/intel/otx/indicator/domain","text":"<p>Look up a domain indicator in OTX.</p> <p>Parameters:</p> Name Type Description <code>domain</code> string Domain to look up (required)","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelotxindicatorhash","title":"GET /v1/intel/otx/indicator/hash","text":"<p>Look up a file hash indicator in OTX.</p> <p>Parameters:</p> Name Type Description <code>hash</code> string MD5, SHA1, or SHA256 hash (required)","tags":["OSINT","Security"]},{"location":"api/intelligence/#get-v1intelotxpulses","title":"GET /v1/intel/otx/pulses","text":"<p>Search for pulses (threat reports) in OTX.</p> <p>Parameters:</p> Name Type Description <code>query</code> string Search query (required) <code>limit</code> integer Maximum pulses (default: 10)","tags":["OSINT","Security"]},{"location":"api/network/","title":"Network Endpoints","text":"<p>The Network API provides a comprehensive suite for API traffic capture, reverse engineering, documentation generation, and monitoring. Using browser automation and proxy interception, it can discover hidden endpoints, generate client code in multiple languages, export to industry-standard formats, and continuously monitor APIs for breaking changes.</p>","tags":["Network","Security"]},{"location":"api/network/#capabilities-overview","title":"Capabilities Overview","text":"Capability Description Traffic Capture Intercept browser network requests via Playwright API Discovery Smart endpoint detection with authentication inference Export Formats OpenAPI 3.0, Postman v2.1, Insomnia v4, HAR 1.2 Command Generation curl, HTTPie, Python, JavaScript, PowerShell GraphQL Introspection Schema discovery for GraphQL APIs API Monitoring Track changes, detect breaking changes, webhook alerts Proxy Mode Capture traffic from any application (mobile, desktop, CLI) Security Testing Fuzzing, auth bypass testing, request replay","tags":["Network","Security"]},{"location":"api/network/#endpoint-summary","title":"Endpoint Summary","text":"Endpoint Method Description <code>/v1/network/capture</code> POST Capture API traffic from a web page <code>/v1/network/discover</code> POST Smart API endpoint discovery <code>/v1/network/generate/commands</code> POST Generate API commands in multiple formats <code>/v1/network/export/openapi</code> POST Export as OpenAPI 3.0 specification <code>/v1/network/export/postman</code> POST Export as Postman Collection v2.1 <code>/v1/network/export/insomnia</code> POST Export as Insomnia Export v4 <code>/v1/network/export/har</code> POST Export as HAR 1.2 format <code>/v1/network/graphql/introspect</code> POST Introspect a GraphQL API schema <code>/v1/network/detect/auth</code> POST Detect authentication methods <code>/v1/network/infer/schema</code> POST Infer JSON schema from samples <code>/v1/network/monitor/create</code> POST Create an API monitor <code>/v1/network/monitor/list</code> GET List all API monitors <code>/v1/network/monitor/{api_id}</code> GET Get monitor details and snapshots <code>/v1/network/monitor/{api_id}/capture</code> POST Trigger a manual snapshot <code>/v1/network/monitor/{api_id}</code> DELETE Delete an API monitor <code>/v1/network/monitor/compare</code> POST Compare two OpenAPI specs <code>/v1/network/proxy/start</code> POST Start an HTTP proxy server <code>/v1/network/proxy/stop</code> POST Stop active proxy session <code>/v1/network/proxy/status</code> GET Get proxy session status <code>/v1/network/proxy/traffic</code> GET Get captured proxy traffic <code>/v1/network/proxy/instructions</code> GET Get proxy setup instructions <code>/v1/network/fuzz</code> POST Security fuzzing for endpoints <code>/v1/network/test/auth-bypass</code> POST Test for auth bypass vulnerabilities <code>/v1/network/replay</code> POST Replay a captured API request <code>/v1/network/replay/batch</code> POST Replay multiple requests <code>/v1/network/analyze</code> POST Analyze captured endpoint patterns <code>/v1/network/diff</code> POST Compare two API captures <code>/v1/network/mock/generate</code> POST Generate mock server config <code>/v1/network/websocket/capture</code> POST Capture WebSocket messages","tags":["Network","Security"]},{"location":"api/network/#post-v1networkcapture","title":"<code>POST /v1/network/capture</code>","text":"<p>Capture API traffic from a web page using browser automation. Opens the target URL in a headless browser, intercepts all network requests, and returns the discovered API endpoints.</p> <p>Best For</p> <p>Reverse engineering web APIs, discovering hidden endpoints, mapping attack surfaces.</p> <p>Target Audience</p> <ul> <li>Security researchers -- Mapping attack surfaces</li> <li>API developers -- Understanding third-party integrations</li> <li>QA engineers -- Documenting API behavior</li> </ul>","tags":["Network","Security"]},{"location":"api/network/#parameters","title":"Parameters","text":"Name Type Default Description <code>url</code> string -- URL to capture traffic from (required) <code>wait_time</code> float <code>3.0</code> Seconds to wait for network activity <code>scroll</code> boolean <code>false</code> Scroll page to trigger lazy loading <code>click_selectors</code> array -- CSS selectors to click (triggers additional requests) <code>filter_hosts</code> array -- Only capture traffic to these hosts <code>exclude_hosts</code> array -- Exclude traffic to these hosts","tags":["Network","Security"]},{"location":"api/network/#examples","title":"Examples","text":"curlcurl (with filters) <pre><code>curl -X POST http://localhost:8001/v1/network/capture \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\": \"https://jsonplaceholder.typicode.com\", \"wait_time\": 5}'\n</code></pre> <pre><code>curl -X POST http://localhost:8001/v1/network/capture \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"url\": \"https://example.com/app\",\n    \"wait_time\": 5.0,\n    \"scroll\": true,\n    \"click_selectors\": [\".load-more\"],\n    \"filter_hosts\": [\"api.example.com\"]\n  }'\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#response","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"url\": \"https://jsonplaceholder.typicode.com\",\n    \"total_requests\": 3,\n    \"api_endpoints\": [\n      {\n        \"url\": \"https://jsonplaceholder.typicode.com/posts\",\n        \"method\": \"GET\",\n        \"path\": \"/posts\",\n        \"response\": {\"status\": 200, \"content_type\": \"application/json\"}\n      }\n    ],\n    \"unique_domains\": [\"jsonplaceholder.typicode.com\"],\n    \"auth_headers_found\": {}\n  }\n}\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#response-fields","title":"Response Fields","text":"Field Type Description <code>url</code> string The captured URL <code>total_requests</code> integer Total network requests intercepted <code>api_endpoints</code> array Discovered API endpoints with method, path, and response info <code>unique_domains</code> array All unique domains observed in traffic <code>auth_headers_found</code> object Authentication headers detected (e.g., Bearer tokens, API keys)","tags":["Network","Security"]},{"location":"api/network/#post-v1networkdiscover","title":"<code>POST /v1/network/discover</code>","text":"<p>Smart API endpoint discovery with authentication detection. Goes beyond simple traffic capture by analyzing patterns and detecting auth mechanisms.</p>","tags":["Network","Security"]},{"location":"api/network/#parameters_1","title":"Parameters","text":"Name Type Default Description <code>url</code> string -- URL to discover APIs from (required) <code>include_subdomains</code> boolean <code>false</code> Include subdomain traffic <code>detect_auth</code> boolean <code>false</code> Detect authentication methods","tags":["Network","Security"]},{"location":"api/network/#example","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/network/discover \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"url\": \"https://api.example.com\",\n    \"include_subdomains\": true,\n    \"detect_auth\": true\n  }'\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#post-v1networkgeneratecommands","title":"<code>POST /v1/network/generate/commands</code>","text":"<p>Generate copy-paste ready API commands in multiple languages and formats. Takes captured endpoint data and produces ready-to-use commands for testing, documentation, or automation.</p> <p>Best For</p> <p>Quick testing, sharing API examples, creating documentation snippets.</p> <p>Target Audience</p> <ul> <li>Developers -- Testing endpoints quickly</li> <li>Technical writers -- Creating API documentation</li> <li>DevOps engineers -- Automating API calls</li> </ul>","tags":["Network","Security"]},{"location":"api/network/#parameters_2","title":"Parameters","text":"Name Type Default Description <code>endpoints</code> array -- Endpoints to generate commands for (required) <code>formats</code> array -- Output formats: <code>curl</code>, <code>httpie</code>, <code>python</code>, <code>javascript</code>, <code>powershell</code> <code>include_headers</code> boolean <code>true</code> Include headers in output","tags":["Network","Security"]},{"location":"api/network/#examples_1","title":"Examples","text":"curlcurl (all formats) <pre><code>curl -X POST http://localhost:8001/v1/network/generate/commands \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"endpoints\": [\n      {\n        \"url\": \"https://api.github.com/users/octocat\",\n        \"method\": \"GET\"\n      }\n    ],\n    \"formats\": [\"curl\"]\n  }'\n</code></pre> <pre><code>curl -X POST http://localhost:8001/v1/network/generate/commands \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"endpoints\": [\n      {\n        \"url\": \"https://api.example.com/users\",\n        \"method\": \"POST\",\n        \"request\": {\n          \"headers\": {\n            \"Authorization\": \"Bearer token\",\n            \"Content-Type\": \"application/json\"\n          },\n          \"body\": \"{\\\"name\\\": \\\"John\\\"}\"\n        }\n      }\n    ],\n    \"formats\": [\"curl\", \"httpie\", \"python\", \"javascript\", \"powershell\"]\n  }'\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#response_1","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"results\": [\n      {\n        \"endpoint\": \"GET /users/octocat\",\n        \"commands\": {\n          \"curl\": \"curl https://api.github.com/users/octocat\"\n        }\n      }\n    ],\n    \"batch_script\": \"#!/bin/bash\\ncurl https://api.github.com/users/octocat\"\n  }\n}\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#generated-format-examples","title":"Generated Format Examples","text":"<p>Below are examples of the output produced for a <code>POST /users</code> endpoint with authentication:</p> curlHTTPiePythonJavaScriptPowerShell <pre><code>curl \\\n  -X POST \\\n  https://api.example.com/users \\\n  -H 'Authorization: Bearer token' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"name\":\"John\"}'\n</code></pre> <pre><code>http POST https://api.example.com/users \\\n  Authorization:'Bearer token' \\\n  name='John'\n</code></pre> <pre><code>import requests\nresponse = requests.post(\n    \"https://api.example.com/users\",\n    headers={\"Authorization\": \"Bearer token\"},\n    json={\"name\": \"John\"}\n)\n</code></pre> <pre><code>const response = await fetch('https://api.example.com/users', {\n  method: 'POST',\n  headers: {'Authorization': 'Bearer token'},\n  body: JSON.stringify({name: 'John'})\n});\n</code></pre> <pre><code>Invoke-WebRequest -Uri \"https://api.example.com/users\" -Method POST `\n  -Headers @{Authorization = \"Bearer token\"} `\n  -Body '{\"name\":\"John\"}'\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#export-endpoints","title":"Export Endpoints","text":"<p>Export captured API endpoints to industry-standard formats for use in API testing tools, documentation generators, and CI/CD pipelines.</p>","tags":["Network","Security"]},{"location":"api/network/#post-v1networkexportopenapi","title":"<code>POST /v1/network/export/openapi</code>","text":"<p>Generate an OpenAPI 3.0 specification from captured endpoints.</p> <p>Best For</p> <p>API documentation, SDK generation, contract testing.</p>","tags":["Network","Security"]},{"location":"api/network/#parameters_3","title":"Parameters","text":"Name Type Required Description <code>endpoints</code> array Yes Captured endpoints to export <code>title</code> string No API title <code>version</code> string No API version string <code>description</code> string No API description","tags":["Network","Security"]},{"location":"api/network/#example_1","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/network/export/openapi \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"endpoints\": [\n      {\n        \"url\": \"https://api.example.com/users\",\n        \"method\": \"GET\",\n        \"response\": {\"status\": 200, \"content_type\": \"application/json\"}\n      }\n    ],\n    \"title\": \"My API\",\n    \"version\": \"1.0.0\",\n    \"description\": \"API documentation\"\n  }'\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#response_2","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"openapi\": \"3.0.3\",\n    \"info\": {\"title\": \"My API\", \"version\": \"1.0.0\"},\n    \"paths\": {\n      \"/users\": {\n        \"get\": {\"responses\": {\"200\": {\"description\": \"OK\"}}}\n      }\n    }\n  }\n}\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#post-v1networkexportpostman","title":"<code>POST /v1/network/export/postman</code>","text":"<p>Export endpoints as a Postman Collection v2.1 file. The output is directly importable in Postman.</p> <p>Best For</p> <p>API testing in Postman, team collaboration on API collections.</p>","tags":["Network","Security"]},{"location":"api/network/#parameters_4","title":"Parameters","text":"Name Type Required Description <code>endpoints</code> array Yes Captured endpoints to export <code>name</code> string No Collection name","tags":["Network","Security"]},{"location":"api/network/#example_2","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/network/export/postman \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"endpoints\": [\n      {\n        \"url\": \"https://api.example.com/users\",\n        \"method\": \"GET\"\n      }\n    ],\n    \"name\": \"My API Collection\"\n  }'\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#post-v1networkexportinsomnia","title":"<code>POST /v1/network/export/insomnia</code>","text":"<p>Export endpoints as Insomnia Export v4 format, including optional environment variables.</p> <p>Best For</p> <p>API testing in Insomnia, organized request management.</p>","tags":["Network","Security"]},{"location":"api/network/#parameters_5","title":"Parameters","text":"Name Type Required Description <code>endpoints</code> array Yes Captured endpoints to export <code>name</code> string No Workspace name <code>include_environment</code> boolean No Include environment variables","tags":["Network","Security"]},{"location":"api/network/#example_3","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/network/export/insomnia \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"endpoints\": [\n      {\n        \"url\": \"https://api.example.com/users\",\n        \"method\": \"GET\"\n      }\n    ],\n    \"name\": \"My API\",\n    \"include_environment\": true\n  }'\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#response_3","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"_type\": \"export\",\n    \"__export_format\": 4,\n    \"resources\": [\n      {\"_id\": \"wrk_xxx\", \"_type\": \"workspace\", \"name\": \"My API\"},\n      {\"_id\": \"env_xxx\", \"_type\": \"environment\", \"name\": \"Base Environment\"},\n      {\"_id\": \"req_xxx\", \"_type\": \"request\", \"name\": \"GET /users\", \"url\": \"...\"}\n    ]\n  }\n}\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#post-v1networkexporthar","title":"<code>POST /v1/network/export/har</code>","text":"<p>Export endpoints as HAR (HTTP Archive) 1.2 format for browser DevTools import and traffic analysis.</p> <p>Best For</p> <p>Browser DevTools import, traffic analysis, performance profiling.</p>","tags":["Network","Security"]},{"location":"api/network/#parameters_6","title":"Parameters","text":"Name Type Required Description <code>page_url</code> string No Page URL context for the HAR file <code>endpoints</code> array Yes Captured endpoints to export","tags":["Network","Security"]},{"location":"api/network/#example_4","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/network/export/har \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"page_url\": \"https://example.com\",\n    \"endpoints\": [\n      {\n        \"url\": \"https://api.example.com/users\",\n        \"method\": \"GET\",\n        \"response\": {\"status\": 200}\n      }\n    ]\n  }'\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#export-format-comparison","title":"Export Format Comparison","text":"Format Version Use Case Import Tool OpenAPI 3.0.3 API documentation, SDK generation, contract testing Swagger UI, Redoc, Stoplight Postman v2.1 Manual API testing, team collaboration Postman App Insomnia v4 Request management, environment variables Insomnia App HAR 1.2 Traffic replay, performance analysis Browser DevTools, Charles Proxy","tags":["Network","Security"]},{"location":"api/network/#post-v1networkgraphqlintrospect","title":"<code>POST /v1/network/graphql/introspect</code>","text":"<p>Introspect a GraphQL API to discover its full schema, including types, queries, mutations, and subscriptions.</p> <p>Best For</p> <p>GraphQL API exploration, schema documentation, client code generation.</p>","tags":["Network","Security"]},{"location":"api/network/#parameters_7","title":"Parameters","text":"Name Type Required Description <code>endpoint_url</code> string Yes GraphQL endpoint URL <code>headers</code> object No Custom headers (e.g., authentication)","tags":["Network","Security"]},{"location":"api/network/#example_5","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/network/graphql/introspect \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"endpoint_url\": \"https://countries.trevorblades.com/graphql\"}'\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#response_4","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"introspection_enabled\": true,\n    \"schema\": {\n      \"queryType\": {\"name\": \"Query\"},\n      \"types\": [\n        {\"kind\": \"OBJECT\", \"name\": \"Country\", \"fields\": [\"...\"]},\n        {\"kind\": \"OBJECT\", \"name\": \"Continent\", \"fields\": [\"...\"]}\n      ]\n    }\n  }\n}\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#response-fields_1","title":"Response Fields","text":"Field Type Description <code>introspection_enabled</code> boolean Whether the GraphQL endpoint allows introspection <code>schema</code> object Full GraphQL schema including types, queries, and mutations <code>schema.queryType</code> object Root query type <code>schema.types</code> array All types defined in the schema","tags":["Network","Security"]},{"location":"api/network/#post-v1networkdetectauth","title":"<code>POST /v1/network/detect/auth</code>","text":"<p>Detect authentication methods used by captured API endpoints. Analyzes headers, cookies, and query parameters to identify auth patterns.</p> <p>Best For</p> <p>Security auditing, API documentation, understanding third-party authentication flows.</p>","tags":["Network","Security"]},{"location":"api/network/#parameters_8","title":"Parameters","text":"Name Type Required Description <code>endpoints</code> array Yes Captured endpoints with request headers","tags":["Network","Security"]},{"location":"api/network/#example_6","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/network/detect/auth \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"endpoints\": [\n      {\"request\": {\"headers\": {\"Authorization\": \"Bearer eyJ...\"}}}\n    ]\n  }'\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#response_5","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"authentication_methods\": [\n      {\"type\": \"bearer\", \"location\": \"header\", \"name\": \"Authorization\", \"confidence\": 1.0},\n      {\"type\": \"api_key\", \"location\": \"header\", \"name\": \"X-API-Key\", \"confidence\": 0.9}\n    ]\n  }\n}\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#response-fields_2","title":"Response Fields","text":"Field Type Description <code>authentication_methods</code> array Detected auth methods <code>authentication_methods[].type</code> string Auth type: <code>bearer</code>, <code>api_key</code>, <code>basic</code>, <code>oauth2</code>, <code>cookie</code> <code>authentication_methods[].location</code> string Where the auth is sent: <code>header</code>, <code>query</code>, <code>cookie</code> <code>authentication_methods[].name</code> string Header/parameter name <code>authentication_methods[].confidence</code> float Detection confidence (0.0 - 1.0)","tags":["Network","Security"]},{"location":"api/network/#post-v1networkinferschema","title":"<code>POST /v1/network/infer/schema</code>","text":"<p>Infer a JSON schema from API response samples. Provide multiple response examples and the endpoint will generate a unified schema.</p>","tags":["Network","Security"]},{"location":"api/network/#parameters_9","title":"Parameters","text":"Name Type Required Description <code>samples</code> array Yes Array of JSON response samples","tags":["Network","Security"]},{"location":"api/network/#example_7","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/network/infer/schema \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"samples\": [\n      {\"id\": 1, \"name\": \"John\", \"email\": \"john@example.com\"},\n      {\"id\": 2, \"name\": \"Jane\", \"email\": \"jane@example.com\"}\n    ]\n  }'\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#monitoring-endpoints","title":"Monitoring Endpoints","text":"<p>Track API changes over time, detect breaking changes, and receive webhook alerts when APIs are modified.</p>","tags":["Network","Security"]},{"location":"api/network/#post-v1networkmonitorcreate","title":"<code>POST /v1/network/monitor/create</code>","text":"<p>Create a new API monitor to track changes over time.</p> <p>Best For</p> <p>Detecting breaking API changes, compliance monitoring, SLA tracking.</p>","tags":["Network","Security"]},{"location":"api/network/#parameters_10","title":"Parameters","text":"Name Type Required Description <code>name</code> string Yes Monitor name <code>url</code> string Yes API URL to monitor <code>interval_minutes</code> integer No Check interval in minutes <code>alert_webhook</code> string No Webhook URL for change alerts (e.g., Slack)","tags":["Network","Security"]},{"location":"api/network/#example_8","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/network/monitor/create \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"GitHub API Monitor\",\n    \"url\": \"https://api.github.com\",\n    \"interval_minutes\": 60,\n    \"alert_webhook\": \"https://hooks.slack.com/...\"\n  }'\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#response_6","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"api_id\": \"abc123\",\n    \"name\": \"GitHub API Monitor\",\n    \"url\": \"https://api.github.com\",\n    \"interval_minutes\": 60,\n    \"enabled\": true,\n    \"next_check\": \"2026-02-04T09:00:00Z\"\n  }\n}\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#response-fields_3","title":"Response Fields","text":"Field Type Description <code>api_id</code> string Unique monitor identifier <code>name</code> string Monitor name <code>url</code> string Monitored URL <code>interval_minutes</code> integer Check interval <code>enabled</code> boolean Whether the monitor is active <code>next_check</code> string ISO 8601 timestamp of next scheduled check","tags":["Network","Security"]},{"location":"api/network/#get-v1networkmonitorlist","title":"<code>GET /v1/network/monitor/list</code>","text":"<p>List all API monitors.</p>","tags":["Network","Security"]},{"location":"api/network/#example_9","title":"Example","text":"<pre><code>curl http://localhost:8001/v1/network/monitor/list\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#get-v1networkmonitorapi_id","title":"<code>GET /v1/network/monitor/{api_id}</code>","text":"<p>Get details of a specific monitor including snapshots and detected changes.</p>","tags":["Network","Security"]},{"location":"api/network/#example_10","title":"Example","text":"<pre><code>curl http://localhost:8001/v1/network/monitor/abc123\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#post-v1networkmonitorapi_idcapture","title":"<code>POST /v1/network/monitor/{api_id}/capture</code>","text":"<p>Manually trigger a snapshot capture for a monitor (outside its normal schedule).</p>","tags":["Network","Security"]},{"location":"api/network/#example_11","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/network/monitor/abc123/capture\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#delete-v1networkmonitorapi_id","title":"<code>DELETE /v1/network/monitor/{api_id}</code>","text":"<p>Delete an API monitor and all its captured snapshots.</p>","tags":["Network","Security"]},{"location":"api/network/#example_12","title":"Example","text":"<pre><code>curl -X DELETE http://localhost:8001/v1/network/monitor/abc123\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#post-v1networkmonitorcompare","title":"<code>POST /v1/network/monitor/compare</code>","text":"<p>Compare two OpenAPI specifications and detect changes, including breaking changes. Ideal for CI/CD pipelines and API versioning workflows.</p> <p>Best For</p> <p>CI/CD pipelines, API versioning, breaking change detection.</p>","tags":["Network","Security"]},{"location":"api/network/#parameters_11","title":"Parameters","text":"Name Type Required Description <code>old_spec</code> object Yes Previous OpenAPI specification <code>new_spec</code> object Yes New OpenAPI specification","tags":["Network","Security"]},{"location":"api/network/#example_13","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/network/monitor/compare \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"old_spec\": {\n      \"paths\": {\n        \"/users\": {\"get\": {}},\n        \"/orders\": {\"delete\": {}}\n      }\n    },\n    \"new_spec\": {\n      \"paths\": {\n        \"/users\": {\"get\": {}, \"post\": {}}\n      }\n    }\n  }'\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#response_7","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"total_changes\": 2,\n    \"breaking_changes\": 1,\n    \"is_breaking\": true,\n    \"changes\": [\n      {\n        \"change_type\": \"endpoint_added\",\n        \"severity\": \"info\",\n        \"description\": \"New method: POST /users\"\n      },\n      {\n        \"change_type\": \"endpoint_removed\",\n        \"severity\": \"critical\",\n        \"description\": \"Removed endpoint: DELETE /orders\"\n      }\n    ]\n  }\n}\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#change-types","title":"Change Types","text":"Type Severity Description <code>endpoint_added</code> info New endpoint discovered <code>endpoint_removed</code> critical Endpoint removed (breaking change) <code>response_schema_changed</code> warning Response structure changed <code>request_schema_changed</code> warning Request parameters changed <code>status_code_changed</code> warning HTTP status code changed <p>Breaking Changes</p> <p>Changes with <code>critical</code> severity indicate breaking changes that may affect API consumers. The <code>is_breaking</code> field in the response is <code>true</code> when any critical change is detected.</p>","tags":["Network","Security"]},{"location":"api/network/#proxy-endpoints","title":"Proxy Endpoints","text":"<p>Start an HTTP proxy server to capture traffic from any application -- mobile apps, desktop apps, CLI tools, or any HTTP client. Clients must explicitly configure proxy settings (non-invasive).</p>","tags":["Network","Security"]},{"location":"api/network/#post-v1networkproxystart","title":"<code>POST /v1/network/proxy/start</code>","text":"<p>Start an HTTP proxy server for traffic capture.</p> <p>Best For</p> <p>Capturing traffic from mobile apps, desktop apps, CLI tools -- any application that supports proxy configuration.</p> <p>Non-Invasive</p> <p>Clients must explicitly configure their proxy settings. The proxy does not intercept traffic automatically.</p>","tags":["Network","Security"]},{"location":"api/network/#parameters_12","title":"Parameters","text":"Name Type Default Description <code>port</code> integer <code>8888</code> Proxy server port <code>filter_hosts</code> array -- Only capture traffic to these hosts <code>exclude_hosts</code> array -- Exclude traffic to these hosts","tags":["Network","Security"]},{"location":"api/network/#example_14","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/network/proxy/start \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"port\": 8888,\n    \"filter_hosts\": [\"api.example.com\"],\n    \"exclude_hosts\": [\"analytics.google.com\"]\n  }'\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#response_8","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"session_id\": \"abc123\",\n    \"port\": 8888,\n    \"status\": \"running\"\n  }\n}\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#post-v1networkproxystop","title":"<code>POST /v1/network/proxy/stop</code>","text":"<p>Stop the active proxy session and retrieve captured traffic.</p>","tags":["Network","Security"]},{"location":"api/network/#example_15","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/network/proxy/stop\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#get-v1networkproxystatus","title":"<code>GET /v1/network/proxy/status</code>","text":"<p>Get current proxy session status.</p>","tags":["Network","Security"]},{"location":"api/network/#example_16","title":"Example","text":"<pre><code>curl http://localhost:8001/v1/network/proxy/status\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#get-v1networkproxytraffic","title":"<code>GET /v1/network/proxy/traffic</code>","text":"<p>Get captured traffic from active or stopped proxy session.</p>","tags":["Network","Security"]},{"location":"api/network/#parameters_13","title":"Parameters","text":"Name Type Default Description <code>session_id</code> string -- Session ID (uses active session if omitted) <code>as_endpoints</code> boolean <code>false</code> Convert traffic to standard endpoint format","tags":["Network","Security"]},{"location":"api/network/#example_17","title":"Example","text":"<pre><code>curl \"http://localhost:8001/v1/network/proxy/traffic?as_endpoints=true\"\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#get-v1networkproxyinstructions","title":"<code>GET /v1/network/proxy/instructions</code>","text":"<p>Get setup instructions for configuring the proxy on different platforms and tools.</p>","tags":["Network","Security"]},{"location":"api/network/#example_18","title":"Example","text":"<pre><code>curl http://localhost:8001/v1/network/proxy/instructions\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#response_9","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"proxy_url\": \"http://192.168.1.100:8888\",\n    \"macos\": \"export HTTP_PROXY=http://192.168.1.100:8888\",\n    \"windows\": \"$env:HTTP_PROXY = \\\"http://192.168.1.100:8888\\\"\",\n    \"linux\": \"export HTTP_PROXY=http://192.168.1.100:8888\",\n    \"ios\": \"Settings &gt; Wi-Fi &gt; Configure Proxy &gt; Manual\",\n    \"android\": \"Settings &gt; Wi-Fi &gt; Modify network &gt; Proxy: Manual\",\n    \"curl\": \"curl -x http://192.168.1.100:8888 https://api.example.com\",\n    \"python\": \"requests.get(url, proxies={'http': 'http://192.168.1.100:8888'})\"\n  }\n}\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#platform-setup","title":"Platform Setup","text":"macOS / LinuxWindows (PowerShell)curlPythoniOSAndroid <pre><code>export HTTP_PROXY=http://192.168.1.100:8888\nexport HTTPS_PROXY=http://192.168.1.100:8888\n</code></pre> <pre><code>$env:HTTP_PROXY = \"http://192.168.1.100:8888\"\n$env:HTTPS_PROXY = \"http://192.168.1.100:8888\"\n</code></pre> <pre><code>curl -x http://192.168.1.100:8888 https://api.example.com\n</code></pre> <pre><code>import requests\nrequests.get(url, proxies={\"http\": \"http://192.168.1.100:8888\"})\n</code></pre> <p>Navigate to Settings &gt; Wi-Fi &gt; (your network) &gt; Configure Proxy &gt; Manual and enter the proxy host and port.</p> <p>Navigate to Settings &gt; Wi-Fi &gt; (long press network) &gt; Modify network &gt; Proxy: Manual and enter the proxy host and port.</p>","tags":["Network","Security"]},{"location":"api/network/#security-testing-endpoints","title":"Security Testing Endpoints","text":"","tags":["Network","Security"]},{"location":"api/network/#post-v1networkfuzz","title":"<code>POST /v1/network/fuzz</code>","text":"<p>Security fuzzing for API endpoints. Sends various attack payloads to test for common vulnerabilities.</p> <p>Use Responsibly</p> <p>Only use fuzzing against APIs you own or have explicit permission to test.</p>","tags":["Network","Security"]},{"location":"api/network/#parameters_14","title":"Parameters","text":"Name Type Required Description <code>url</code> string Yes Target URL to fuzz <code>params</code> object No Base query parameters <code>payloads</code> array No Payload types to use <code>max_requests</code> integer No Maximum number of fuzz requests","tags":["Network","Security"]},{"location":"api/network/#payload-types","title":"Payload Types","text":"Payload Description Example <code>sql_injection</code> SQL injection payloads <code>' OR '1'='1</code>, <code>1; DROP TABLE</code> <code>xss</code> Cross-site scripting <code>&lt;script&gt;alert(1)&lt;/script&gt;</code> <code>command_injection</code> OS command injection <code>; cat /etc/passwd</code> <code>path_traversal</code> Directory traversal <code>../../../etc/passwd</code>","tags":["Network","Security"]},{"location":"api/network/#example_19","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/network/fuzz \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"url\": \"https://httpbin.org/get\",\n    \"params\": {\"search\": \"test\"},\n    \"payloads\": [\"sql_injection\", \"xss\"],\n    \"max_requests\": 10\n  }'\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#post-v1networktestauth-bypass","title":"<code>POST /v1/network/test/auth-bypass</code>","text":"<p>Test for authentication bypass vulnerabilities by attempting requests without credentials, with modified tokens, and other bypass techniques.</p>","tags":["Network","Security"]},{"location":"api/network/#example_20","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/network/test/auth-bypass \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"endpoints\": [\n      {\n        \"url\": \"https://api.example.com/admin/users\",\n        \"method\": \"GET\",\n        \"request\": {\"headers\": {\"Authorization\": \"Bearer eyJ...\"}}\n      }\n    ]\n  }'\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#replay-endpoints","title":"Replay Endpoints","text":"","tags":["Network","Security"]},{"location":"api/network/#post-v1networkreplay","title":"<code>POST /v1/network/replay</code>","text":"<p>Replay a captured API request, optionally modifying headers, parameters, or body.</p>","tags":["Network","Security"]},{"location":"api/network/#parameters_15","title":"Parameters","text":"Name Type Required Description <code>endpoint</code> object Yes Captured endpoint object to replay <code>modify_headers</code> object No Headers to add or override <code>modify_params</code> object No Query parameters to add or override","tags":["Network","Security"]},{"location":"api/network/#example_21","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/network/replay \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"endpoint\": {\n      \"url\": \"https://api.example.com/users\",\n      \"method\": \"GET\",\n      \"request\": {\"headers\": {\"Authorization\": \"Bearer old_token\"}}\n    },\n    \"modify_headers\": {\"Authorization\": \"Bearer new_token\"},\n    \"modify_params\": {\"page\": \"2\"}\n  }'\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#post-v1networkreplaybatch","title":"<code>POST /v1/network/replay/batch</code>","text":"<p>Replay multiple captured requests in sequence or parallel.</p>","tags":["Network","Security"]},{"location":"api/network/#example_22","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/network/replay/batch \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"endpoints\": [\n      {\"url\": \"https://api.example.com/users\", \"method\": \"GET\"},\n      {\"url\": \"https://api.example.com/posts\", \"method\": \"GET\"}\n    ]\n  }'\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#analysis-endpoints","title":"Analysis Endpoints","text":"","tags":["Network","Security"]},{"location":"api/network/#post-v1networkanalyze","title":"<code>POST /v1/network/analyze</code>","text":"<p>Analyze captured endpoints for patterns, authentication mechanisms, API versioning, and common conventions.</p>","tags":["Network","Security"]},{"location":"api/network/#example_23","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/network/analyze \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"endpoints\": [\n      {\"url\": \"https://api.example.com/v2/users\", \"method\": \"GET\"},\n      {\"url\": \"https://api.example.com/v2/posts\", \"method\": \"GET\"}\n    ]\n  }'\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#post-v1networkdiff","title":"<code>POST /v1/network/diff</code>","text":"<p>Compare two sets of API captures to find differences in endpoints, parameters, response schemas, or authentication.</p>","tags":["Network","Security"]},{"location":"api/network/#example_24","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/network/diff \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"before\": [\n      {\"url\": \"https://api.example.com/users\", \"method\": \"GET\"}\n    ],\n    \"after\": [\n      {\"url\": \"https://api.example.com/users\", \"method\": \"GET\"},\n      {\"url\": \"https://api.example.com/users\", \"method\": \"POST\"}\n    ]\n  }'\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#post-v1networkmockgenerate","title":"<code>POST /v1/network/mock/generate</code>","text":"<p>Generate a mock server configuration from captured endpoints. Useful for frontend development, testing, and CI/CD environments.</p>","tags":["Network","Security"]},{"location":"api/network/#example_25","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/network/mock/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"endpoints\": [\n      {\n        \"url\": \"https://api.example.com/users\",\n        \"method\": \"GET\",\n        \"response\": {\"status\": 200, \"body\": [{\"id\": 1, \"name\": \"John\"}]}\n      }\n    ]\n  }'\n</code></pre>","tags":["Network","Security"]},{"location":"api/network/#post-v1networkwebsocketcapture","title":"<code>POST /v1/network/websocket/capture</code>","text":"<p>Capture WebSocket connections and messages from a webpage. Opens the target URL in a headless browser and records all WebSocket activity.</p>","tags":["Network","Security"]},{"location":"api/network/#parameters_16","title":"Parameters","text":"Name Type Default Description <code>url</code> string -- URL to capture WebSocket traffic from (required) <code>wait_time</code> integer <code>10</code> Seconds to listen for WebSocket messages","tags":["Network","Security"]},{"location":"api/network/#example_26","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/network/websocket/capture \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"url\": \"https://example.com/app\",\n    \"wait_time\": 10\n  }'\n</code></pre>","tags":["Network","Security"]},{"location":"api/recon/","title":"Reconnaissance","text":"<p>Website reconnaissance, CMS detection, and API discovery. Identify data access patterns and discover hidden APIs on any target.</p>","tags":["OSINT","Security"]},{"location":"api/recon/#overview","title":"Overview","text":"<p>The reconnaissance endpoints combine multiple analysis techniques into a single toolkit for understanding how a website exposes data. Use individual endpoints for targeted checks, or run a full recon sweep that combines all capabilities in one request.</p>","tags":["OSINT","Security"]},{"location":"api/recon/#key-features","title":"Key Features","text":"Feature Description CMS / Framework detection Drupal, WordPress, Next.js, Nuxt.js, Shopify, Gatsby, Ghost, Squarespace API endpoint discovery REST, GraphQL, JSON:API, RSS feeds Embedded data extraction JSON-LD, <code>__NEXT_DATA__</code>, Drupal settings, Redux / Vuex / Apollo state JSON:API exploration Discover Drupal resource types and build filter URLs Sitemap search Regex pattern matching against sitemap URLs Full reconnaissance All of the above in a single request","tags":["OSINT","Security"]},{"location":"api/recon/#endpoints","title":"Endpoints","text":"","tags":["OSINT","Security"]},{"location":"api/recon/#post-v1recondiscovercms-cms-detection","title":"POST <code>/v1/recon/discover/cms</code> -- CMS Detection","text":"<p>Detect the CMS or framework used by a website.</p>","tags":["OSINT","Security"]},{"location":"api/recon/#request-body","title":"Request Body","text":"<pre><code>{\n  \"url\": \"https://www.mass.gov\"\n}\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/recon/#detected-cms-types","title":"Detected CMS Types","text":"CMS Indicators <code>drupal</code> <code>x-generator</code> header, <code>/jsonapi</code>, <code>Drupal.settings</code> <code>wordpress</code> <code>/wp-json</code>, <code>/wp-content</code>, WP meta generator <code>nextjs</code> <code>__NEXT_DATA__</code>, <code>/_next/static</code> <code>nuxtjs</code> <code>__NUXT__</code>, <code>/_nuxt</code> <code>shopify</code> <code>cdn.shopify.com</code>, <code>Shopify.theme</code> <code>gatsby</code> Gatsby meta, <code>/__gatsby</code> <code>ghost</code> Ghost CMS indicators <code>squarespace</code> Squarespace headers","tags":["OSINT","Security"]},{"location":"api/recon/#example","title":"Example","text":"curlPython <pre><code>curl -X POST http://localhost:8001/v1/recon/discover/cms \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\": \"https://www.mass.gov\"}'\n</code></pre> <pre><code>import requests\n\nresp = requests.post(\"http://localhost:8001/v1/recon/discover/cms\", json={\n    \"url\": \"https://www.mass.gov\"\n})\ncms = resp.json()[\"data\"]\nprint(f\"{cms['cms']} v{cms['version']} (confidence {cms['confidence']})\")\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/recon/#response","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"cms\": \"drupal\",\n    \"version\": \"10\",\n    \"confidence\": 0.8,\n    \"indicators\": [\n      \"Header: x-generator\",\n      \"API: /jsonapi responds\",\n      \"API: /node/1?_format=json responds\"\n    ],\n    \"api_endpoints\": [\"/jsonapi\", \"/node/1?_format=json\"]\n  }\n}\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/recon/#post-v1recondiscoverapi-api-discovery","title":"POST <code>/v1/recon/discover/api</code> -- API Discovery","text":"<p>Discover API endpoints on a website by probing common patterns.</p>","tags":["OSINT","Security"]},{"location":"api/recon/#request-body_1","title":"Request Body","text":"<pre><code>{\n  \"url\": \"https://example.com\",\n  \"cms_hint\": \"drupal\"\n}\n</code></pre> Parameter Type Required Description <code>url</code> string yes Target website URL <code>cms_hint</code> string no Hint to narrow the probe set (e.g. <code>drupal</code>, <code>wordpress</code>)","tags":["OSINT","Security"]},{"location":"api/recon/#probed-patterns","title":"Probed Patterns","text":"<ul> <li>REST -- <code>/api</code>, <code>/api/v1</code>, <code>/api/v2</code>, <code>/rest</code></li> <li>Drupal -- <code>/jsonapi</code>, <code>/jsonapi/node</code>, <code>/jsonapi/media</code></li> <li>WordPress -- <code>/wp-json</code>, <code>/wp-json/wp/v2/posts</code></li> <li>GraphQL -- <code>/graphql</code>, <code>/api/graphql</code></li> <li>Feeds -- <code>/feed</code>, <code>/rss</code>, <code>/atom.xml</code></li> </ul>","tags":["OSINT","Security"]},{"location":"api/recon/#example_1","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/recon/discover/api \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\": \"https://example.com\", \"cms_hint\": \"drupal\"}'\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/recon/#response_1","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"url\": \"https://example.com\",\n    \"apis_found\": [\n      {\n        \"url\": \"https://example.com/jsonapi\",\n        \"type\": \"jsonapi\",\n        \"method\": \"GET\",\n        \"status\": 200,\n        \"content_type\": \"application/vnd.api+json\",\n        \"filterable\": true,\n        \"pagination\": true\n      },\n      {\n        \"url\": \"https://example.com/graphql\",\n        \"type\": \"graphql\",\n        \"method\": \"POST\",\n        \"status\": 200\n      }\n    ],\n    \"total\": 2\n  }\n}\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/recon/#post-v1reconextractembedded-embedded-data-extraction","title":"POST <code>/v1/recon/extract/embedded</code> -- Embedded Data Extraction","text":"<p>Extract embedded JSON and structured data from HTML pages.</p>","tags":["OSINT","Security"]},{"location":"api/recon/#request-body_2","title":"Request Body","text":"<pre><code>{\n  \"url\": \"https://www.example.com/product-page\"\n}\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/recon/#extracted-data-types","title":"Extracted Data Types","text":"Type Description <code>json-ld</code> Schema.org structured data <code>drupal-settings</code> Drupal JS settings object <code>next-data</code> Next.js <code>__NEXT_DATA__</code> <code>nuxt-data</code> Nuxt.js <code>__NUXT__</code> <code>initial-state</code> Redux / Vuex initial state <code>apollo-state</code> Apollo GraphQL cache","tags":["OSINT","Security"]},{"location":"api/recon/#example_2","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/recon/extract/embedded \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\": \"https://www.example.com/product-page\"}'\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/recon/#response_2","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"url\": \"https://www.example.com/product-page\",\n    \"embedded_data\": [\n      {\n        \"type\": \"json-ld\",\n        \"data\": {\n          \"@context\": \"https://schema.org\",\n          \"@type\": \"Product\",\n          \"name\": \"Example Product\"\n        },\n        \"location\": \"https://www.example.com/product-page\"\n      },\n      {\n        \"type\": \"next-data\",\n        \"data\": {\"props\": {\"pageProps\": {}}},\n        \"location\": \"https://www.example.com/product-page\"\n      }\n    ],\n    \"total\": 2\n  }\n}\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/recon/#post-v1reconjsonapiexplore-jsonapi-exploration","title":"POST <code>/v1/recon/jsonapi/explore</code> -- JSON:API Exploration","text":"<p>Explore a Drupal JSON:API endpoint to discover available resource types and build filter URLs.</p>","tags":["OSINT","Security"]},{"location":"api/recon/#request-body_3","title":"Request Body","text":"<pre><code>{\n  \"url\": \"https://www.mass.gov\",\n  \"search_term\": \"media\"\n}\n</code></pre> Parameter Type Required Description <code>url</code> string yes Target Drupal site URL <code>search_term</code> string no Filter resource types by keyword","tags":["OSINT","Security"]},{"location":"api/recon/#example_3","title":"Example","text":"curlPython <pre><code>curl -X POST http://localhost:8001/v1/recon/jsonapi/explore \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\": \"https://www.mass.gov\", \"search_term\": \"media\"}'\n</code></pre> <pre><code>import requests\n\nresp = requests.post(\"http://localhost:8001/v1/recon/jsonapi/explore\", json={\n    \"url\": \"https://www.mass.gov\",\n    \"search_term\": \"media\",\n})\ndata = resp.json()[\"data\"]\nprint(f\"Found {data['total_types']} resource types\")\nfor t in data[\"available_types\"]:\n    print(f\"  {t['type']} -&gt; {t['href']}\")\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/recon/#response_3","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"jsonapi_root\": \"https://www.mass.gov/jsonapi\",\n    \"total_types\": 275,\n    \"available_types\": [\n      {\"type\": \"media--document\", \"href\": \"https://www.mass.gov/jsonapi/media/document\"},\n      {\"type\": \"media--media_video\", \"href\": \"https://www.mass.gov/jsonapi/media/media_video\"},\n      {\"type\": \"node--page\", \"href\": \"https://www.mass.gov/jsonapi/node/page\"}\n    ],\n    \"suggested_filters\": [\n      {\n        \"description\": \"Search media--document by title\",\n        \"url\": \"https://www.mass.gov/jsonapi/media/document?filter[title][operator]=CONTAINS&amp;filter[title][value]=SEARCH_TERM\"\n      }\n    ],\n    \"search_term\": \"media\"\n  }\n}\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/recon/#jsonapi-filter-examples","title":"JSON:API Filter Examples","text":"<pre><code># Get breach notifications starting with \"2025-\"\ncurl \"https://example.gov/jsonapi/media/document?filter[field_title][condition][path]=field_title&amp;filter[field_title][condition][operator]=STARTS_WITH&amp;filter[field_title][condition][value]=2025-\"\n\n# Sort by creation date (newest first)\ncurl \"https://example.gov/jsonapi/media/document?sort=-created&amp;page[limit]=50\"\n\n# Combine filters\ncurl \"https://example.gov/jsonapi/node/page?filter[status]=1&amp;filter[title][operator]=CONTAINS&amp;filter[title][value]=breach\"\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/recon/#post-v1reconsitemapsearch-sitemap-search","title":"POST <code>/v1/recon/sitemap/search</code> -- Sitemap Search","text":"<p>Search a website's sitemap for URLs matching a regex pattern.</p>","tags":["OSINT","Security"]},{"location":"api/recon/#request-body_4","title":"Request Body","text":"<pre><code>{\n  \"url\": \"https://example.com\",\n  \"pattern\": \"breach.*pdf\",\n  \"limit\": 100\n}\n</code></pre> Parameter Type Required Default Description <code>url</code> string yes -- Target website URL <code>pattern</code> string yes -- Regex pattern to match against sitemap URLs <code>limit</code> int no 100 Maximum matching URLs to return <p>Sitemap Access</p> <p>Some sites (e.g. mass.gov) block sitemap access from datacenter IPs. Use a residential proxy for these targets.</p>","tags":["OSINT","Security"]},{"location":"api/recon/#example_4","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/recon/sitemap/search \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\": \"https://example.com\", \"pattern\": \"breach.*pdf\", \"limit\": 100}'\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/recon/#response_4","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"sitemap_url\": \"https://example.com/sitemap.xml\",\n    \"total_urls\": 15000,\n    \"matching_urls\": [\n      \"https://example.com/docs/breach-report-2024.pdf\",\n      \"https://example.com/docs/breach-notification.pdf\"\n    ],\n    \"pattern\": \"breach.*pdf\",\n    \"truncated\": false\n  }\n}\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/recon/#post-v1recon-full-reconnaissance","title":"POST <code>/v1/recon</code> -- Full Reconnaissance","text":"<p>Perform full reconnaissance on a website, combining CMS detection, API discovery, embedded data extraction, and sitemap search in a single request.</p>","tags":["OSINT","Security"]},{"location":"api/recon/#request-body_5","title":"Request Body","text":"<pre><code>{\n  \"url\": \"https://www.mass.gov\",\n  \"goal\": \"breach notification data\"\n}\n</code></pre> Parameter Type Required Description <code>url</code> string yes Target website URL <code>goal</code> string no Description of what you are looking for (improves recommendations)","tags":["OSINT","Security"]},{"location":"api/recon/#example_5","title":"Example","text":"curlPython <pre><code>curl -X POST http://localhost:8001/v1/recon \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\": \"https://www.mass.gov\", \"goal\": \"breach notification data\"}'\n</code></pre> <pre><code>import requests\n\nresp = requests.post(\"http://localhost:8001/v1/recon\", json={\n    \"url\": \"https://www.mass.gov\",\n    \"goal\": \"breach notification data\",\n})\nrecon = resp.json()[\"data\"]\nprint(f\"CMS: {recon['cms']['cms']} v{recon['cms']['version']}\")\nprint(f\"APIs found: {len(recon['apis_found'])}\")\nfor rec in recon[\"recommendations\"]:\n    print(f\"  -&gt; {rec}\")\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/recon/#response_5","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"domain\": \"www.mass.gov\",\n    \"cms\": {\n      \"cms\": \"drupal\",\n      \"version\": \"10\",\n      \"confidence\": 0.8,\n      \"indicators\": [\"Header: x-generator\", \"API: /jsonapi responds\"],\n      \"api_endpoints\": [\"/jsonapi\", \"/node/1?_format=json\"]\n    },\n    \"apis_found\": [\n      {\"url\": \"https://www.mass.gov/jsonapi\", \"type\": \"jsonapi\", \"status\": 200}\n    ],\n    \"feeds_found\": [],\n    \"embedded_data\": [],\n    \"sitemap_urls\": [],\n    \"technologies\": [\"drupal (confidence: 80%)\"],\n    \"recommendations\": [\n      \"Use /jsonapi/media/document with filters like ?filter[field_title][operator]=STARTS_WITH&amp;filter[field_title][value]=PREFIX\"\n    ]\n  }\n}\n</code></pre>","tags":["OSINT","Security"]},{"location":"api/recon/#use-cases","title":"Use Cases","text":"Audience Purpose Security researchers Discover API attack surface before testing OSINT analysts Find data access patterns for investigations Developers Understand how a site exposes data Data engineers Identify data sources for scraping and integration","tags":["OSINT","Security"]},{"location":"api/rules/","title":"Rules Engine","text":"<p>The Rules API provides a declarative rule engine with three subsystems: Declarative Rulesets for automated content extraction, Sigma Rules for log analysis and SIEM integration, and YARA Rules for binary pattern matching.</p>","tags":["Security","Automation"]},{"location":"api/rules/#declarative-rulesets","title":"Declarative Rulesets","text":"<p>Manage declarative rulesets for automated content extraction and actions. Rulesets target specific domains and contain rule groups with URL patterns and scraping/action rules.</p>","tags":["Security","Automation"]},{"location":"api/rules/#list-rulesets","title":"List Rulesets","text":"<p><code>GET /v1/rules</code></p> <p>List all loaded rulesets, optionally filtered by domain.</p> <p>Parameters:</p> Name Type Default Description <code>domain</code> string -- Filter by target domain <code>enabled_only</code> boolean <code>true</code> Only return enabled rulesets cURLList All <pre><code>curl \"http://localhost:8001/v1/rules?domain=example.com\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/rules?enabled_only=false\"\n</code></pre> Response <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"rulesets\": [\n      {\n        \"id\": \"rs_abc123\",\n        \"name\": \"Example Ruleset\",\n        \"description\": \"Extract data from example.com\",\n        \"target_domains\": [\"example.com\"],\n        \"rule_groups_count\": 3,\n        \"created_at\": \"2026-01-15T10:30:00Z\",\n        \"updated_at\": \"2026-01-20T14:00:00Z\"\n      }\n    ],\n    \"total\": 1\n  }\n}\n</code></pre>","tags":["Security","Automation"]},{"location":"api/rules/#create-ruleset","title":"Create Ruleset","text":"<p><code>POST /v1/rules</code></p> <p>Create a new ruleset. You can provide either <code>yaml_content</code> (raw YAML string) or structured <code>rule_groups</code>.</p> YAML Content <pre><code>curl -X POST http://localhost:8001/v1/rules \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"ruleset_name\": \"My Ruleset\",\n    \"description\": \"Extract product data\",\n    \"target_domains\": [\"shop.example.com\"],\n    \"yaml_content\": \"ruleset_name: My Ruleset\\nrule_groups:\\n  - group_name: products\\n    url_patterns:\\n      - \\\"/product/*\\\"\\n    scraping_rules:\\n      - field: title\\n        selector: h1.product-title\"\n  }'\n</code></pre> Response <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"id\": \"rs_def456\",\n    \"name\": \"My Ruleset\",\n    \"rule_groups_count\": 1,\n    \"message\": \"Ruleset created successfully\"\n  }\n}\n</code></pre>","tags":["Security","Automation"]},{"location":"api/rules/#get-ruleset","title":"Get Ruleset","text":"<p><code>GET /v1/rules/{ruleset_id}</code></p> <p>Retrieve a ruleset by its ID.</p> <p>Parameters:</p> Name Type Default Description <code>include_yaml</code> boolean <code>false</code> Include YAML representation in response <pre><code>curl \"http://localhost:8001/v1/rules/rs_abc123?include_yaml=true\"\n</code></pre>","tags":["Security","Automation"]},{"location":"api/rules/#update-ruleset","title":"Update Ruleset","text":"<p><code>PUT /v1/rules/{ruleset_id}</code></p> <p>Update a ruleset. Only the provided fields are modified.</p> <p>Request Body:</p> <pre><code>{\n  \"description\": \"Updated description\",\n  \"target_domains\": [\"shop.example.com\", \"store.example.com\"]\n}\n</code></pre> <pre><code>curl -X PUT http://localhost:8001/v1/rules/rs_abc123 \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"description\": \"Updated description\", \"target_domains\": [\"shop.example.com\", \"store.example.com\"]}'\n</code></pre>","tags":["Security","Automation"]},{"location":"api/rules/#delete-ruleset","title":"Delete Ruleset","text":"<p><code>DELETE /v1/rules/{ruleset_id}</code></p> <p>Delete a ruleset permanently.</p> <pre><code>curl -X DELETE \"http://localhost:8001/v1/rules/rs_abc123\"\n</code></pre>","tags":["Security","Automation"]},{"location":"api/rules/#validate-ruleset","title":"Validate Ruleset","text":"<p><code>POST /v1/rules/validate</code></p> <p>Validate YAML content without loading it into the rule engine. Useful for CI/CD pipelines or editor integrations.</p> cURL <pre><code>curl -X POST http://localhost:8001/v1/rules/validate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"yaml_content\": \"ruleset_name: Test\\nrule_groups:\\n  - group_name: test\"}'\n</code></pre> Response <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"valid\": true,\n    \"errors\": [],\n    \"warnings\": [\"No scraping_rules defined in group 'test'\"]\n  }\n}\n</code></pre> <p>Validation in CI/CD</p> <p>Use the validate endpoint to check rulesets before deploying them. A response with <code>valid: true</code> but non-empty <code>warnings</code> means the ruleset will load but may not behave as expected.</p>","tags":["Security","Automation"]},{"location":"api/rules/#test-ruleset","title":"Test Ruleset","text":"<p><code>POST /v1/rules/test</code></p> <p>Test a ruleset against a live URL to preview extraction results.</p> By Ruleset IDWith Inline YAML <pre><code>curl -X POST http://localhost:8001/v1/rules/test \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"url\": \"https://example.com/product/123\",\n    \"ruleset_id\": \"rs_abc123\",\n    \"extract_data\": true\n  }'\n</code></pre> <pre><code>curl -X POST http://localhost:8001/v1/rules/test \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"url\": \"https://example.com/product/123\",\n    \"yaml_content\": \"ruleset_name: Test\\nrule_groups:\\n  - group_name: products\\n    url_patterns:\\n      - \\\"/product/*\\\"\\n    scraping_rules:\\n      - field: title\\n        selector: h1\",\n    \"extract_data\": true\n  }'\n</code></pre> Response <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"url\": \"https://example.com/product/123\",\n    \"ruleset_name\": \"Product Extractor\",\n    \"matching_groups\": 1,\n    \"matched_group_names\": [\"product_page\"],\n    \"extraction_results\": [\"...\"],\n    \"total_fields_extracted\": 5\n  }\n}\n</code></pre>","tags":["Security","Automation"]},{"location":"api/rules/#match-url","title":"Match URL","text":"<p><code>POST /v1/rules/match</code></p> <p>Find all rulesets that match a given URL. Useful for debugging which rules would apply to a page.</p> <p>Parameters:</p> Name Type Description <code>url</code> string URL to match against all rulesets (required) <pre><code>curl -X POST \"http://localhost:8001/v1/rules/match?url=https://example.com/page\"\n</code></pre> Response <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"url\": \"https://example.com/page\",\n    \"matches\": [\n      {\n        \"ruleset_id\": \"rs_abc123\",\n        \"ruleset_name\": \"Example Ruleset\",\n        \"matching_groups\": [\n          {\n            \"group_name\": \"main_content\",\n            \"scraping_rules\": 5,\n            \"action_rules\": 2\n          }\n        ]\n      }\n    ],\n    \"total_rulesets\": 1,\n    \"total_groups\": 1\n  }\n}\n</code></pre>","tags":["Security","Automation"]},{"location":"api/rules/#sigma-rules","title":"Sigma Rules","text":"<p>Sigma is a generic and open signature format for SIEM systems. The Sigma Rules API allows you to validate, convert, parse, and match Sigma rules against log data.</p> <p>Supported Backends</p> <p>Sigma rules can be converted to queries for: Splunk, Elastic, QRadar, and native Sigma format.</p>","tags":["Security","Automation"]},{"location":"api/rules/#list-backends","title":"List Backends","text":"<p><code>GET /v1/rules/sigma/backends</code></p> <p>List available Sigma conversion backends.</p> <pre><code>curl http://localhost:8001/v1/rules/sigma/backends\n</code></pre> Response <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"backends\": [\"splunk\", \"elastic\", \"qradar\", \"sigma\"]\n  }\n}\n</code></pre>","tags":["Security","Automation"]},{"location":"api/rules/#validate-sigma-rule","title":"Validate Sigma Rule","text":"<p><code>POST /v1/rules/sigma/validate</code></p> <p>Validate the syntax and structure of a Sigma rule.</p> <p>Request Body:</p> <pre><code>{\n  \"rule\": \"title: Test Rule\\nstatus: test\\nlogsource:\\n  product: windows\\ndetection:\\n  selection:\\n    EventID: 4624\\n  condition: selection\"\n}\n</code></pre> cURL <pre><code>curl -X POST http://localhost:8001/v1/rules/sigma/validate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"rule\": \"title: Test Rule\\nstatus: test\\nlogsource:\\n  product: windows\\ndetection:\\n  selection:\\n    EventID: 4624\\n  condition: selection\"}'\n</code></pre> Response <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"valid\": true,\n    \"errors\": [],\n    \"warnings\": []\n  }\n}\n</code></pre>","tags":["Security","Automation"]},{"location":"api/rules/#convert-sigma-rule","title":"Convert Sigma Rule","text":"<p><code>POST /v1/rules/sigma/convert</code></p> <p>Convert a Sigma rule to a backend-specific query language (e.g., Splunk SPL, Elastic Query DSL).</p> <p>Request Body:</p> Field Type Description <code>rule</code> string Sigma rule in YAML format (required) <code>backend</code> string Target backend: <code>splunk</code>, <code>elastic</code>, <code>qradar</code>, <code>sigma</code> (required) cURL <pre><code>curl -X POST http://localhost:8001/v1/rules/sigma/convert \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"rule\": \"title: Login Event\\nstatus: test\\nlogsource:\\n  product: windows\\ndetection:\\n  selection:\\n    EventID: 4624\\n  condition: selection\",\n    \"backend\": \"splunk\"\n  }'\n</code></pre> Response <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"backend\": \"splunk\",\n    \"query\": \"EventID=4624\"\n  }\n}\n</code></pre>","tags":["Security","Automation"]},{"location":"api/rules/#parse-sigma-rule","title":"Parse Sigma Rule","text":"<p><code>POST /v1/rules/sigma/parse</code></p> <p>Parse a Sigma rule and extract its metadata, detection logic, and structure.</p> <p>Request Body:</p> <pre><code>{\n  \"rule\": \"title: Test Rule\\nstatus: test\\n...\"\n}\n</code></pre> <pre><code>curl -X POST http://localhost:8001/v1/rules/sigma/parse \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"rule\": \"title: Test Rule\\nstatus: test\\nlogsource:\\n  product: windows\\ndetection:\\n  selection:\\n    EventID: 4624\\n  condition: selection\"}'\n</code></pre>","tags":["Security","Automation"]},{"location":"api/rules/#match-logs-against-sigma-rule","title":"Match Logs Against Sigma Rule","text":"<p><code>POST /v1/rules/sigma/match</code></p> <p>Evaluate log events against a Sigma rule and return matching entries.</p> <p>Request Body:</p> Field Type Description <code>rule</code> string Sigma rule in YAML format (required) <code>logs</code> array Array of log event objects to evaluate (required) cURL <pre><code>curl -X POST http://localhost:8001/v1/rules/sigma/match \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"rule\": \"title: Login Event\\nstatus: test\\nlogsource:\\n  product: windows\\ndetection:\\n  selection:\\n    EventID: 4624\\n  condition: selection\",\n    \"logs\": [\n      {\"EventID\": 4624, \"User\": \"admin\"},\n      {\"EventID\": 4625, \"User\": \"guest\"}\n    ]\n  }'\n</code></pre> Response <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"matches\": [\n      {\"index\": 0, \"log\": {\"EventID\": 4624, \"User\": \"admin\"}}\n    ],\n    \"total_logs\": 2,\n    \"matched\": 1\n  }\n}\n</code></pre>","tags":["Security","Automation"]},{"location":"api/rules/#yara-rules","title":"YARA Rules","text":"<p>YARA is a tool for identifying and classifying malware samples via pattern matching. The YARA Rules API enables validation, compilation, and scanning of data or URLs against YARA rules.</p>","tags":["Security","Automation"]},{"location":"api/rules/#engine-info","title":"Engine Info","text":"<p><code>GET /v1/rules/yara/info</code></p> <p>Check YARA engine availability and version.</p> <pre><code>curl http://localhost:8001/v1/rules/yara/info\n</code></pre> Response <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"available\": true,\n    \"version\": \"4.3.0\"\n  }\n}\n</code></pre>","tags":["Security","Automation"]},{"location":"api/rules/#validate-yara-rule","title":"Validate YARA Rule","text":"<p><code>POST /v1/rules/yara/validate</code></p> <p>Validate a YARA rule for syntax correctness.</p> <p>Request Body:</p> <pre><code>{\n  \"rule\": \"rule test_rule { strings: $a = \\\"test\\\" condition: $a }\"\n}\n</code></pre> <pre><code>curl -X POST http://localhost:8001/v1/rules/yara/validate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"rule\": \"rule test_rule { strings: $a = \\\"test\\\" condition: $a }\"}'\n</code></pre>","tags":["Security","Automation"]},{"location":"api/rules/#compile-yara-rule","title":"Compile YARA Rule","text":"<p><code>POST /v1/rules/yara/compile</code></p> <p>Compile a YARA rule and return compilation statistics. Useful for checking rule complexity.</p> <p>Request Body:</p> <pre><code>{\n  \"rule\": \"rule test_rule { strings: $a = \\\"test\\\" condition: $a }\"\n}\n</code></pre> <pre><code>curl -X POST http://localhost:8001/v1/rules/yara/compile \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"rule\": \"rule test_rule { strings: $a = \\\"test\\\" condition: $a }\"}'\n</code></pre>","tags":["Security","Automation"]},{"location":"api/rules/#scan-data","title":"Scan Data","text":"<p><code>POST /v1/rules/yara/scan</code></p> <p>Scan raw data (base64-encoded) against a YARA rule.</p> <p>Request Body:</p> Field Type Description <code>rule</code> string YARA rule source (required) <code>data</code> string Data to scan (required) <code>encoding</code> string Encoding of data field: <code>base64</code>, <code>utf-8</code> cURL <pre><code>curl -X POST http://localhost:8001/v1/rules/yara/scan \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"rule\": \"rule test_rule { strings: $a = \\\"malware\\\" condition: $a }\",\n    \"data\": \"base64-encoded-data\",\n    \"encoding\": \"base64\"\n  }'\n</code></pre> Response <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"matches\": [\n      {\n        \"rule\": \"test_rule\",\n        \"strings\": [\n          {\"identifier\": \"$a\", \"offset\": 10, \"data\": \"malware\"}\n        ],\n        \"meta\": {}\n      }\n    ],\n    \"scanned_bytes\": 1024,\n    \"matched\": true\n  }\n}\n</code></pre>","tags":["Security","Automation"]},{"location":"api/rules/#scan-url-content","title":"Scan URL Content","text":"<p><code>POST /v1/rules/yara/scan/url</code></p> <p>Fetch content from a URL and scan it against a YARA rule.</p> <p>Request Body:</p> Field Type Description <code>rule</code> string YARA rule source (required) <code>url</code> string URL whose content will be scanned (required) <pre><code>curl -X POST http://localhost:8001/v1/rules/yara/scan/url \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"rule\": \"rule detect_php { strings: $a = \\\"&lt;?php\\\" condition: $a }\",\n    \"url\": \"https://example.com/script.php\"\n  }'\n</code></pre>","tags":["Security","Automation"]},{"location":"api/search/","title":"Search &amp; System Endpoints","text":"<p>The Search API provides access to 70+ search engines through a unified interface. All endpoints return structured JSON with consistent response envelopes, pagination, and optional NLP entity enrichment.</p>","tags":["Search"]},{"location":"api/search/#system-endpoints","title":"System Endpoints","text":"<p>System endpoints provide health checks, version information, and cache management. They are essential for monitoring, orchestration, and operational tasks.</p>","tags":["Search"]},{"location":"api/search/#get-v1healthlive","title":"GET <code>/v1/health/live</code>","text":"<p>Kubernetes liveness probe. Fast check with no dependency verification.</p> <p>Best For</p> <p>Container orchestration health checks that need sub-millisecond response times.</p> <p>Target Audience</p> <ul> <li>DevOps engineers configuring Kubernetes deployments</li> <li>Infrastructure teams setting up load balancer health checks</li> <li>Monitoring systems (Prometheus, Datadog) checking service availability</li> </ul> <ul> <li>Instantly confirms the API process is running</li> <li>Does not check dependencies (use <code>/v1/health/ready</code> for that)</li> <li>Safe to call frequently without performance impact</li> <li>Returns <code>200</code> if the process is alive, even if dependencies are down</li> </ul> <p>Example Request:</p> <pre><code>curl http://localhost:8001/v1/health/live\n</code></pre> <p>Response:</p> <pre><code>{\"status\": \"alive\"}\n</code></pre>","tags":["Search"]},{"location":"api/search/#get-v1healthready","title":"GET <code>/v1/health/ready</code>","text":"<p>Kubernetes readiness probe. Checks critical dependencies.</p> <p>Best For</p> <p>Determining if the API is ready to accept traffic.</p> <p>Target Audience</p> <ul> <li>Kubernetes operators configuring readiness probes</li> <li>Load balancers deciding which instances to route traffic to</li> <li>Deployment pipelines waiting for healthy rollout</li> </ul> <ul> <li>Verifies cache (Valkey) and search backend (SearXNG) are accessible</li> <li>Returns <code>503</code> if critical dependencies are down (prevents traffic routing)</li> <li>Use in rolling deployments to ensure new pods are ready before receiving traffic</li> </ul> <p>Example Request:</p> <pre><code>curl http://localhost:8001/v1/health/ready\n</code></pre> Healthy (200)Not Ready (503) <pre><code>{\n  \"status\": \"ready\",\n  \"checks\": {\n    \"valkey\": true,\n    \"searxng\": true\n  }\n}\n</code></pre> <pre><code>{\n  \"status\": \"not_ready\",\n  \"checks\": {\n    \"valkey\": false,\n    \"searxng\": true\n  }\n}\n</code></pre>","tags":["Search"]},{"location":"api/search/#get-v1healthdeep","title":"GET <code>/v1/health/deep</code>","text":"<p>Full health check with all service statuses.</p> <p>Best For</p> <p>Comprehensive system diagnostics and debugging connectivity issues.</p> <p>Target Audience</p> <ul> <li>System administrators troubleshooting service issues</li> <li>On-call engineers diagnosing outages</li> <li>Monitoring dashboards displaying service status</li> <li>Support teams verifying infrastructure health</li> </ul> <ul> <li>Shows status of all services: API, SearXNG, Valkey, Selectors, MinerU, NLP</li> <li>Includes memory usage, uptime, and connection details</li> <li>Identifies which specific component is failing</li> <li>Not recommended for high-frequency polling (use <code>/v1/health/ready</code> instead)</li> </ul> <p>Example Request:</p> <pre><code># Check why searches are failing\ncurl http://localhost:8001/v1/health/deep | jq '.data.checks'\n# If searxng.status is \"unhealthy\", the search backend is down\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"status\": \"healthy\",\n    \"checks\": {\n      \"api\": {\n        \"status\": \"healthy\",\n        \"uptime_seconds\": 12\n      },\n      \"searxng\": {\n        \"status\": \"healthy\",\n        \"url\": \"http://searxng:8080\"\n      },\n      \"valkey\": {\n        \"status\": \"healthy\",\n        \"memory_used\": \"1.32M\"\n      },\n      \"selectors\": {\n        \"status\": \"healthy\",\n        \"url\": \"http://selectors:8002\"\n      },\n      \"mineru\": {\n        \"status\": \"healthy\",\n        \"url\": \"http://mineru:7986\"\n      },\n      \"nlp\": {\n        \"status\": \"healthy\",\n        \"model\": \"en_core_web_sm\"\n      }\n    }\n  },\n  \"meta\": {\n    \"request_id\": \"1d2b5824-debf-4c11-83bb-637ba6d627c5\",\n    \"timestamp\": \"2026-01-31T03:12:35.437350Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 18\n  }\n}\n</code></pre> <p>Health Check Status Values:</p> Status Description <code>healthy</code> Service is operational <code>degraded</code> Service is partially working <code>unhealthy</code> Service is down <code>not_configured</code> Service is not configured <code>unavailable</code> Service cannot be reached <code>not_loaded</code> Component not loaded (e.g., NLP model)","tags":["Search"]},{"location":"api/search/#get-v1version","title":"GET <code>/v1/version</code>","text":"<p>API version information.</p> <p>Best For</p> <p>Version checking before making API calls.</p> <ul> <li>Verify you are connecting to the expected API version</li> <li>Useful for multi-environment setups (dev/staging/prod)</li> <li>Lightweight call for connection testing</li> </ul> <p>Example Request:</p> <pre><code>curl http://localhost:8001/v1/version\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"api_version\": \"1.0.0\",\n    \"name\": \"SearXNG Unified API\",\n    \"description\": \"Self-hosted search, extraction, and intelligence API\"\n  },\n  \"meta\": {\n    \"request_id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\",\n    \"timestamp\": \"2026-01-31T03:00:00.000000Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 0\n  }\n}\n</code></pre>","tags":["Search"]},{"location":"api/search/#get-v1cachestats","title":"GET <code>/v1/cache/stats</code>","text":"<p>Cache statistics.</p> <p>Best For</p> <p>Monitoring cache performance and capacity planning.</p> <ul> <li>Track cache hit rates and memory consumption</li> <li>Identify when cache is getting full</li> <li>Monitor connected clients for connection leak detection</li> </ul> <p>Example Request:</p> <pre><code>curl http://localhost:8001/v1/cache/stats\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"total_keys\": 42,\n    \"memory_used\": \"5.2M\",\n    \"connected_clients\": 3,\n    \"uptime_days\": 7\n  },\n  \"meta\": {\n    \"request_id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\",\n    \"timestamp\": \"2026-01-31T03:00:00.000000Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 2\n  }\n}\n</code></pre>","tags":["Search"]},{"location":"api/search/#delete-v1cache","title":"DELETE <code>/v1/cache</code>","text":"<p>Clear cached search results.</p> <p>Best For</p> <p>Forcing fresh results when cached data is stale or incorrect.</p> <ul> <li>Clear stale search results after SearXNG configuration changes</li> <li>Force fresh data during development/testing</li> <li>Recover from corrupted cache entries</li> </ul> <p>Parameters:</p> Name Type Default Description <code>pattern</code> string <code>search:*</code> Key pattern to clear (must start with <code>search:</code>) <p>Example Request:</p> <pre><code># Clear all cached searches for \"python\"\ncurl -X DELETE \"http://localhost:8001/v1/cache?pattern=search:python*\"\n\n# Clear all search cache after updating SearXNG engines\ncurl -X DELETE \"http://localhost:8001/v1/cache?pattern=search:*\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"cleared\": 15,\n    \"pattern\": \"search:*\"\n  },\n  \"meta\": {\n    \"request_id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\",\n    \"timestamp\": \"2026-01-31T03:00:00.000000Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 5\n  }\n}\n</code></pre>","tags":["Search"]},{"location":"api/search/#search-endpoints","title":"Search Endpoints","text":"<p>All search endpoints follow a consistent pattern: they accept a query string parameter <code>q</code>, optional <code>limit</code>, <code>cursor</code> for pagination, and <code>fields</code> for response field selection. Responses use a standard envelope with <code>success</code>, <code>data</code>, <code>meta</code>, and optional <code>pagination</code> sections.</p>","tags":["Search"]},{"location":"api/search/#get-v1search","title":"GET <code>/v1/search</code>","text":"<p>Execute a web search with optional NLP enrichment.</p> <p>Best For</p> <p>General-purpose web searching with entity extraction for data enrichment.</p> <p>Target Audience</p> <ul> <li>Researchers -- Gathering information with automatic entity tagging</li> <li>Data analysts -- Building datasets with structured entity data</li> <li>OSINT investigators -- Finding and categorizing public information</li> <li>Content curators -- Discovering relevant content with metadata</li> <li>Developers -- Building search-powered applications</li> </ul> <p>Why Use This:</p> <ul> <li>Aggregates results from 70+ search engines (not just one source)</li> <li>Automatically extracts entities (people, organizations, locations, dates, money)</li> <li>Built-in caching reduces latency for repeated queries</li> <li>Field selection reduces payload size for bandwidth-sensitive apps</li> <li>Supports advanced operators like <code>site:</code>, <code>filetype:</code>, <code>intitle:</code></li> </ul> <p>Parameters:</p> Name Type Required Default Description <code>q</code> string Yes -- Search query (supports Google dork syntax) <code>limit</code> integer No <code>10</code> Maximum results (1-100) <code>cursor</code> string No -- Pagination cursor <code>engines</code> string No <code>google,brave,duckduckgo</code> Comma-separated engine list <code>fields</code> string No -- Comma-separated fields to return <code>enrich</code> boolean No <code>true</code> Add NLP entity extraction <code>llm</code> boolean No <code>false</code> Add LLM summaries (slower)","tags":["Search"]},{"location":"api/search/#dork-syntax","title":"Dork Syntax","text":"<p>The search endpoint supports Google dork syntax for advanced queries:</p> Operator Example Description <code>site:</code> <code>site:example.com</code> Limit to domain <code>filetype:</code> <code>filetype:pdf</code> Specific file types <code>intitle:</code> <code>intitle:\"phrase\"</code> Words in title <code>\"...\"</code> <code>\"exact phrase\"</code> Exact match <code>-</code> <code>-exclude</code> Exclude terms <p>Example Requests:</p> Basic SearchCompany ResearchSecurity ResearchField SelectionLLM Summaries <pre><code>curl \"http://localhost:8001/v1/search?q=site:github.com+python&amp;limit=5&amp;enrich=true\"\n</code></pre> <pre><code># Find news and extract mentioned people/orgs\ncurl \"http://localhost:8001/v1/search?q=OpenAI+funding&amp;enrich=true&amp;limit=20\"\n</code></pre> <pre><code># Find exposed documents\ncurl \"http://localhost:8001/v1/search?q=site:company.com+filetype:pdf+confidential\"\n</code></pre> <pre><code># Get only specific fields to reduce payload\ncurl \"http://localhost:8001/v1/search?q=python+tutorial&amp;fields=title,url,snippet\"\n</code></pre> <pre><code># Add LLM summaries for each result (slower but more useful)\ncurl \"http://localhost:8001/v1/search?q=machine+learning+basics&amp;llm=true&amp;limit=5\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"query\": \"site:github.com python\", // (1)\n    \"results\": [\n      {\n        \"title\": \"Python - GitHub\",\n        \"url\": \"https://github.com/python\",\n        \"domain\": \"github.com\",\n        \"snippet\": \"The Python programming language...\",\n        \"engines\": [\"google\", \"brave\", \"duckduckgo\"], // (2)\n        \"score\": 25.5, // (3)\n        \"position\": 1,\n        \"entities\": [ // (4)\n          {\"text\": \"Python\", \"label\": \"ORG\", \"description\": \"Organization\"}\n        ],\n        \"published_date\": null,\n        \"thumbnail\": \"\",\n        \"llm_summary\": null\n      }\n    ],\n    \"engines_used\": [\"google\", \"brave\", \"duckduckgo\", \"bing\", \"startpage\"],\n    \"entities_summary\": { // (5)\n      \"ORG\": {\n        \"description\": \"Organization\",\n        \"items\": [\"Python\", \"GitHub\"]\n      },\n      \"GPE\": {\n        \"description\": \"Location\",\n        \"items\": [\"San Francisco\"]\n      }\n    }\n  },\n  \"meta\": {\n    \"request_id\": \"2e46fc86-54c2-4582-b9f2-34dda38ed691\",\n    \"timestamp\": \"2026-01-31T03:12:51.192585Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 2500,\n    \"cached\": false\n  },\n  \"pagination\": {\n    \"next_cursor\": \"eyJpZCI6ICI1In0=\",\n    \"has_more\": true,\n    \"returned\": 5\n  }\n}\n</code></pre> <ol> <li>The original search query is echoed back exactly as submitted, including any dork operators. Useful for confirming the server parsed your request correctly.</li> <li>Lists every search engine that returned this specific result. When multiple engines agree on a result, it increases confidence in relevance.</li> <li>Aggregated relevance score across all engines. Higher values indicate stronger relevance. A result returned by 3 engines scores higher than one returned by a single engine.</li> <li>NLP-extracted entities from the result snippet. Only populated when <code>enrich=true</code> (the default). Each entity includes a spaCy label code and human-readable description.</li> <li>Aggregated entity counts across all results in the response, grouped by entity type. Useful for building faceted navigation or understanding the dominant topics in a result set.</li> </ol>","tags":["Search"]},{"location":"api/search/#search-result-fields","title":"Search Result Fields","text":"Field Type Description <code>title</code> string Result title <code>url</code> string Full URL <code>domain</code> string Domain extracted from URL <code>snippet</code> string Content snippet (max 500 chars) <code>engines</code> array Search engines that returned this result <code>score</code> float Relevance score (higher is better) <code>position</code> integer Position in results (1-indexed) <code>entities</code> array Extracted NLP entities (if <code>enrich=true</code>) <code>published_date</code> string Publication date (if available) <code>thumbnail</code> string Thumbnail URL (if available) <code>llm_summary</code> string LLM-generated summary (if <code>llm=true</code>)","tags":["Search"]},{"location":"api/search/#entity-object","title":"Entity Object","text":"Field Type Description <code>text</code> string Entity text <code>label</code> string Entity type code (see table below) <code>description</code> string Human-readable label description","tags":["Search"]},{"location":"api/search/#nlp-entity-labels","title":"NLP Entity Labels","text":"Label Description <code>ORG</code> Organization <code>PERSON</code> Person <code>GPE</code> Location (Geo-Political Entity) <code>DATE</code> Date <code>MONEY</code> Money <code>EVENT</code> Event <code>PRODUCT</code> Product <code>LAW</code> Law/Regulation <code>PERCENT</code> Percentage <code>CARDINAL</code> Number","tags":["Search"]},{"location":"api/search/#get-v1searchimages","title":"GET <code>/v1/search/images</code>","text":"<p>Search for images.</p> <p>Best For</p> <p>Finding images for projects, research, or content creation.</p> <p>Target Audience</p> <ul> <li>Designers -- Finding reference images and inspiration</li> <li>Content creators -- Sourcing images for articles and presentations</li> <li>Researchers -- Collecting visual data for analysis</li> <li>Developers -- Building image galleries or search features</li> </ul> <p>Why Use This:</p> <ul> <li>Aggregates images from multiple search engines (Bing, Startpage, etc.)</li> <li>Provides both full-size URLs and thumbnails for preview</li> <li>Safe search enabled by default to filter inappropriate content</li> <li>Returns source domain for attribution</li> </ul> <p>Parameters:</p> Name Type Required Default Description <code>q</code> string Yes -- Image search query <code>limit</code> integer No <code>20</code> Maximum results (1-100) <code>cursor</code> string No -- Pagination cursor <code>safe</code> boolean No <code>true</code> Safe search filter <code>fields</code> string No -- Fields to return <p>Example Requests:</p> Basic Image SearchDesign ReferenceProduct Photos <pre><code>curl \"http://localhost:8001/v1/search/images?q=sunset+beach&amp;limit=5\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/search/images?q=minimalist+logo+design&amp;limit=20\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/search/images?q=leather+wallet+product+photo&amp;limit=10\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"query\": \"cat\",\n    \"results\": [\n      {\n        \"title\": \"Cat Day Pictures: Your Shot's Favorite Felines | National Geographic\",\n        \"url\": \"https://i.natgeofe.com/n/fa48a59d-202e-4f47-9a22-6672c2b025d7/GettyImages-83454800-CROP_4x3.jpg\",\n        \"thumbnail\": \"https://ts1.mm.bing.net/th?id=OIP.MyEv2JqEIeTwpOtV7liSFAHaFj&amp;pid=15.1\",\n        \"source\": \"www.nationalgeographic.com\",\n        \"width\": null,\n        \"height\": null,\n        \"engines\": [\"bing images\", \"startpage images\"]\n      }\n    ],\n    \"total\": 5\n  },\n  \"meta\": {\n    \"request_id\": \"18decbd3-5f66-424b-bf9d-62ce82ceedcf\",\n    \"timestamp\": \"2026-01-31T03:13:30.830338Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 5294\n  }\n}\n</code></pre> <p>Image Result Fields:</p> Field Type Description <code>title</code> string Image title/alt text <code>url</code> string Full-size image URL <code>thumbnail</code> string Thumbnail URL <code>source</code> string Source website domain <code>width</code> integer Image width in pixels (if available) <code>height</code> integer Image height in pixels (if available) <code>engines</code> array Search engines that returned this result","tags":["Search"]},{"location":"api/search/#get-v1searchvideos","title":"GET <code>/v1/search/videos</code>","text":"<p>Search for videos.</p> <p>Best For</p> <p>Finding video content across multiple platforms.</p> <p>Target Audience</p> <ul> <li>Educators -- Finding tutorial and educational videos</li> <li>Marketers -- Researching video content strategies</li> <li>Journalists -- Finding video clips for stories</li> <li>Researchers -- Collecting video sources for analysis</li> </ul> <p>Why Use This:</p> <ul> <li>Searches YouTube, PeerTube, and other video platforms simultaneously</li> <li>Returns video metadata including duration and publish date</li> <li>No need to search each platform separately</li> <li>Useful for building video aggregation features</li> </ul> <p>Parameters:</p> Name Type Required Default Description <code>q</code> string Yes -- Video search query <code>limit</code> integer No <code>20</code> Maximum results (1-100) <code>cursor</code> string No -- Pagination cursor <code>fields</code> string No -- Fields to return <p>Example Requests:</p> Tutorial SearchProduct DemoDocumentary <pre><code>curl \"http://localhost:8001/v1/search/videos?q=react+hooks+tutorial+2026&amp;limit=10\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/search/videos?q=%22Company+Name%22+product+demo\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/search/videos?q=climate+change+documentary&amp;limit=20\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"query\": \"cooking tutorial\",\n    \"results\": [\n      {\n        \"title\": \"Easy Cooking Tutorial\",\n        \"url\": \"https://youtube.com/watch?v=abc123\",\n        \"thumbnail\": \"https://i.ytimg.com/vi/abc123/hqdefault.jpg\",\n        \"duration\": \"10:30\",\n        \"source\": \"youtube.com\",\n        \"published_date\": \"2026-01-15T00:00:00Z\",\n        \"engines\": [\"youtube\", \"peertube\"]\n      }\n    ],\n    \"total\": 5\n  },\n  \"meta\": {\n    \"request_id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\",\n    \"timestamp\": \"2026-01-31T12:00:00.000000Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 2500\n  }\n}\n</code></pre> <p>Video Result Fields:</p> Field Type Description <code>title</code> string Video title <code>url</code> string Video page URL <code>thumbnail</code> string Video thumbnail URL <code>duration</code> string Video duration (format varies) <code>source</code> string Source platform domain <code>published_date</code> string Publication date (ISO 8601, if available) <code>engines</code> array Search engines that returned this result","tags":["Search"]},{"location":"api/search/#get-v1searchnews","title":"GET <code>/v1/search/news</code>","text":"<p>Search for news articles.</p> <p>Best For</p> <p>Monitoring news coverage and staying current on topics.</p> <p>Target Audience</p> <ul> <li>PR professionals -- Tracking brand mentions and media coverage</li> <li>Analysts -- Following industry news and market trends</li> <li>Journalists -- Researching stories and finding sources</li> <li>Investors -- Monitoring company and market news</li> <li>Researchers -- Building news datasets for analysis</li> </ul> <p>Why Use This:</p> <ul> <li>Time-filtered searches (past day, week, month, year)</li> <li>Aggregates from Bing News, Startpage News, and other sources</li> <li>Returns publication timestamps for timeline analysis</li> <li>Includes article thumbnails when available</li> </ul> <p>Parameters:</p> Name Type Required Default Description <code>q</code> string Yes -- News search query <code>limit</code> integer No <code>20</code> Maximum results (1-100) <code>cursor</code> string No -- Pagination cursor <code>freshness</code> string No <code>week</code> Time range: <code>day</code>, <code>week</code>, <code>month</code>, <code>year</code> <code>fields</code> string No -- Fields to return <p>Example Requests:</p> Breaking NewsTrend AnalysisDaily DigestBrand Monitoring <pre><code>curl \"http://localhost:8001/v1/search/news?q=Tesla+earnings&amp;freshness=day\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/search/news?q=artificial+intelligence+regulation&amp;freshness=month&amp;limit=50\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/search/news?q=cybersecurity+breach&amp;freshness=day&amp;limit=20\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/search/news?q=%22your+company%22+OR+%22competitor%22&amp;freshness=day\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"query\": \"technology\",\n    \"freshness\": \"day\",\n    \"results\": [\n      {\n        \"title\": \"Exxon says it has the technology needed for Venezuela's high-cost crude\",\n        \"url\": \"https://www.msn.com/en-us/money/markets/exxon-says-it-has-the-technology-needed-for-venezuelas-high-cost-crude/ar-AA1Vl3Bs\",\n        \"source\": \"www.msn.com\",\n        \"snippet\": \"By Sheila Dang and Arunima Kumar Jan 30 (Reuters) - Exxon Mobil CEO Darren Woods told analysts on Friday that the company has the technology...\",\n        \"published_date\": \"2026-01-30T16:01:32\",\n        \"thumbnail\": \"https://www.bing.com/th?id=ORMS.76701531e3f638dcbbc1973f46d8ac57&amp;pid=News\",\n        \"engines\": [\"bing news\", \"startpage news\"]\n      }\n    ],\n    \"total\": 10\n  },\n  \"meta\": {\n    \"request_id\": \"17817de9-8c07-4b9d-ad01-aa6f80dc2ad1\",\n    \"timestamp\": \"2026-01-31T03:31:32.390286Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 1625\n  }\n}\n</code></pre> <p>News Result Fields:</p> Field Type Description <code>title</code> string Article headline <code>url</code> string Article URL <code>source</code> string News source domain <code>snippet</code> string Article snippet (max 300 chars) <code>published_date</code> string Publication timestamp (if available) <code>thumbnail</code> string Article thumbnail URL (if available) <code>engines</code> array Search engines that returned this result","tags":["Search"]},{"location":"api/search/#get-v1searchsuggest","title":"GET <code>/v1/search/suggest</code>","text":"<p>Get autocomplete suggestions.</p> <p>Best For</p> <p>Implementing search-as-you-type functionality in applications.</p> <p>Target Audience</p> <ul> <li>Frontend developers -- Building search boxes with autocomplete</li> <li>UX designers -- Improving search discoverability</li> <li>Product teams -- Enhancing search user experience</li> </ul> <p>Why Use This:</p> <ul> <li>Provides instant suggestions as users type</li> <li>Reduces user effort and improves search accuracy</li> <li>Sub-200ms response times for real-time autocomplete</li> <li>Integrates with frontend frameworks (React, Vue, etc.)</li> </ul> <p>Parameters:</p> Name Type Required Description <code>q</code> string Yes Partial search query <p>Example Requests:</p> curlJavaScript Integration <pre><code>curl \"http://localhost:8001/v1/search/suggest?q=prog\"\n</code></pre> <pre><code>// Debounced autocomplete\nconst fetchSuggestions = async (query) =&gt; {\n  const response = await fetch(\n    `/v1/search/suggest?q=${encodeURIComponent(query)}`\n  );\n  const data = await response.json();\n  return data.data.suggestions;\n};\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"query\": \"prog\",\n    \"suggestions\": [\n      \"programming\",\n      \"programming languages\",\n      \"progress\",\n      \"program\"\n    ]\n  },\n  \"meta\": {\n    \"request_id\": \"a1b2c3d4-e5f6-7890-abcd-ef1234567890\",\n    \"timestamp\": \"2026-01-31T03:00:00.000000Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 150\n  }\n}\n</code></pre>","tags":["Search"]},{"location":"api/search/#get-v1searchshopping","title":"GET <code>/v1/search/shopping</code>","text":"<p>Search for products with price filtering.</p> <p>Best For</p> <p>Product research, price comparison, and e-commerce data collection.</p> <p>Target Audience</p> <ul> <li>Consumers -- Comparing prices across retailers</li> <li>Market researchers -- Analyzing product pricing trends</li> <li>E-commerce teams -- Competitive price monitoring</li> <li>Procurement teams -- Finding suppliers and comparing costs</li> </ul> <p>Why Use This:</p> <ul> <li>Filter results by price range to match budget</li> <li>Compare prices across multiple retailers in one query</li> <li>Extract pricing data for competitive analysis</li> <li>Currency-aware filtering for international searches</li> </ul> <p>Parameters:</p> Name Type Required Default Description <code>q</code> string Yes -- Product search query <code>limit</code> integer No <code>20</code> Maximum results (1-100) <code>cursor</code> string No -- Pagination cursor <code>min_price</code> float No -- Minimum price filter <code>max_price</code> float No -- Maximum price filter <code>currency</code> string No <code>USD</code> Currency code <code>fields</code> string No -- Fields to return <p>Example Requests:</p> Budget SearchPrice ComparisonInternational PricingPrice Monitoring <pre><code>curl \"http://localhost:8001/v1/search/shopping?q=gaming+laptop&amp;min_price=800&amp;max_price=1500\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/search/shopping?q=Sony+WH-1000XM5+headphones\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/search/shopping?q=espresso+machine&amp;currency=EUR&amp;max_price=500\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/search/shopping?q=iPhone+15+Pro+256GB&amp;fields=title,url,price,domain\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"query\": \"laptop\",\n    \"filters\": {\n      \"min_price\": 500,\n      \"max_price\": 1500,\n      \"currency\": \"USD\"\n    },\n    \"results\": [\n      {\n        \"title\": \"Dell XPS 15 Laptop\",\n        \"url\": \"https://...\",\n        \"domain\": \"dell.com\",\n        \"snippet\": \"15.6\\\" laptop with Intel Core i7...\",\n        \"price\": 1299.99,\n        \"currency\": \"USD\",\n        \"engines\": [\"google\", \"brave\"]\n      }\n    ],\n    \"total\": 10\n  },\n  \"meta\": {...}\n}\n</code></pre>","tags":["Search"]},{"location":"api/search/#get-v1searchclustered","title":"GET <code>/v1/search/clustered</code>","text":"<p>Search with automatic topic clustering using ML embeddings.</p> <p>Best For</p> <p>Understanding the landscape of a topic by grouping related results.</p> <p>Target Audience</p> <ul> <li>Researchers -- Understanding different aspects of a research topic</li> <li>Analysts -- Identifying market segments or trend categories</li> <li>Content strategists -- Finding content gaps and topic clusters</li> <li>Students -- Exploring different angles of a subject</li> <li>Product managers -- Understanding user needs by cluster</li> </ul> <p>Why Use This:</p> <ul> <li>Automatically groups semantically similar results using ML</li> <li>Reveals different perspectives/subtopics within a query</li> <li>HDBSCAN finds natural clusters; K-Means forces exact cluster count</li> <li>Uses sentence-transformers for semantic understanding</li> <li>Great for exploratory research and topic mapping</li> </ul> <p>Parameters:</p> Name Type Required Default Description <code>q</code> string Yes -- Search query <code>limit</code> integer No <code>20</code> Maximum results (1-100) <code>method</code> string No <code>hdbscan</code> Clustering method: <code>hdbscan</code> or <code>kmeans</code> <code>n_clusters</code> integer No <code>5</code> Number of clusters (for kmeans) <code>fields</code> string No -- Fields to return <p>Example Requests:</p> Topic ExplorationForced Cluster CountContent Gap AnalysisPython Topic Map <pre><code>curl \"http://localhost:8001/v1/search/clustered?q=artificial+intelligence&amp;limit=30\"\n# Returns clusters like: \"Machine Learning\", \"AI Ethics\", \"Neural Networks\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/search/clustered?q=electric+vehicles&amp;method=kmeans&amp;n_clusters=5\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/search/clustered?q=site:yourcompetitor.com&amp;limit=50\"\n</code></pre> <pre><code>import requests\n\nresponse = requests.get('http://localhost:8001/v1/search/clustered', params={\n    'q': 'python programming',\n    'limit': 50,\n    'method': 'hdbscan'\n})\ndata = response.json()\n\nfor cluster in data['data']['clusters']:\n    print(f\"{cluster['label']} ({cluster['count']} results)\")\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"query\": \"artificial intelligence\",\n    \"results\": [\n      {\n        \"title\": \"Introduction to Machine Learning\",\n        \"url\": \"https://...\",\n        \"cluster_id\": 0,\n        \"cluster_label\": \"Machine Learning Algorithms\"\n      }\n    ],\n    \"clusters\": [\n      {\"id\": 0, \"label\": \"Machine Learning Algorithms\", \"count\": 8},\n      {\"id\": 1, \"label\": \"Neural Networks Deep\", \"count\": 6},\n      {\"id\": 2, \"label\": \"AI Ethics Regulation\", \"count\": 4}\n    ],\n    \"total\": 18\n  },\n  \"meta\": {...}\n}\n</code></pre>","tags":["Search"]},{"location":"api/search/#get-v1searchscholar","title":"GET <code>/v1/search/scholar</code>","text":"<p>Search academic papers and scholarly content.</p> <p>Best For</p> <p>Finding peer-reviewed research, papers, and academic sources.</p> <p>Target Audience</p> <ul> <li>Researchers -- Finding papers for literature reviews</li> <li>Students -- Sourcing academic references for assignments</li> <li>Scientists -- Discovering prior art and related work</li> <li>Patent attorneys -- Prior art searches</li> <li>R&amp;D teams -- Technical research and validation</li> </ul> <p>Why Use This:</p> <ul> <li>Searches academic databases (ArXiv, PubMed, PDB, BASE, etc.)</li> <li>Returns structured academic metadata (DOI, authors, year)</li> <li>Includes direct PDF links when available</li> <li>Extracts publication year for filtering</li> </ul> <p>Parameters:</p> Name Type Required Default Description <code>q</code> string Yes -- Academic search query <code>limit</code> integer No <code>20</code> Maximum results (1-100) <code>cursor</code> string No -- Pagination cursor <code>fields</code> string No -- Fields to return <p>Example Requests:</p> Literature ReviewAuthor SearchMethodology SearchFilter PDFs (jq)Python Bibliography <pre><code>curl \"http://localhost:8001/v1/search/scholar?q=transformer+neural+networks&amp;limit=30\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/search/scholar?q=author:Geoffrey+Hinton+deep+learning\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/search/scholar?q=CRISPR+gene+editing+methodology\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/search/scholar?q=quantum+computing+algorithms&amp;limit=20\" \\\n  | jq '.data.results[] | select(.pdf_url != null)'\n</code></pre> <pre><code>import requests\n\nresponse = requests.get('http://localhost:8001/v1/search/scholar', params={\n    'q': 'large language models safety',\n    'limit': 50\n})\n\nfor paper in response.json()['data']['results']:\n    print(f\"- {paper['title']} ({paper.get('publication_year', 'N/A')})\")\n    if paper.get('doi'):\n        print(f\"  DOI: {paper['doi']}\")\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"query\": \"machine learning\",\n    \"results\": [\n      {\n        \"title\": \"T33-ml30 - Designed Tetrahedral Protein Cage Using Machine Learning Algorithms\",\n        \"url\": \"https://www.ebi.ac.uk/pdbe/entry/pdb/8ukm\",\n        \"snippet\": \"A suite of designed protein cages using machine learning and protein fragment-based protocols...\",\n        \"source\": \"www.ebi.ac.uk\",\n        \"engines\": [\"pdbe\"],\n        \"publication_year\": 2024,\n        \"authors\": [],\n        \"doi\": null,\n        \"pdf_url\": null\n      }\n    ],\n    \"total\": 5\n  },\n  \"meta\": {\n    \"request_id\": \"f83a3183-bc41-4b49-9d0b-eb5adcf0db8e\",\n    \"timestamp\": \"2026-01-31T03:31:52.032059Z\",\n    \"version\": \"1.0.0\",\n    \"processing_time_ms\": 1681\n  }\n}\n</code></pre> <p>Scholar Result Fields:</p> Field Type Description <code>title</code> string Paper/article title <code>url</code> string Paper URL <code>snippet</code> string Abstract or summary (max 500 chars) <code>source</code> string Source domain (arxiv.org, pubmed.gov, etc.) <code>engines</code> array Academic search engines that returned this <code>publication_year</code> integer Year of publication (extracted from text) <code>authors</code> array List of author names (if available) <code>doi</code> string Digital Object Identifier (if available) <code>pdf_url</code> string Direct PDF URL (if available)","tags":["Search"]},{"location":"api/search/#get-v1searchmusic","title":"GET <code>/v1/search/music</code>","text":"<p>Search for music across streaming platforms and music databases.</p> <p>Best For</p> <p>Finding songs, artists, albums, and music metadata.</p> <p>Engines: Bandcamp, SoundCloud, Deezer, YouTube Music, Mixcloud, Genius (lyrics)</p> <p>Parameters:</p> Name Type Required Default Description <code>q</code> string Yes -- Music search query (artist, song, album) <code>limit</code> integer No <code>20</code> Maximum results (1-100) <code>cursor</code> string No -- Pagination cursor <code>fields</code> string No -- Fields to return <p>Example Request:</p> <pre><code>curl \"http://localhost:8001/v1/search/music?q=electronic+ambient&amp;limit=10\"\n</code></pre> <p>Response Fields:</p> Field Type Description <code>title</code> string Track/album title <code>url</code> string Link to music page <code>content</code> string Description or lyrics snippet <code>source</code> string Platform (bandcamp, soundcloud, etc.) <code>thumbnail</code> string Album art URL","tags":["Search"]},{"location":"api/search/#get-v1searchfiles","title":"GET <code>/v1/search/files</code>","text":"<p>Search for files, torrents, and downloadable content.</p> <p>Best For</p> <p>Finding software, ISOs, documents, and open-source downloads.</p> <p>Engines: Anna's Archive, PirateBay, Kickass, App Stores, 1337x</p> <p>Parameters:</p> Name Type Required Default Description <code>q</code> string Yes -- File search query <code>limit</code> integer No <code>20</code> Maximum results (1-100) <code>cursor</code> string No -- Pagination cursor <code>fields</code> string No -- Fields to return <p>Example Request:</p> <pre><code>curl \"http://localhost:8001/v1/search/files?q=linux+iso&amp;limit=10\"\n</code></pre> <p>Response Fields:</p> Field Type Description <code>title</code> string File/torrent name <code>url</code> string Download or info page URL <code>content</code> string File description <code>filesize</code> string File size (if available) <code>source</code> string Source site <code>magnet</code> string Magnet link (for torrents)","tags":["Search"]},{"location":"api/search/#get-v1searchsocial","title":"GET <code>/v1/search/social</code>","text":"<p>Search social media platforms and communities.</p> <p>Best For</p> <p>Finding discussions, community posts, and social content.</p> <p>Engines: Reddit, Mastodon, Lemmy communities</p> <p>Parameters:</p> Name Type Required Default Description <code>q</code> string Yes -- Social media search query <code>limit</code> integer No <code>20</code> Maximum results (1-100) <code>cursor</code> string No -- Pagination cursor <code>fields</code> string No -- Fields to return <p>Example Request:</p> <pre><code>curl \"http://localhost:8001/v1/search/social?q=programming+tips&amp;limit=10\"\n</code></pre> <p>Response Fields:</p> Field Type Description <code>title</code> string Post title <code>url</code> string Post URL <code>content</code> string Post content/snippet <code>platform</code> string Platform name (reddit, mastodon, lemmy) <code>subreddit</code> string Subreddit/community name <code>author</code> string Post author","tags":["Search"]},{"location":"api/search/#get-v1searchrepos","title":"GET <code>/v1/search/repos</code>","text":"<p>Search code repositories and developer platforms.</p> <p>Best For</p> <p>Finding open-source projects, code examples, and developer tools.</p> <p>Engines: GitHub, GitLab, Codeberg, HuggingFace, Ollama models</p> <p>Parameters:</p> Name Type Required Default Description <code>q</code> string Yes -- Repository/code search query <code>limit</code> integer No <code>20</code> Maximum results (1-100) <code>cursor</code> string No -- Pagination cursor <code>fields</code> string No -- Fields to return <p>Example Request:</p> <pre><code>curl \"http://localhost:8001/v1/search/repos?q=fastapi+template&amp;limit=10\"\n</code></pre> <p>Response Fields:</p> Field Type Description <code>title</code> string Repository name <code>url</code> string Repository URL <code>content</code> string Repository description <code>platform</code> string Platform (github, gitlab, huggingface) <code>stars</code> integer Star count (if available) <code>language</code> string Primary language","tags":["Search"]},{"location":"api/search/#get-v1searchmap","title":"GET <code>/v1/search/map</code>","text":"<p>Search for locations, places, and geographic data.</p> <p>Best For</p> <p>Finding addresses, businesses, and points of interest.</p> <p>Engines: OpenStreetMap, Apple Maps, Photon geocoder</p> <p>Parameters:</p> Name Type Required Default Description <code>q</code> string Yes -- Location/place search query <code>limit</code> integer No <code>20</code> Maximum results (1-50) <code>cursor</code> string No -- Pagination cursor <code>fields</code> string No -- Fields to return <p>Example Request:</p> <pre><code>curl \"http://localhost:8001/v1/search/map?q=coffee+shops+seattle&amp;limit=10\"\n</code></pre> <p>Response Fields:</p> Field Type Description <code>title</code> string Place name <code>url</code> string Map link <code>address</code> string Full address <code>latitude</code> float Latitude coordinate <code>longitude</code> float Longitude coordinate <code>osm_id</code> string OpenStreetMap ID","tags":["Search"]},{"location":"api/search/#get-v1searchonion","title":"GET <code>/v1/search/onion</code>","text":"<p>Search the Tor network (.onion sites) safely and anonymously.</p> <p>Best For</p> <p>Privacy-focused research and dark web content discovery.</p> <p>Performance Note</p> <p>Tor searches are slower (10-30s) due to onion routing latency.</p> <p>Engines: Ahmia (Tor Project-endorsed), Torch</p> <p>Security Features:</p> <ul> <li>All queries routed through Tor SOCKS5h proxy (DNS leak safe)</li> <li>No IP address exposure to .onion sites</li> <li>Results filtered by Ahmia's blacklist (removes harmful content)</li> </ul> <p>Parameters:</p> Name Type Required Default Description <code>q</code> string Yes -- Search query for .onion sites <code>limit</code> integer No <code>20</code> Maximum results (1-100) <code>cursor</code> string No -- Pagination cursor <code>fields</code> string No -- Fields to return <p>Example Request:</p> <pre><code>curl \"http://localhost:8001/v1/search/onion?q=privacy+tools&amp;limit=10\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"query\": \"privacy tools\",\n    \"results\": [\n      {\n        \"title\": \"Privacy Tools - The Hidden Wiki\",\n        \"url\": \"http://zqktlwi...onion/wiki/Privacy\",\n        \"content\": \"Collection of privacy-focused tools and services...\",\n        \"is_onion\": true,\n        \"source\": \"zqktlwi...onion\",\n        \"engines\": [\"ahmia\"]\n      }\n    ],\n    \"total\": 20,\n    \"tor_routed\": true,\n    \"engines_available\": [\"ahmia\", \"torch\"],\n    \"warning\": \"Results contain .onion URLs requiring Tor Browser to access\"\n  }\n}\n</code></pre> <p>Response Fields:</p> Field Type Description <code>title</code> string Page title <code>url</code> string .onion URL <code>content</code> string Page snippet <code>is_onion</code> boolean Always <code>true</code> for this endpoint <code>source</code> string Source domain <code>tor_routed</code> boolean Confirms query went through Tor","tags":["Search"]},{"location":"api/search/#get-v1searchit","title":"GET <code>/v1/search/it</code>","text":"<p>Search technical and programming resources.</p> <p>Best For</p> <p>Finding code examples, documentation, and developer Q&amp;A.</p> <p>Engines: Stack Overflow, Stack Exchange, SuperUser, GitHub Gist, MDN Web Docs, DevDocs, Hacker News, Lobsters, Ask Ubuntu</p> <p>Parameters:</p> Name Type Required Default Description <code>q</code> string Yes -- Technical/programming search query <code>limit</code> integer No <code>20</code> Maximum results (1-100) <code>cursor</code> string No -- Pagination cursor <code>fields</code> string No -- Fields to return <p>Example Request:</p> <pre><code>curl \"http://localhost:8001/v1/search/it?q=python+async+await+tutorial&amp;limit=10\"\n</code></pre> <p>Response Fields:</p> Field Type Description <code>title</code> string Article/question title <code>url</code> string Resource URL <code>content</code> string Snippet or answer preview <code>domain</code> string Source domain <code>type</code> string Result type: <code>q&amp;a</code>, <code>code</code>, <code>documentation</code>, <code>article</code> <code>engines</code> array Source engines","tags":["Search"]},{"location":"api/search/#get-v1searchpackages","title":"GET <code>/v1/search/packages</code>","text":"<p>Search package registries and software repositories.</p> <p>Best For</p> <p>Finding libraries, frameworks, and dependencies for your projects.</p> <p>Engines: PyPI (Python), npm (JavaScript), crates.io (Rust), Docker Hub, Packagist (PHP), RubyGems, NuGet</p> <p>Parameters:</p> Name Type Required Default Description <code>q</code> string Yes -- Package/library search query <code>limit</code> integer No <code>20</code> Maximum results (1-100) <code>cursor</code> string No -- Pagination cursor <code>fields</code> string No -- Fields to return <p>Example Request:</p> <pre><code>curl \"http://localhost:8001/v1/search/packages?q=http+client&amp;limit=10\"\n</code></pre> <p>Response Fields:</p> Field Type Description <code>title</code> string Package name <code>url</code> string Package registry URL <code>content</code> string Package description <code>registry</code> string Registry: <code>pypi</code>, <code>npm</code>, <code>crates</code>, <code>docker</code>, <code>packagist</code>, <code>rubygems</code>, <code>nuget</code> <code>package_name</code> string Exact package name <code>version</code> string Latest version (if available)","tags":["Search"]},{"location":"api/search/#get-v1searchscience","title":"GET <code>/v1/search/science</code>","text":"<p>Search scientific databases and computational resources.</p> <p>Best For</p> <p>Scientific research, datasets, and computational answers.</p> <p>Difference from <code>/v1/search/scholar</code></p> <ul> <li><code>/scholar</code> focuses on academic papers and citations</li> <li><code>/science</code> includes computational answers, datasets, and broader scientific resources</li> </ul> <p>Engines: Wolfram Alpha, PubMed, arXiv, Semantic Scholar, Science.gov</p> <p>Parameters:</p> Name Type Required Default Description <code>q</code> string Yes -- Scientific search query <code>limit</code> integer No <code>20</code> Maximum results (1-100) <code>cursor</code> string No -- Pagination cursor <code>fields</code> string No -- Fields to return <p>Example Request:</p> <pre><code>curl \"http://localhost:8001/v1/search/science?q=protein+folding+dataset&amp;limit=10\"\n</code></pre> <p>Response Fields:</p> Field Type Description <code>title</code> string Resource title <code>url</code> string Resource URL <code>content</code> string Description or abstract <code>domain</code> string Source domain <code>source_type</code> string Type: <code>computation</code>, <code>biomedical</code>, <code>preprint</code>, <code>dataset</code>, <code>article</code> <code>authors</code> array Author list (if available) <code>published_date</code> string Publication date","tags":["Search"]},{"location":"api/search/#get-v1searchqa","title":"GET <code>/v1/search/qa</code>","text":"<p>Search Q&amp;A platforms for questions and answers.</p> <p>Best For</p> <p>Finding solutions to problems, how-to guides, and community knowledge.</p> <p>Engines: Stack Exchange network, Quora, Reddit Q&amp;A subreddits, Yahoo Answers archive</p> <p>Parameters:</p> Name Type Required Default Description <code>q</code> string Yes -- Question or problem description <code>limit</code> integer No <code>20</code> Maximum results (1-100) <code>cursor</code> string No -- Pagination cursor <code>fields</code> string No -- Fields to return <p>Example Request:</p> <pre><code>curl \"http://localhost:8001/v1/search/qa?q=how+to+fix+memory+leak+python&amp;limit=10\"\n</code></pre> <p>Response Fields:</p> Field Type Description <code>title</code> string Question title <code>url</code> string Question URL <code>content</code> string Question or top answer snippet <code>platform</code> string Platform: <code>stackoverflow</code>, <code>stackexchange</code>, <code>quora</code>, <code>reddit</code> <code>domain</code> string Source domain <code>has_accepted_answer</code> boolean Whether question has accepted answer <code>vote_count</code> integer Vote/score count","tags":["Search"]},{"location":"api/search/#get-v1searchtorrents","title":"GET <code>/v1/search/torrents</code>","text":"<p>Search for torrents across multiple trackers.</p> <p>Best For</p> <p>Finding Linux ISOs, open source software, public domain content.</p> <p>Engines: PirateBay, 1337x, Kickass, BTDigg, Nyaa (anime), TokyoToshokan, SolidTorrents, BT4G</p> <p>Note</p> <p>Results are sorted by seeder count (highest first). For legal content only.</p> <p>Parameters:</p> Name Type Required Default Description <code>q</code> string Yes -- Torrent search query <code>limit</code> integer No <code>20</code> Maximum results (1-100) <code>cursor</code> string No -- Pagination cursor <code>fields</code> string No -- Fields to return <p>Example Request:</p> <pre><code>curl \"http://localhost:8001/v1/search/torrents?q=ubuntu+22.04&amp;limit=10\"\n</code></pre> <p>Response Fields:</p> Field Type Description <code>title</code> string Torrent name <code>url</code> string Torrent page URL <code>magnet</code> string Magnet link for direct download <code>size</code> string File size (e.g., \"4.2 GB\") <code>seeders</code> string Number of seeders <code>leechers</code> string Number of leechers <code>source</code> string Tracker/engine name","tags":["Search"]},{"location":"api/search/#get-v1searchbooks","title":"GET <code>/v1/search/books</code>","text":"<p>Search for ebooks and digital books.</p> <p>Best For</p> <p>Finding textbooks, technical books, research papers, fiction.</p> <p>Engines: Anna's Archive (largest shadow library), Library Genesis (LibGen), Z-Library</p> <p>Parameters:</p> Name Type Required Default Description <code>q</code> string Yes -- Book title, author, or ISBN <code>limit</code> integer No <code>20</code> Maximum results (1-100) <code>cursor</code> string No -- Pagination cursor <code>fields</code> string No -- Fields to return <p>Example Request:</p> <pre><code>curl \"http://localhost:8001/v1/search/books?q=python+machine+learning&amp;limit=10\"\n</code></pre> <p>Response Fields:</p> Field Type Description <code>title</code> string Book title <code>url</code> string Download/info page URL <code>author</code> string/array Author name(s) <code>publisher</code> string Publisher name <code>year</code> string Publication year <code>format</code> string File format: <code>PDF</code>, <code>EPUB</code>, <code>MOBI</code>, etc. <code>filesize</code> string File size <code>language</code> string Book language <code>source</code> string Source library","tags":["Search"]},{"location":"api/search/#get-v1searchmovies","title":"GET <code>/v1/search/movies</code>","text":"<p>Search for movies and TV shows.</p> <p>Best For</p> <p>Finding movie information, ratings, cast details, and reviews.</p> <p>Engines: IMDB, Moviepilot, SensCritique, Rottentomatoes</p> <p>Parameters:</p> Name Type Required Default Description <code>q</code> string Yes -- Movie or TV show title <code>limit</code> integer No <code>20</code> Maximum results (1-100) <code>cursor</code> string No -- Pagination cursor <code>fields</code> string No -- Fields to return <p>Example Request:</p> <pre><code>curl \"http://localhost:8001/v1/search/movies?q=inception&amp;limit=10\"\n</code></pre> <p>Response Fields:</p> Field Type Description <code>title</code> string Movie/show title <code>url</code> string Info page URL <code>year</code> string Release year <code>rating</code> string Rating/score <code>genre</code> string Genre(s) <code>director</code> string Director name <code>cast</code> string Cast members <code>thumbnail</code> string Poster/thumbnail URL <code>source</code> string Database source (imdb, etc.)","tags":["Search"]},{"location":"api/search/#get-v1searchapps","title":"GET <code>/v1/search/apps</code>","text":"<p>Search for mobile and desktop applications.</p> <p>Best For</p> <p>Finding apps across all major app stores, including open source alternatives.</p> <p>Engines: Google Play Store (Android), Apple App Store (iOS), F-Droid (open source Android), APK Mirror</p> <p>Parameters:</p> Name Type Required Default Description <code>q</code> string Yes -- App name or category <code>limit</code> integer No <code>20</code> Maximum results (1-100) <code>cursor</code> string No -- Pagination cursor <code>fields</code> string No -- Fields to return <p>Example Request:</p> <pre><code>curl \"http://localhost:8001/v1/search/apps?q=password+manager&amp;limit=10\"\n</code></pre> <p>Response Fields:</p> Field Type Description <code>title</code> string App name <code>url</code> string App store URL <code>platform</code> string Platform: <code>android</code>, <code>ios</code>, <code>android (open source)</code> <code>developer</code> string Developer/publisher name <code>rating</code> string User rating <code>price</code> string Price (if available) <code>thumbnail</code> string App icon URL <code>source</code> string Store name","tags":["Search"]},{"location":"api/security/","title":"Security Testing","text":"<p>The Security API provides fuzzing, vulnerability scanning, and CVE intelligence tools for authorized security assessments.</p> <p>Authorization Required</p> <p>Only use these endpoints against systems you own or have explicit written permission to test. Unauthorized security testing is illegal in most jurisdictions.</p>","tags":["Security"]},{"location":"api/security/#directory-fuzzing","title":"Directory Fuzzing","text":"<p>Discover hidden directories, files, and paths on a target web server using wordlist-based fuzzing.</p>","tags":["Security"]},{"location":"api/security/#list-wordlists","title":"List Wordlists","text":"<p><code>GET /v1/security/fuzz/wordlists</code></p> <p>List available built-in wordlists for directory and file fuzzing.</p> <pre><code>curl http://localhost:8001/v1/security/fuzz/wordlists\n</code></pre> Response <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"wordlists\": [\n      {\"name\": \"common\", \"description\": \"Common directories and files\", \"count\": 50, \"category\": \"directories\"},\n      {\"name\": \"dirs\", \"description\": \"Directory names only\", \"count\": 35, \"category\": \"directories\"},\n      {\"name\": \"files\", \"description\": \"Common file names\", \"count\": 40, \"category\": \"files\"},\n      {\"name\": \"api\", \"description\": \"Common API endpoints\", \"count\": 30, \"category\": \"api\"}\n    ]\n  }\n}\n</code></pre> <p>Available Wordlists:</p> Name Category Count Description <code>common</code> directories 50 Common directories and files <code>dirs</code> directories 35 Directory names only <code>files</code> files 40 Common file names <code>api</code> api 30 Common API endpoints","tags":["Security"]},{"location":"api/security/#list-payloads","title":"List Payloads","text":"<p><code>GET /v1/security/fuzz/payloads</code></p> <p>List available payload sets for parameter fuzzing.</p> <pre><code>curl http://localhost:8001/v1/security/fuzz/payloads\n</code></pre> Response <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"payloads\": [\n      {\"name\": \"xss\", \"description\": \"XSS test payloads\", \"count\": 10, \"category\": \"xss\"},\n      {\"name\": \"sqli\", \"description\": \"SQL injection payloads\", \"count\": 15, \"category\": \"sqli\"},\n      {\"name\": \"lfi\", \"description\": \"LFI test payloads\", \"count\": 10, \"category\": \"lfi\"},\n      {\"name\": \"traversal\", \"description\": \"Path traversal payloads\", \"count\": 10, \"category\": \"traversal\"}\n    ]\n  }\n}\n</code></pre> <p>Available Payload Sets:</p> Name Category Count Description <code>xss</code> xss 10 XSS test payloads <code>sqli</code> sqli 15 SQL injection payloads <code>lfi</code> lfi 10 LFI test payloads <code>traversal</code> traversal 10 Path traversal payloads","tags":["Security"]},{"location":"api/security/#fuzz-directories","title":"Fuzz Directories","text":"<p><code>POST /v1/security/fuzz/directories</code></p> <p>Perform wfuzz-style directory and path discovery on a target.</p> <p>Parameters:</p> Name Type Default Description <code>target</code> string -- Target base URL (required) <code>wordlist</code> string -- Built-in wordlist name <code>paths</code> array -- Custom paths to test <code>extensions</code> array -- File extensions to append (e.g., <code>.php</code>, <code>.bak</code>) <code>limit</code> integer <code>100</code> Maximum requests <code>rate_limit</code> float <code>10</code> Requests per second <code>timeout</code> float <code>10</code> Per-request timeout in seconds With WordlistWith Custom Paths <pre><code>curl -X POST http://localhost:8001/v1/security/fuzz/directories \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"target\": \"http://example.com\",\n    \"wordlist\": \"common\",\n    \"extensions\": [\".php\", \".bak\"],\n    \"limit\": 100,\n    \"rate_limit\": 10\n  }'\n</code></pre> <pre><code>curl -X POST http://localhost:8001/v1/security/fuzz/directories \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"target\": \"http://example.com\",\n    \"paths\": [\"admin\", \"backup\", \"config\", \".git\", \".env\"],\n    \"extensions\": [\".php\", \".bak\", \".old\"]\n  }'\n</code></pre> Response <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"target\": \"http://example.com\",\n    \"results\": [\n      {\"url\": \"http://example.com/admin\", \"status_code\": 200, \"content_length\": 1234}\n    ],\n    \"found\": 1,\n    \"tested\": 50,\n    \"elapsed_ms\": 5200\n  }\n}\n</code></pre>","tags":["Security"]},{"location":"api/security/#parameter-fuzzing","title":"Parameter Fuzzing","text":"","tags":["Security"]},{"location":"api/security/#fuzz-parameters","title":"Fuzz Parameters","text":"<p><code>POST /v1/security/fuzz/params</code></p> <p>Fuzz URL or form parameters with payload sets to detect injection vulnerabilities.</p> <p>Request Body:</p> Name Type Description <code>target</code> string Target URL (required) <code>method</code> string HTTP method: <code>GET</code> or <code>POST</code> <code>params</code> object Parameters with <code>FUZZ</code> placeholder <code>payloads</code> string Payload set name (e.g., <code>xss</code>, <code>sqli</code>) <code>limit</code> integer Maximum requests XSS FuzzingSQLi Fuzzing <pre><code>curl -X POST http://localhost:8001/v1/security/fuzz/params \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"target\": \"http://example.com/search\",\n    \"method\": \"GET\",\n    \"params\": {\"q\": \"FUZZ\"},\n    \"payloads\": \"xss\",\n    \"limit\": 50\n  }'\n</code></pre> <pre><code>curl -X POST http://localhost:8001/v1/security/fuzz/params \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"target\": \"http://example.com/search\",\n    \"method\": \"GET\",\n    \"params\": {\"q\": \"FUZZ\"},\n    \"payloads\": \"sqli\",\n    \"limit\": 50\n  }'\n</code></pre>","tags":["Security"]},{"location":"api/security/#vulnerability-scanning","title":"Vulnerability Scanning","text":"<p>Automated vulnerability detection covering security headers, XSS, SQL injection, and local file inclusion.</p>","tags":["Security"]},{"location":"api/security/#scan-security-headers","title":"Scan Security Headers","text":"<p><code>GET /v1/security/scan/headers</code></p> <p>Analyze HTTP security headers for a URL and produce a security grade.</p> <p>Parameters:</p> Name Type Description <code>url</code> string URL to scan (required) <pre><code>curl \"http://localhost:8001/v1/security/scan/headers?url=https://example.com\"\n</code></pre> Response <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"url\": \"https://example.com\",\n    \"headers\": {\n      \"Strict-Transport-Security\": {\"value\": \"max-age=31536000\", \"present\": true}\n    },\n    \"missing\": [\"Content-Security-Policy\", \"X-Frame-Options\"],\n    \"issues\": [\"Missing critical header: Content-Security-Policy\"],\n    \"score\": 65,\n    \"grade\": \"C\"\n  }\n}\n</code></pre>","tags":["Security"]},{"location":"api/security/#scan-for-xss","title":"Scan for XSS","text":"<p><code>POST /v1/security/scan/xss</code></p> <p>Scan a URL for Cross-Site Scripting vulnerabilities.</p> <p>Request Body:</p> Field Type Description <code>url</code> string Target URL (required) <code>params</code> object URL parameters to test <code>depth</code> integer Scan depth level <pre><code>curl -X POST http://localhost:8001/v1/security/scan/xss \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\": \"http://example.com/search\", \"params\": {\"q\": \"test\"}, \"depth\": 1}'\n</code></pre>","tags":["Security"]},{"location":"api/security/#scan-for-sql-injection","title":"Scan for SQL Injection","text":"<p><code>POST /v1/security/scan/sqli</code></p> <p>Scan a URL for SQL Injection vulnerabilities.</p> <p>Request Body:</p> Field Type Description <code>url</code> string Target URL (required) <code>params</code> object URL parameters to test <code>depth</code> integer Scan depth level <pre><code>curl -X POST http://localhost:8001/v1/security/scan/sqli \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\": \"http://example.com/item\", \"params\": {\"id\": \"1\"}, \"depth\": 1}'\n</code></pre>","tags":["Security"]},{"location":"api/security/#scan-for-lfi","title":"Scan for LFI","text":"<p><code>POST /v1/security/scan/lfi</code></p> <p>Scan a URL for Local File Inclusion vulnerabilities.</p> <p>Request Body:</p> Field Type Description <code>url</code> string Target URL (required) <code>params</code> object URL parameters to test <pre><code>curl -X POST http://localhost:8001/v1/security/scan/lfi \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\": \"http://example.com/page\", \"params\": {\"file\": \"index.php\"}}'\n</code></pre>","tags":["Security"]},{"location":"api/security/#full-vulnerability-scan","title":"Full Vulnerability Scan","text":"<p><code>POST /v1/security/scan</code></p> <p>Run a comprehensive vulnerability scan combining all available checks.</p> <p>Request Body:</p> Field Type Default Description <code>url</code> string -- Target URL (required) <code>params</code> object -- Parameters to test <code>checks</code> array All checks Checks to run: <code>headers</code>, <code>xss</code>, <code>sqli</code>, <code>lfi</code> <code>depth</code> integer <code>1</code> Scan depth level Full ScanHeaders Only <pre><code>curl -X POST http://localhost:8001/v1/security/scan \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"url\": \"http://example.com\",\n    \"params\": {\"q\": \"test\"},\n    \"checks\": [\"headers\", \"xss\", \"sqli\", \"lfi\"],\n    \"depth\": 1\n  }'\n</code></pre> <pre><code>curl -X POST http://localhost:8001/v1/security/scan \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"url\": \"http://example.com\",\n    \"checks\": [\"headers\"]\n  }'\n</code></pre> Response <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"url\": \"http://example.com\",\n    \"vulnerable\": true,\n    \"summary\": {\"high\": 1, \"medium\": 2, \"info\": 3},\n    \"findings\": [\n      {\n        \"type\": \"xss\",\n        \"severity\": \"high\",\n        \"url\": \"http://example.com/search?q=&lt;script&gt;\",\n        \"parameter\": \"q\",\n        \"description\": \"Potential XSS - payload reflected\",\n        \"remediation\": \"Sanitize and encode user input\"\n      }\n    ],\n    \"checks_performed\": [\"headers\", \"xss\", \"sqli\", \"lfi\"],\n    \"tested\": 45\n  }\n}\n</code></pre> <p>Severity Levels</p> <p>Findings are categorized by severity:</p> Severity Description high Exploitable vulnerability requiring immediate attention medium Potential vulnerability or configuration weakness info Informational finding or best practice recommendation","tags":["Security"]},{"location":"api/security/#port-scanning-cve-intelligence","title":"Port Scanning &amp; CVE Intelligence","text":"<p>Network-level vulnerability assessment with port scanning and CVE database lookup.</p>","tags":["Security"]},{"location":"api/security/#check-availability","title":"Check Availability","text":"<p><code>GET /v1/vuln/check</code></p> <p>Check if the vulnerability scanning subsystem is available and operational.</p> <pre><code>curl http://localhost:8001/v1/vuln/check\n</code></pre>","tags":["Security"]},{"location":"api/security/#scan-target","title":"Scan Target","text":"<p><code>POST /v1/vuln/scan</code></p> <p>Perform a vulnerability scan on a target host, checking specified ports for known vulnerabilities.</p> <p>Request Body:</p> Field Type Description <code>target</code> string Target hostname or IP (required) <code>ports</code> array List of specific ports to scan <code>timeout</code> integer Scan timeout in seconds <pre><code>curl -X POST http://localhost:8001/v1/vuln/scan \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"target\": \"example.com\", \"ports\": [80, 443, 8080], \"timeout\": 30}'\n</code></pre>","tags":["Security"]},{"location":"api/security/#scan-ports","title":"Scan Ports","text":"<p><code>POST /v1/vuln/ports</code></p> <p>Perform a port scan on a target to discover open services.</p> <p>Request Body:</p> Field Type Description <code>target</code> string Target hostname or IP (required) <code>ports</code> string Port range (e.g., <code>1-1000</code>, <code>80,443,8080</code>) <pre><code>curl -X POST http://localhost:8001/v1/vuln/ports \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"target\": \"example.com\", \"ports\": \"1-1000\"}'\n</code></pre>","tags":["Security"]},{"location":"api/security/#search-cve-database","title":"Search CVE Database","text":"<p><code>GET /v1/vuln/cve/search</code></p> <p>Search the CVE (Common Vulnerabilities and Exposures) database.</p> <p>Parameters:</p> Name Type Description <code>query</code> string Free-text search query <code>product</code> string Filter by product name <code>severity</code> string Filter by severity: <code>CRITICAL</code>, <code>HIGH</code>, <code>MEDIUM</code>, <code>LOW</code> <code>limit</code> integer Maximum results to return By ProductBy Query <pre><code>curl \"http://localhost:8001/v1/vuln/cve/search?product=apache&amp;severity=CRITICAL&amp;limit=10\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/vuln/cve/search?query=log4j&amp;severity=CRITICAL\"\n</code></pre>","tags":["Security"]},{"location":"api/security/#get-cve-details","title":"Get CVE Details","text":"<p><code>GET /v1/vuln/cve/{cve_id}</code></p> <p>Retrieve detailed information about a specific CVE entry.</p> <pre><code>curl http://localhost:8001/v1/vuln/cve/CVE-2021-44228\n</code></pre>","tags":["Security"]},{"location":"api/spider-generator/","title":"Spider Generator","text":"<p>Generate production-ready Scrapy spider code from target URLs using TDD methodology. The generator analyses page structure, detects APIs, selects the optimal template, and produces spider code with matching test suites.</p>","tags":["Automation"]},{"location":"api/spider-generator/#overview","title":"Overview","text":"<p>The Spider Generator automates the tedious parts of building web scrapers. Point it at a URL and it will:</p> <ol> <li>Analyse the page structure, detect APIs, and determine rendering requirements.</li> <li>Select the optimal spider template (API, table, PDF, or Camoufox browser).</li> <li>Generate production-ready Scrapy spider code with a matching pytest test suite.</li> <li>Validate the generated code for syntax errors and structural issues.</li> <li>Test the spider in a sandboxed environment.</li> </ol>","tags":["Automation"]},{"location":"api/spider-generator/#key-features","title":"Key Features","text":"Feature Description Automatic site analysis Structure detection, API probing, rendering requirements Multiple templates API spider, table spider, PDF spider, Camoufox browser spider Strategy auto-detection Picks the best template based on page analysis Code validation Syntax, imports, and class structure checks Sandboxed testing Runs pytest against generated code in isolation","tags":["Automation"]},{"location":"api/spider-generator/#endpoints","title":"Endpoints","text":"","tags":["Automation"]},{"location":"api/spider-generator/#post-v1spidergengenerate-generate-spider","title":"POST <code>/v1/spidergen/generate</code> -- Generate Spider","text":"<p>Generate a Scrapy spider for a target URL.</p>","tags":["Automation"]},{"location":"api/spider-generator/#request-body","title":"Request Body","text":"<pre><code>{\n  \"state_name\": \"california\",\n  \"target_url\": \"https://oag.ca.gov/privacy/databreach/list\",\n  \"config\": {\n    \"use_camoufox\": \"auto\",\n    \"spider_type\": \"auto\",\n    \"custom_settings\": null\n  }\n}\n</code></pre> Parameter Type Required Description <code>state_name</code> string yes Identifier for the spider (used in class naming) <code>target_url</code> string yes URL to generate a spider for <code>config</code> object no Generation configuration (see below)","tags":["Automation"]},{"location":"api/spider-generator/#config-options","title":"Config Options","text":"Parameter Values Description <code>use_camoufox</code> <code>auto</code>, <code>always</code>, <code>never</code> Browser rendering mode <code>spider_type</code> <code>auto</code>, <code>api</code>, <code>table</code>, <code>pdf</code> Template selection <code>custom_settings</code> object Custom Scrapy settings overrides","tags":["Automation"]},{"location":"api/spider-generator/#example","title":"Example","text":"curlPython <pre><code>curl -X POST http://localhost:8001/v1/spidergen/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"state_name\": \"california\",\n    \"target_url\": \"https://oag.ca.gov/privacy/databreach/list\",\n    \"config\": {\"use_camoufox\": \"auto\", \"spider_type\": \"auto\"}\n  }'\n</code></pre> <pre><code>import requests\n\nresp = requests.post(\"http://localhost:8001/v1/spidergen/generate\", json={\n    \"state_name\": \"california\",\n    \"target_url\": \"https://oag.ca.gov/privacy/databreach/list\",\n    \"config\": {\"use_camoufox\": \"auto\", \"spider_type\": \"auto\"},\n})\ndata = resp.json()[\"data\"]\nprint(f\"Template: {data['template_used']}\")\nprint(f\"Needs browser: {data['analysis']['needs_browser']}\")\n# Save spider code\nwith open(\"california_spider.py\", \"w\") as f:\n    f.write(data[\"spider_code\"])\n</code></pre>","tags":["Automation"]},{"location":"api/spider-generator/#response","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"spider_code\": \"import scrapy\\n...\",\n    \"test_code\": \"import pytest\\n...\",\n    \"analysis\": {\n      \"url\": \"https://oag.ca.gov/privacy/databreach/list\",\n      \"detected_type\": \"table\",\n      \"needs_browser\": true,\n      \"apis_found\": []\n    },\n    \"template_used\": \"camoufox_spider\"\n  }\n}\n</code></pre>","tags":["Automation"]},{"location":"api/spider-generator/#post-v1spidergengenerate-quick-quick-generate","title":"POST <code>/v1/spidergen/generate-quick</code> -- Quick Generate","text":"<p>Quick spider generation without the full TDD cycle. Faster but may produce less tested code. Use <code>/generate</code> for production-quality spiders.</p> <p>If <code>config.spider_type</code> is specified (not \"auto\"), uses that template directly without analysis.</p>","tags":["Automation"]},{"location":"api/spider-generator/#request-body_1","title":"Request Body","text":"<p>Same as <code>/v1/spidergen/generate</code>.</p>","tags":["Automation"]},{"location":"api/spider-generator/#example_1","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/spidergen/generate-quick \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"state_name\": \"texas\",\n    \"target_url\": \"https://example.com/data\",\n    \"config\": {\"spider_type\": \"table\"}\n  }'\n</code></pre>","tags":["Automation"]},{"location":"api/spider-generator/#get-v1spidergenhealth-health-check","title":"GET <code>/v1/spidergen/health</code> -- Health Check","text":"<p>Check spider generator service health.</p> <pre><code>curl http://localhost:8001/v1/spidergen/health\n</code></pre>","tags":["Automation"]},{"location":"api/spider-generator/#post-v1spidergenanalyze-analyse-url","title":"POST <code>/v1/spidergen/analyze</code> -- Analyse URL","text":"<p>Analyse a URL to determine the optimal scraping strategy without generating code.</p>","tags":["Automation"]},{"location":"api/spider-generator/#request-body_2","title":"Request Body","text":"<pre><code>{\n  \"url\": \"https://example.com/data\",\n  \"discover_apis\": true\n}\n</code></pre> Parameter Type Required Description <code>url</code> string yes Target URL to analyse <code>discover_apis</code> bool no Probe for API endpoints (default: <code>true</code>)","tags":["Automation"]},{"location":"api/spider-generator/#example_2","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/spidergen/analyze \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\": \"https://example.com/data\", \"discover_apis\": true}'\n</code></pre>","tags":["Automation"]},{"location":"api/spider-generator/#response_1","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"url\": \"https://example.com/data\",\n    \"title\": \"Data Portal\",\n    \"detected_type\": \"api\",\n    \"needs_browser\": false,\n    \"apis_found\": [\n      {\n        \"url\": \"https://api.example.com/v1/records\",\n        \"method\": \"GET\",\n        \"content_type\": \"application/json\"\n      }\n    ],\n    \"page_structure\": {\n      \"tables\": 0,\n      \"links\": 45,\n      \"forms\": 1\n    }\n  }\n}\n</code></pre>","tags":["Automation"]},{"location":"api/spider-generator/#get-v1spidergentemplates-list-templates","title":"GET <code>/v1/spidergen/templates</code> -- List Templates","text":"<p>List all available spider templates with descriptions and usage guidance.</p>","tags":["Automation"]},{"location":"api/spider-generator/#example_3","title":"Example","text":"<pre><code>curl http://localhost:8001/v1/spidergen/templates\n</code></pre>","tags":["Automation"]},{"location":"api/spider-generator/#response_2","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"templates\": [\n      {\n        \"name\": \"api_spider\",\n        \"description\": \"Spider for JSON/REST API endpoints\",\n        \"use_when\": \"Target exposes a structured API\"\n      },\n      {\n        \"name\": \"table_spider\",\n        \"description\": \"Spider for HTML table extraction\",\n        \"use_when\": \"Data is in HTML tables with pagination\"\n      },\n      {\n        \"name\": \"pdf_spider\",\n        \"description\": \"Spider for PDF document extraction\",\n        \"use_when\": \"Data is in downloadable PDF files\"\n      },\n      {\n        \"name\": \"camoufox_spider\",\n        \"description\": \"Browser-based spider using Camoufox anti-detect Firefox\",\n        \"use_when\": \"Site requires JavaScript rendering or has bot detection\"\n      }\n    ]\n  }\n}\n</code></pre>","tags":["Automation"]},{"location":"api/spider-generator/#template-selection-guide","title":"Template Selection Guide","text":"Template Best For <code>api_spider</code> Sites with a JSON / REST API backend <code>table_spider</code> Pages with HTML <code>&lt;table&gt;</code> elements and server-side pagination <code>pdf_spider</code> Portals that serve data as downloadable PDF documents <code>camoufox_spider</code> JavaScript-heavy sites or sites with bot detection (Cloudflare, DataDome)","tags":["Automation"]},{"location":"api/spider-generator/#post-v1spidergenvalidate-validate-code","title":"POST <code>/v1/spidergen/validate</code> -- Validate Code","text":"<p>Validate generated spider code for syntax errors and structural issues.</p>","tags":["Automation"]},{"location":"api/spider-generator/#request-body_3","title":"Request Body","text":"<pre><code>{\n  \"code\": \"import scrapy\\n\\nclass MySpider(scrapy.Spider):\\n    name = 'test'\\n    ...\"\n}\n</code></pre>","tags":["Automation"]},{"location":"api/spider-generator/#example_4","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/spidergen/validate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"code\": \"import scrapy\\n\\nclass MySpider(scrapy.Spider):\\n    name = \\\"test\\\"\\n    start_urls = [\\\"https://example.com\\\"]\"}'\n</code></pre>","tags":["Automation"]},{"location":"api/spider-generator/#response_3","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"valid\": true,\n    \"errors\": [],\n    \"warnings\": [\"Missing custom_settings for download delay\"]\n  }\n}\n</code></pre>","tags":["Automation"]},{"location":"api/spider-generator/#post-v1spidergentest-run-tests","title":"POST <code>/v1/spidergen/test</code> -- Run Tests","text":"<p>Run tests against generated spider code in a sandboxed environment.</p>","tags":["Automation"]},{"location":"api/spider-generator/#request-body_4","title":"Request Body","text":"<pre><code>{\n  \"spider_code\": \"import scrapy\\n...\",\n  \"test_code\": \"import pytest\\n...\",\n  \"timeout\": 30\n}\n</code></pre> Parameter Type Required Description <code>spider_code</code> string yes Generated spider source code <code>test_code</code> string yes Generated pytest test code <code>timeout</code> int no Test execution timeout in seconds (default: 30)","tags":["Automation"]},{"location":"api/spider-generator/#example_5","title":"Example","text":"<pre><code>curl -X POST http://localhost:8001/v1/spidergen/test \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"spider_code\": \"import scrapy\\n...\",\n    \"test_code\": \"import pytest\\n...\",\n    \"timeout\": 30\n  }'\n</code></pre>","tags":["Automation"]},{"location":"api/spider-generator/#response_4","title":"Response","text":"<pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"passed\": true,\n    \"tests_run\": 5,\n    \"tests_passed\": 5,\n    \"tests_failed\": 0,\n    \"output\": \"===== 5 passed in 2.3s =====\"\n  }\n}\n</code></pre>","tags":["Automation"]},{"location":"api/streaming/","title":"Streaming Endpoints","text":"<p>The Streaming API uses Server-Sent Events (SSE) to deliver search results in real time as they arrive from upstream engines. Instead of waiting for all engines to respond before returning a single JSON payload, SSE streams individual results the moment they are available -- reducing perceived latency from seconds to milliseconds.</p>","tags":["Search"]},{"location":"api/streaming/#sse-overview","title":"SSE Overview","text":"<p>What are Server-Sent Events?</p> <p>SSE is a standard HTTP protocol (part of the HTML5 spec) that allows a server to push events to the client over a single, long-lived HTTP connection. Unlike WebSockets, SSE is unidirectional (server to client), uses plain HTTP, and works through proxies and firewalls without special configuration.</p>","tags":["Search"]},{"location":"api/streaming/#how-it-works","title":"How It Works","text":"<ol> <li>The client opens a <code>GET</code> request with <code>Accept: text/event-stream</code></li> <li>The server holds the connection open and sends events as they become available</li> <li>Each event has a type (<code>event:</code>) and payload (<code>data:</code>)</li> <li>The client closes the connection when the <code>complete</code> event arrives</li> </ol>","tags":["Search"]},{"location":"api/streaming/#sse-event-format","title":"SSE Event Format","text":"<p>Each event in the stream follows this structure:</p> <pre><code>event: &lt;event_type&gt;\ndata: &lt;json_payload&gt;\n</code></pre> <p>Events are separated by a blank line. The <code>data:</code> field contains a JSON object that varies by event type.</p>","tags":["Search"]},{"location":"api/streaming/#endpoints","title":"Endpoints","text":"","tags":["Search"]},{"location":"api/streaming/#get-v1streamsearch","title":"GET <code>/v1/stream/search</code>","text":"<p>Server-Sent Events stream for real-time search results.</p> <p>Best For</p> <p>Building responsive UIs that display results as they arrive.</p> <p>Target Audience</p> <ul> <li>Frontend developers -- Building real-time search interfaces</li> <li>UX designers -- Creating progressive loading experiences</li> <li>Performance-focused teams -- Reducing perceived latency</li> <li>Dashboard builders -- Live updating search widgets</li> </ul> <p>Why Use This:</p> <ul> <li>Results appear immediately as they are fetched (no waiting for all engines)</li> <li>Reduces perceived latency -- users see first result in ~500ms vs 2-3s</li> <li>Perfect for \"infinite scroll\" or progressive loading UIs</li> <li>Includes completion event with total count and timing</li> <li>Standard SSE protocol works with all modern browsers</li> </ul> <p>Parameters:</p> Name Type Required Default Description <code>q</code> string Yes -- Search query <code>engines</code> string No <code>google,brave,duckduckgo</code> Comma-separated engines <code>limit</code> integer No <code>10</code> Maximum results (1-50)","tags":["Search"]},{"location":"api/streaming/#event-types","title":"Event Types","text":"Event Description <code>start</code> Stream started -- contains query info and request ID <code>result</code> Individual search result object <code>error</code> Error occurred during search <code>complete</code> Stream finished -- contains summary with total count and timing","tags":["Search"]},{"location":"api/streaming/#example-request","title":"Example Request","text":"<pre><code>curl -N \"http://localhost:8001/v1/stream/search?q=python&amp;limit=5\"\n</code></pre>","tags":["Search"]},{"location":"api/streaming/#example-sse-response","title":"Example SSE Response","text":"<pre><code>event: start\ndata: {\"query\": \"python\", \"request_id\": \"uuid\", \"engines\": [\"google\"], \"limit\": 5}\n\nevent: result\ndata: {\"position\": 1, \"title\": \"Python.org\", \"url\": \"https://python.org\", ...}\n\nevent: result\ndata: {\"position\": 2, \"title\": \"Python Tutorial\", \"url\": \"https://...\", ...}\n\nevent: complete\ndata: {\"total\": 5, \"engines_used\": [\"google\", \"brave\"], \"processing_time_ms\": 2500}\n</code></pre>","tags":["Search"]},{"location":"api/streaming/#client-examples","title":"Client Examples","text":"JavaScript (EventSource)React ComponentPython (sseclient)curl (raw stream) <pre><code>const eventSource = new EventSource(\n  'http://localhost:8001/v1/stream/search?q=python&amp;limit=10'\n);\n\neventSource.addEventListener('result', (e) =&gt; {\n  const result = JSON.parse(e.data);\n  console.log('New result:', result.title);\n});\n\neventSource.addEventListener('complete', (e) =&gt; {\n  const summary = JSON.parse(e.data);\n  console.log(`Done: ${summary.total} results in ${summary.processing_time_ms}ms`);\n  eventSource.close();\n});\n\neventSource.addEventListener('error', (e) =&gt; {\n  console.error('Stream error');\n  eventSource.close();\n});\n</code></pre> <pre><code>import { useEffect, useState } from 'react';\n\nfunction LiveSearch({ query }) {\n  const [results, setResults] = useState([]);\n  const [status, setStatus] = useState('idle');\n\n  useEffect(() =&gt; {\n    if (!query) return;\n\n    setResults([]);\n    setStatus('searching');\n\n    const eventSource = new EventSource(\n      `http://localhost:8001/v1/stream/search?q=${encodeURIComponent(query)}&amp;limit=20`\n    );\n\n    eventSource.addEventListener('result', (e) =&gt; {\n      const result = JSON.parse(e.data);\n      setResults(prev =&gt; [...prev, result]);\n    });\n\n    eventSource.addEventListener('complete', (e) =&gt; {\n      setStatus('complete');\n      eventSource.close();\n    });\n\n    eventSource.addEventListener('error', (e) =&gt; {\n      setStatus('error');\n      eventSource.close();\n    });\n\n    return () =&gt; eventSource.close();\n  }, [query]);\n\n  return (\n    &lt;div&gt;\n      {results.map((r, i) =&gt; &lt;SearchResult key={i} {...r} /&gt;)}\n      {status === 'searching' &amp;&amp; &lt;Spinner /&gt;}\n    &lt;/div&gt;\n  );\n}\n</code></pre> <pre><code>import requests\nimport sseclient\nimport json\n\nurl = 'http://localhost:8001/v1/stream/search?q=python+programming&amp;limit=10'\nresponse = requests.get(url, stream=True)\nclient = sseclient.SSEClient(response)\n\nfor event in client.events():\n    if event.event == 'result':\n        result = json.loads(event.data)\n        print(f\"[{result['position']}] {result['title']}\")\n        print(f\"    {result['url']}\")\n    elif event.event == 'complete':\n        summary = json.loads(event.data)\n        print(f\"\\nDone: {summary['total']} results in {summary['processing_time_ms']}ms\")\n        break\n    elif event.event == 'error':\n        print(f\"Error: {event.data}\")\n        break\n</code></pre> <pre><code># Stream results to terminal\ncurl -N \"http://localhost:8001/v1/stream/search?q=python+programming&amp;limit=10\"\n\n# Watch results arrive in real-time\ncurl -N \"http://localhost:8001/v1/stream/search?q=breaking+news\" | while read line; do\n  echo \"$line\"\ndone\n</code></pre>","tags":["Search"]},{"location":"api/streaming/#get-v1streammulti-search","title":"GET <code>/v1/stream/multi-search</code>","text":"<p>Stream results from multiple search queries in parallel using SSE.</p> <p>Best For</p> <p>Running multiple related searches simultaneously and streaming all results.</p> <p>Parameters:</p> Name Type Required Default Description <code>q</code> string Yes -- Comma-separated search queries <code>engines</code> string No <code>google,brave,duckduckgo</code> Search engines <code>limit</code> integer No <code>10</code> Results per query (1-50)","tags":["Search"]},{"location":"api/streaming/#event-types_1","title":"Event Types","text":"Event Description <code>query_start</code> Indicates a new query is starting (includes query text) <code>result</code> Individual search result (includes query identifier) <code>query_complete</code> A single query finished with per-query stats <code>complete</code> All queries finished","tags":["Search"]},{"location":"api/streaming/#example-request_1","title":"Example Request","text":"<pre><code># Search for multiple topics simultaneously\ncurl -N \"http://localhost:8001/v1/stream/multi-search?q=python+tutorial,javascript+basics,rust+programming&amp;limit=5\"\n</code></pre>","tags":["Search"]},{"location":"api/streaming/#use-cases","title":"Use Cases","text":"<p>Use Cases</p> <ul> <li>Comparison dashboards -- Compare results across multiple search terms side by side</li> <li>Multi-topic research -- Research several related topics in a single request</li> <li>A/B query testing -- Compare how different phrasings return different results</li> </ul>","tags":["Search"]},{"location":"api/telegram/","title":"Telegram Intel","text":"<p>Telegram threat intelligence channel tracking and OSINT capabilities. Monitor ransomware groups, data leak marketplaces, hacktivist channels, stealer log sellers, and other threat actors operating on Telegram.</p> <p>Overview</p> <p>The Telegram Intel module provides a curated database of threat-related Telegram channels with categorization, risk scoring, and threat actor attribution. Sync from external sources like ransomware.live to stay current.</p>","tags":["OSINT","Dark Web"]},{"location":"api/telegram/#quick-filters","title":"Quick Filters","text":"<p>Specialized endpoints for the most common threat categories:</p> Endpoint Description <code>GET /v1/telegram/critical</code> All critical-risk channels (highest priority) <code>GET /v1/telegram/ransomware</code> Ransomware operator channels <code>GET /v1/telegram/data-leaks</code> Data breach marketplaces <code>GET /v1/telegram/stealer-logs</code> Infostealer credential channels <code>GET /v1/telegram/hacktivism</code> Hacktivist group channels","tags":["OSINT","Dark Web"]},{"location":"api/telegram/#channel-categories","title":"Channel Categories","text":"","tags":["OSINT","Dark Web"]},{"location":"api/telegram/#get-v1telegramcategories","title":"GET /v1/telegram/categories","text":"<p>List all channel categories with counts.</p> <p>Categories include:</p> Category Description <code>ransomware</code> Ransomware operator channels <code>data_leaks</code> Data breach marketplaces <code>hacking_tools</code> Hacking tools and utilities <code>malware</code> Malware samples and development <code>stealer_logs</code> Infostealer logs (RedLine, Raccoon, Vidar, etc.) <code>initial_access</code> Initial access brokers <code>apt_groups</code> APT group channels <code>hacktivism</code> Hacktivist groups <pre><code>curl http://localhost:8001/v1/telegram/categories\n</code></pre>","tags":["OSINT","Dark Web"]},{"location":"api/telegram/#channel-management","title":"Channel Management","text":"","tags":["OSINT","Dark Web"]},{"location":"api/telegram/#get-v1telegramchannels","title":"GET /v1/telegram/channels","text":"<p>List all tracked Telegram channels. Returns channels sorted by risk level (critical first).</p> <p>Parameters:</p> Name Type Default Description <code>category</code> string - Filter by category (ransomware, data_leaks, etc.) <code>risk_level</code> string - Filter: <code>low</code>, <code>medium</code>, <code>high</code>, <code>critical</code> <code>tags</code> string - Filter by tags <code>search</code> string - Search within channel data <code>threat_actor</code> string - Filter by associated threat actor <code>limit</code> integer 100 Maximum results <code>offset</code> integer 0 Pagination offset curlPython <pre><code># List all ransomware channels\ncurl \"http://localhost:8001/v1/telegram/channels?category=ransomware\"\n\n# List critical-risk channels\ncurl \"http://localhost:8001/v1/telegram/channels?risk_level=critical&amp;limit=20\"\n\n# Filter by threat actor\ncurl \"http://localhost:8001/v1/telegram/channels?threat_actor=LockBit\"\n</code></pre> <pre><code>import requests\n\n# List high-risk data leak channels\nresponse = requests.get(\"http://localhost:8001/v1/telegram/channels\", params={\n    \"category\": \"data_leaks\",\n    \"risk_level\": \"high\",\n    \"limit\": 50\n})\nchannels = response.json()[\"data\"][\"channels\"]\nfor ch in channels:\n    print(f\"@{ch['username']} - {ch['title']} [{ch['risk_level']}]\")\n</code></pre>","tags":["OSINT","Dark Web"]},{"location":"api/telegram/#post-v1telegramchannels","title":"POST /v1/telegram/channels","text":"<p>Add a new channel to track.</p> <p>Request Body:</p> Field Type Required Description <code>username</code> string Yes Telegram username (without @) <code>title</code> string Yes Display name <code>category</code> string No Channel category <code>threat_actor</code> string No Associated threat actor/group <code>risk_level</code> string No <code>low</code>, <code>medium</code>, <code>high</code>, <code>critical</code> <code>tags</code> array No List of tags <pre><code>curl -X POST http://localhost:8001/v1/telegram/channels \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"username\": \"example_channel\",\n    \"title\": \"Example Threat Channel\",\n    \"category\": \"ransomware\",\n    \"threat_actor\": \"ExampleGroup\",\n    \"risk_level\": \"high\",\n    \"tags\": [\"ransomware\", \"RaaS\"]\n  }'\n</code></pre>","tags":["OSINT","Dark Web"]},{"location":"api/telegram/#get-v1telegramchannelssearch","title":"GET /v1/telegram/channels/search","text":"<p>Full-text search across all channel fields (username, title, description, threat actor, tags).</p> <p>Parameters:</p> Name Type Required Default Description <code>q</code> string Yes - Search query <code>limit</code> integer No 50 Maximum results <pre><code>curl \"http://localhost:8001/v1/telegram/channels/search?q=lockbit&amp;limit=20\"\n</code></pre>","tags":["OSINT","Dark Web"]},{"location":"api/telegram/#get-v1telegramchannelsusername","title":"GET /v1/telegram/channels/{username}","text":"<p>Get full details for a specific channel including category, tags, threat actor, risk level, member count, and direct URL.</p> <pre><code>curl http://localhost:8001/v1/telegram/channels/example_channel\n</code></pre>","tags":["OSINT","Dark Web"]},{"location":"api/telegram/#patch-v1telegramchannelsusername","title":"PATCH /v1/telegram/channels/{username}","text":"<p>Update a channel's metadata. All fields are optional -- only provided fields will be updated.</p> <pre><code>curl -X PATCH http://localhost:8001/v1/telegram/channels/example_channel \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"risk_level\": \"critical\", \"tags\": [\"ransomware\", \"RaaS\", \"active\"]}'\n</code></pre>","tags":["OSINT","Dark Web"]},{"location":"api/telegram/#delete-v1telegramchannelsusername","title":"DELETE /v1/telegram/channels/{username}","text":"<p>Remove a channel from tracking.</p> <pre><code>curl -X DELETE http://localhost:8001/v1/telegram/channels/example_channel\n</code></pre>","tags":["OSINT","Dark Web"]},{"location":"api/telegram/#threat-category-shortcuts","title":"Threat Category Shortcuts","text":"","tags":["OSINT","Dark Web"]},{"location":"api/telegram/#get-v1telegramcritical","title":"GET /v1/telegram/critical","text":"<p>Quick access to all critical-risk channels -- highest priority for monitoring.</p> Name Type Default Description <code>limit</code> integer 100 Maximum results <pre><code>curl \"http://localhost:8001/v1/telegram/critical?limit=20\"\n</code></pre>","tags":["OSINT","Dark Web"]},{"location":"api/telegram/#get-v1telegramransomware","title":"GET /v1/telegram/ransomware","text":"<p>Quick access to all ransomware operator channels.</p> Name Type Default Description <code>limit</code> integer 100 Maximum results <pre><code>curl \"http://localhost:8001/v1/telegram/ransomware\"\n</code></pre>","tags":["OSINT","Dark Web"]},{"location":"api/telegram/#get-v1telegramdata-leaks","title":"GET /v1/telegram/data-leaks","text":"<p>Quick access to data breach marketplace channels.</p> Name Type Default Description <code>limit</code> integer 100 Maximum results <pre><code>curl \"http://localhost:8001/v1/telegram/data-leaks\"\n</code></pre>","tags":["OSINT","Dark Web"]},{"location":"api/telegram/#get-v1telegramstealer-logs","title":"GET /v1/telegram/stealer-logs","text":"<p>Quick access to infostealer credential channels (RedLine, Raccoon, Vidar, etc.).</p> Name Type Default Description <code>limit</code> integer 100 Maximum results <pre><code>curl \"http://localhost:8001/v1/telegram/stealer-logs\"\n</code></pre>","tags":["OSINT","Dark Web"]},{"location":"api/telegram/#get-v1telegramhacktivism","title":"GET /v1/telegram/hacktivism","text":"<p>Quick access to hacktivist group channels.</p> Name Type Default Description <code>limit</code> integer 100 Maximum results <pre><code>curl \"http://localhost:8001/v1/telegram/hacktivism\"\n</code></pre>","tags":["OSINT","Dark Web"]},{"location":"api/telegram/#intelligence-analytics","title":"Intelligence &amp; Analytics","text":"","tags":["OSINT","Dark Web"]},{"location":"api/telegram/#get-v1telegramstats","title":"GET /v1/telegram/stats","text":"<p>Overall statistics: total channels, breakdown by category, risk level, country, and tracked threat actors.</p> <pre><code>curl http://localhost:8001/v1/telegram/stats\n</code></pre>","tags":["OSINT","Dark Web"]},{"location":"api/telegram/#get-v1telegramthreat-actors","title":"GET /v1/telegram/threat-actors","text":"<p>List all tracked threat actors with their associated channels.</p> <pre><code>curl http://localhost:8001/v1/telegram/threat-actors\n</code></pre>","tags":["OSINT","Dark Web"]},{"location":"api/telegram/#get-v1telegramsources","title":"GET /v1/telegram/sources","text":"<p>List available external data sources for syncing.</p> <pre><code>curl http://localhost:8001/v1/telegram/sources\n</code></pre>","tags":["OSINT","Dark Web"]},{"location":"api/telegram/#data-management","title":"Data Management","text":"","tags":["OSINT","Dark Web"]},{"location":"api/telegram/#post-v1telegramsync","title":"POST /v1/telegram/sync","text":"<p>Sync channels from external threat intel sources. Currently syncs from ransomware.live Telegram data. Adds new channels and updates member counts for existing ones.</p> <pre><code>curl -X POST http://localhost:8001/v1/telegram/sync\n</code></pre>","tags":["OSINT","Dark Web"]},{"location":"api/telegram/#get-v1telegramexport","title":"GET /v1/telegram/export","text":"<p>Export channels in various formats.</p> <p>Parameters:</p> Name Type Default Description <code>format</code> string <code>json</code> Export format: <code>json</code>, <code>csv</code>, <code>urls</code> <code>category</code> string - Filter by category JSONCSVURL List <pre><code>curl \"http://localhost:8001/v1/telegram/export?format=json&amp;category=ransomware\"\n</code></pre> <pre><code>curl \"http://localhost:8001/v1/telegram/export?format=csv\" -o channels.csv\n</code></pre> <pre><code># Plain list of t.me URLs\ncurl \"http://localhost:8001/v1/telegram/export?format=urls\"\n</code></pre>","tags":["OSINT","Dark Web"]},{"location":"getting-started/","title":"Getting Started","text":"<p>Everything you need to start using the Crosshair API.</p> <ul> <li> <p> Quick Start</p> <p>Clone the repo, start Docker, and run your first search in under 2 minutes.</p> <p> Quick Start</p> </li> <li> <p> Authentication</p> <p>Configure API keys, set rate limits, and secure your deployment.</p> <p> Authentication</p> </li> <li> <p> Response Format</p> <p>Understand the JSON envelope, error codes, pagination, and response headers.</p> <p> Response Format</p> </li> </ul>"},{"location":"operations/","title":"Operations","text":"<p>Deployment, configuration, and security reference.</p> <ul> <li> <p> Configuration</p> <p>Environment variables, Docker Compose settings, and external API key setup.</p> <p> Configuration</p> </li> <li> <p> Security</p> <p>SSRF prevention, rate limiting, query validation, and webhook secret hashing.</p> <p> Security</p> </li> </ul>"},{"location":"reference/openapi/","title":"Interactive API Explorer","text":"<p>Explore and test all API endpoints interactively using the OpenAPI specification.</p> <p></p> <p>Base URL</p> <p>The API is available at <code>http://localhost:8001</code>. All endpoints are prefixed with <code>/v1/</code>.</p> <p>Authentication</p> <p>If API keys are configured, add your key via the Authorize button above, using the <code>X-API-Key</code> header.</p>"}]}